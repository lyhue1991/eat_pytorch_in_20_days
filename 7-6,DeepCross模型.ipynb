{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bcfb14",
   "metadata": {},
   "source": [
    "\n",
    "# 7-6ï¼ŒDeepCrossæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91553e",
   "metadata": {},
   "source": [
    "è°·æ­Œåœ¨CTRé¢„ä¼°å’Œæ¨èæ’åºæ¨¡å‹æ–¹é¢æœ‰3ç¯‡ç›¸å¯¹é‡è¦çš„æ–‡ç« ã€‚\n",
    "\n",
    "ç¬¬1ç¯‡æ˜¯2016å¹´çš„Deep&Wide,ç¬¬2ç¯‡æ˜¯2017å¹´çš„Deep&Cross(DCN),ç¬¬3ç¯‡æ˜¯2020å¹´çš„DCN-V2.\n",
    "\n",
    "è¿™3ç¯‡æ–‡ç« çš„æ ¸å¿ƒæ€æƒ³åˆ†åˆ«ç”¨1å¥è¯æ¦‚æ‹¬å¦‚ä¸‹ï¼š\n",
    "\n",
    "ç¬¬1ç¯‡Deep&Wideè¯´ï¼Œçº¿æ€§å›å½’LRå…·æœ‰å¥½çš„è®°å¿†èƒ½åŠ›(Wideéƒ¨åˆ†),å¤šå±‚æ„ŸçŸ¥æœºDNNå…·æœ‰å¥½çš„æ³›åŒ–èƒ½åŠ›(Deepéƒ¨åˆ†),æŠŠå®ƒä»¬å¹¶è¡Œçš„ç»“åˆèµ·æ¥é£Ÿç”¨å£æ„ŸçœŸé¦™.\n",
    "\n",
    "ç¬¬2ç¯‡Deep&Crossè¯´ï¼Œå°†Deep&Wideçš„Wideéƒ¨åˆ†æ¢æˆä¸€ä¸ªCrossNetï¼Œå¯ä»¥è‡ªåŠ¨æ•è·å¤šå±‚çº§ç‰¹å¾äº¤å‰ï¼Œå‡å°‘äººå·¥ç‰¹å¾å·¥ç¨‹ã€‚æ–°çš„æ¨¡å‹å«åšDCNå£æ„Ÿæ›´é¦™ï¼\n",
    "\n",
    "ç¬¬3ç¯‡DCNV2è¯´ï¼ŒDCNçš„CrossNetçš„ä»…ä»…ç”¨ä¸€ä¸ªå‘é‡æ¥å»ºæ¨¡äº¤å‰èƒ½åŠ›, æ‹Ÿåˆèƒ½åŠ›å¤ªå¼±, é…æ–¹å‡çº§æ¢æˆçŸ©é˜µå§ã€‚å‡çº§åçš„DCNV2é‡å¤§ç®¡é¥±ï¼Œåˆé¦™åˆé¥±ï¼ä»€ä¹ˆï¼ŒçŸ©é˜µå¤ªå¤§è·‘çš„æ…¢ï¼Œä¸å¤ªå¥½æ¶ˆåŒ–? åˆ«æ€¥ï¼Œæˆ‘ä»¬è¿˜æœ‰ç§˜åˆ¶é…æ–¹ï¼ŒæŠŠçŸ©é˜µæ¢æˆä¸¤ä¸ªä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ï¼Œè¿™ä¸ªç”¨äº†ç§˜åˆ¶é…æ–¹çš„æ¨¡å‹å«åšDCN-Mixï¼Œå˜å˜£è„†ï¼Œåˆé¦™åˆé¥±è¿˜å¥½æ¶ˆåŒ–ï¼Œä½ å€¼å¾—æ‹¥æœ‰ï¼\n",
    "\n",
    "ä»Šå¤©æˆ‘ä»¬å°±æ¥å°å°DCNã€DCNV2ä»¥åŠå˜å˜£è„†çš„DCN-Mixï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ebc71",
   "metadata": {},
   "source": [
    "å‚è€ƒæ–‡æ¡£ï¼š\n",
    "\n",
    "* Deep&Wideè®ºæ–‡ï¼š https://arxiv.org/pdf/1606.07792v1.pdf\n",
    "\n",
    "* DCNè®ºæ–‡ï¼šhttps://arxiv.org/pdf/1708.05123.pdf \n",
    "\n",
    "* DCNV2è®ºæ–‡ï¼š https://arxiv.org/pdf/2008.13535.pdf\n",
    "\n",
    "* è§£å¯†Deep&Cross: https://zhuanlan.zhihu.com/p/55234968 \n",
    "\n",
    "* å¤§è§„æ¨¡æ’åºç³»ç»Ÿä¸­çš„ç‰¹å¾äº¤å‰DCN-V2: https://zhuanlan.zhihu.com/p/353223660\n",
    "\n",
    "* ä»£ç å®ç°å‚è€ƒï¼šhttps://github.com/shenweichen/DeepCTR-Torch/blob/bc881dcd417fec64f840b0cacce124bc86b3687c/deepctr_torch/layers/interaction.py#L406-L537\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39112727",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<font color=\"red\">\n",
    " \n",
    "å…¬ä¼—å· **ç®—æ³•ç¾é£Ÿå±‹** å›å¤å…³é”®è¯ï¼š**pytorch**ï¼Œ è·å–æœ¬é¡¹ç›®æºç å’Œæ‰€ç”¨æ•°æ®é›†ç™¾åº¦äº‘ç›˜ä¸‹è½½é“¾æ¥ã€‚\n",
    "    \n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb6a1a",
   "metadata": {},
   "source": [
    "## ä¸€ï¼ŒDeepCrossåŸç†è§£æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14735953",
   "metadata": {},
   "source": [
    "è°·æ­Œçš„DeepCrossç³»åˆ—æ¨¡å‹(DCN-vector, DCN-matrix, DCN-mix) å›´ç»•è§£å†³çš„æ ¸å¿ƒé—®é¢˜ä¸‰é«˜é—®é¢˜ã€‚\n",
    "\n",
    "å¦‚ä½•å®ç° é«˜æ•ˆç‡ï¼Œé«˜è¡¨è¾¾èƒ½åŠ› çš„ é«˜é˜¶æ˜¾å¼ç‰¹å¾äº¤å‰ï¼Ÿ\n",
    "\n",
    "è¿™ä¸‰é«˜é—®é¢˜åˆ†åˆ«æ˜¯ä»€ä¹ˆå«ä¹‰å‘¢ï¼Ÿ\n",
    "\n",
    "1ï¼Œé«˜é˜¶ï¼šè‡³å°‘æ”¯æŒä¸‰é˜¶ç‰¹å¾äº¤å‰ã€‚å®è·µè¯æ˜ï¼Œåœ¨CTRå’Œæ¨èé¢†åŸŸï¼ŒäºŒé˜¶ç‰¹å¾äº¤å‰å’Œä¸‰é˜¶ç‰¹å¾äº¤å‰æ˜¯æœ‰ç”¨çš„ï¼Œå››é˜¶åŠä»¥ä¸Šçš„ç‰¹å¾äº¤å‰æ²¡å•¥ç”¨ã€‚FMåªæ˜¯äºŒé˜¶æ˜¾å¼ç‰¹å¾äº¤å‰ã€‚ \n",
    "\n",
    "2ï¼Œé«˜æ•ˆç‡ï¼šç‰¹å¾äº¤å‰æ¨¡å—çš„é¢„æµ‹æ•ˆç‡æœ€å¥½æ˜¯O(n),å…¶ä¸­nä¸ºç‰¹å¾æ•°é‡ã€‚\n",
    "\n",
    "3ï¼Œé«˜è¡¨è¾¾èƒ½åŠ›ï¼šç‰¹å¾äº¤å‰æ¨¡å—æœ‰è¶³å¤Ÿå¤§çš„å‚æ•°å®¹é‡æ¥æ•è·å„ç§å¯èƒ½çš„ç‰¹å¾äº¤å‰ã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬åˆ†åˆ«çœ‹çœ‹ DCN-vectorï¼Œ DCN-matrix, DCN-mixåœ¨ä¸‰é«˜é—®é¢˜çš„è§£å†³æ€è·¯å’Œè§£å†³ç¨‹åº¦ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5aa021",
   "metadata": {},
   "source": [
    "### 1ï¼Œ DCN-vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5959a9",
   "metadata": {},
   "source": [
    "DCN-vector ä¹Ÿå°±æ˜¯ 2017å¹´å‘å¸ƒçš„æœ€åˆç‰ˆæœ¬çš„DCNï¼Œ å¾ˆå¥½åœ°è§£å†³äº†ä¸‰é«˜é—®é¢˜ä¸­çš„ä¸¤é«˜ã€‚\n",
    "\n",
    "1ï¼Œé«˜é˜¶ (DCN-vectorå¯ä»¥å¾ˆæ–¹ä¾¿åœ°æ”¯æŒä¸‰é˜¶åŠä»¥ä¸Šçš„æ˜¾å¼ç‰¹å¾äº¤å‰)\n",
    "\n",
    "2ï¼Œé«˜æ•ˆç‡ (DCN-vectorç‰¹å¾äº¤å‰æ¨¡å—çš„æ¨ç†å¤æ‚åº¦æ˜¯O(n)ï¼Œéå¸¸é«˜æ•ˆï¼Œå’ŒFMä¸åˆ†ä¼¯ä»²)\n",
    "\n",
    "æˆ‘ä»¬æ¥çœ‹çœ‹DCN-vectoræ€ä¹ˆåšçš„æ˜¾å¼ç‰¹å¾äº¤å‰ã€‚\n",
    "\n",
    "å…ˆçœ‹å›¾å’Œå…¬å¼ã€‚\n",
    "\n",
    "æœ‰ä¸¤ä¸ªè¦ç‚¹ï¼Œä¸€ä¸ªæ˜¯ç”¨å’Œx0ç­‰é•¿çš„å‚æ•°å‘é‡æ¥æ•è·äº¤å‰å…³ç³»ï¼Œç¬¬äºŒä¸ªæ˜¯å‚ç…§äº†æ®‹å·®æ¨¡å—çš„è®¾è®¡å°†ä½é˜¶é‡åŠ å›åˆ°è¾“å‡ºä¸­ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7563e",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wduan1hmj20a30570sv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78c0cf",
   "metadata": {},
   "source": [
    "ä»£ç å’Œå›¾ä¸€æ ·å¥½æ‡‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e109057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "class CrossNetVector(nn.Module):\n",
    "    def __init__(self, d_in, n_cross=2):\n",
    "        super().__init__()\n",
    "        self.n_cross = n_cross\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_in,1,bias=False) for i in range(self.n_cross)])\n",
    "        self.biases = nn.ParameterList(\n",
    "            [nn.Parameter(torch.zeros(d_in)) for i in range(self.n_cross)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        xi = x\n",
    "        for i in range(self.n_cross):\n",
    "            xi = x0*self.linears[i](xi)+self.biases[i]+xi\n",
    "        return xi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fcd0b6",
   "metadata": {},
   "source": [
    "CrossNetVectorçš„ç»“æ„éå¸¸ç®€æ´ã€‚æ¯å‡é«˜ä¸€é˜¶ç‰¹å¾äº¤å‰ï¼Œä¸»è¦è¿ç®—æ˜¯å°†ä¸Šä¸€é˜¶ç‰¹å¾å‘é‡xiå’Œäº¤å‰æ¨¡å—å‚æ•°å‘é‡wåšç‚¹ç§¯ï¼Œå¹¶æ•°ä¹˜åŸå§‹ç‰¹å¾å‘é‡x0ã€‚\n",
    "\n",
    "è¯¥è®¡ç®—çš„è®¡ç®—å‰åå‘é‡çš„ç»´åº¦ä¸å˜ï¼Œè®¡ç®—é‡å’Œç‰¹å¾å‘é‡çš„é•¿åº¦næˆæ­£æ¯”ï¼Œæ‰€ä»¥å¤æ‚åº¦æ˜¯O(n).\n",
    "\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶CrossNetVectorçš„å¤æ‚åº¦å¾ˆä½ä»…ä»…æ˜¯O(n),  ä½†æ˜¯ $x_{0}*x^{'}*\\omega$ è¿™ä¸€é¡¹å±•å¼€åæ˜¯åŒ…æ‹¬ä»»æ„ä¸¤ä¸ªç‰¹å¾çš„äº¤å‰ä¹˜ç§¯è´¡çŒ®çš„ã€‚\n",
    "\n",
    "æˆ‘ä»¬çŸ¥é“ä»»æ„ä¸¤ä¸ªç‰¹å¾çš„äº¤å‰å…±æœ‰ n(n-1)/2 ç§ç»„åˆæ–¹å¼ï¼Œè¿™ä¸ªå¤æ‚åº¦æ˜¯O(n^2)çš„ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆ CrossNetVector èƒ½å¤Ÿä»¥ O(n)çš„å¤æ‚åº¦å®ç°åŒ…æ‹¬O(n^2)é‡çº§çš„ç‰¹å¾äº¤å‰ç»„åˆè´¡çŒ®çš„å‘¢ï¼Ÿ\n",
    "\n",
    "åŸå› æ˜¯CrossNetVectorè¿™ç§ç»“æ„å®é™…ä¸Šå…·æœ‰æƒé‡å…±äº«çš„ç‰¹æ€§ã€‚\n",
    "\n",
    "å‡è®¾æœ‰5ä¸ªç‰¹å¾ x1,x2,x3,x4,x5, å¦‚æœè¦ç‹¬ç«‹åœ°æè¿°å®ƒä»¬ä¹‹é—´çš„ä¸¤ä¸¤äº¤å‰è¦10ä¸ªå‚æ•°ï¼Œä½†æ˜¯CrossNetVectorä¸­ä»…ä»…æœ‰5ä¸ªè¡¨è¿°ç‰¹å¾äº¤å‰ç³»æ•°çš„å‚æ•° w1,w2,w3,w4,w5\n",
    "\n",
    "å…¶ä¸­ x1x2çš„äº¤å‰é¡¹çš„ç³»æ•°æ˜¯ (w1+w2), x1x3çš„äº¤å‰é¡¹ç³»æ•°æ—¶ (w1+w3),...,x1x5çš„äº¤å‰é¡¹ç³»æ•°æ˜¯ (w1+w5), å¯è§ï¼Œw1è¿™ä¸ªå‚æ•°è¢«ç”¨åœ¨äº† x1å’Œä»»ä½•å‚æ•°çš„äº¤å‰é¡¹ä¸­ä½œä¸ºç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ä¸€ç§æƒé‡å…±äº«çš„ç‰¹æ€§ï¼Œè¿™ç§ç‰¹æ€§æœ‰åŠ©äºDCNå˜å¾—è½»ä¾¿é«˜æ•ˆï¼Œå¹¶åƒFMé‚£æ ·å­¦ä¹ åˆ°ç¨€ç–ç‰¹å¾çš„äº¤å‰ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4965699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4daccd07",
   "metadata": {},
   "source": [
    "### 2, DCN-matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114c672",
   "metadata": {},
   "source": [
    "DCN-vector è™½ç„¶è§£å†³äº†ä¸‰é«˜ä¸­çš„ä¸¤é«˜: é«˜é˜¶ã€é«˜æ•ˆç‡ã€‚\n",
    "\n",
    "ä½†æ˜¯å®ƒåœ¨å¦ä¸€é«˜ä¹Ÿå°±æ˜¯é«˜è¡¨è¾¾èƒ½åŠ›æ–¹é¢æœ‰äº›ç¼ºé™·ï¼Œå®ƒä»…ä»…ç”¨ä¸€ä¸ªå’Œè¾“å…¥ç‰¹å¾ç­‰é•¿çš„å‘é‡æ¥åˆ»ç”»ç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä¸è¶³ä»¥è¡¨è¿°ä¸°å¯Œçš„ç‰¹å¾äº¤å‰æ¨¡å¼ã€‚\n",
    "\n",
    "CrossNetVectorçš„å‚æ•°æ•°é‡ç›¸æ¯”Deepéƒ¨åˆ†çš„MLPçš„å‚æ•°æ•°é‡æ¥çœ‹å®åœ¨å¾®ä¸è¶³é“ï¼Œå¯èƒ½å­˜åœ¨è®°å¿†å®¹é‡ç“¶é¢ˆã€‚\n",
    "\n",
    "æ¢è¨€ä¹‹ï¼Œå®ƒçš„æƒå€¼å…±äº«å¯èƒ½åšçš„æœ‰äº›è¿‡å¤´äº†ã€‚\n",
    "\n",
    "æ­£æ˜¯åŸºäºè¿™æ ·çš„æ´å¯Ÿï¼ŒDCN-matrix åœ¨DCN-vectoråŸºç¡€ä¸Šåšäº†ä¸€äº›è½»å¾®çš„æ”¹åŠ¨ï¼Œç”¨çŸ©é˜µä»£æ›¿äº†å‘é‡ï¼Œå¹¶é€‚é…æ€§åœ°ä¿®æ”¹äº†ä¸€äº›è¿ç®—ç¬¦ï¼Œå¼ºåŒ–äº†æ˜¾å¼ç‰¹å¾äº¤å‰é¡¹çš„è¡¨è¾¾èƒ½åŠ›ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da09f6",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wgsq1zcyj208u0473yj.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4a3d7",
   "metadata": {},
   "source": [
    "ä»£ç æ¯”å›¾æ›´åŠ å¥½æ‡‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e93a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "class CrossNetMatrix(nn.Module):\n",
    "    def __init__(self, d_in, n_cross=2):\n",
    "        super().__init__()\n",
    "        self.n_cross = n_cross\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_in,d_in) for i in range(self.n_cross)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        xi = x\n",
    "        for i in range(self.n_cross):\n",
    "            xi = x0*self.linears[i](xi)+xi\n",
    "        return xi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a9a52",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ° CrossNetMatrix ç”¨ä¸€ä¸ª $n\\times n$ çš„äº¤äº’çŸ©é˜µ$W$æ¥åˆ»ç”» ç‰¹å¾ä¹‹é—´çš„äº¤äº’ï¼Œå®é™…ä¸Š$W$çš„ç¬¬iè¡Œç¬¬jåˆ—çš„å…ƒç´  $W_{ij}$\n",
    "\n",
    "è¡¨è¾¾çš„å°±æ˜¯ç¬¬iä¸ªç‰¹å¾å’Œç¬¬jä¸ªç‰¹å¾ä¹‹é—´çš„äº¤äº’é¡¹ç³»æ•°ã€‚ å› æ­¤ CrossNetMatrix çš„å¤æ‚åº¦æ˜¯O(n^2)çš„ï¼Œå¹¶ä¸”å…·æœ‰å¾ˆå¥½çš„è§£é‡Šå‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88a0f8",
   "metadata": {},
   "source": [
    "äºæ˜¯ CrossNetMatrixæ•´ä½“æ¥è¯´æ˜¯ç‰ºç‰²äº† é«˜æ•ˆæ€§ï¼Œæ¢æ¥äº†é«˜è¡¨è¾¾èƒ½åŠ›ã€‚ä¸è¿‡ï¼Œåœ¨è®¸å¤šç‰¹å¾æ•°é‡ä¸ç®—ç‰¹åˆ«å¤šçš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚nåœ¨1000ä»¥ä¸‹çš„æ—¶å€™ï¼ŒCrossNetMatrixä¾æ—§æ˜¯éå¸¸é«˜æ•ˆçš„ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28fe65",
   "metadata": {},
   "source": [
    "### 3, DCN-mix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a18321",
   "metadata": {},
   "source": [
    "DCN-mixå¯ä»¥è¯´æ˜¯ DCN-matrixåŸºç¡€ä¸Šæå‡è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒé«˜è¡¨è¾¾èƒ½åŠ›çš„ä¸€ç§æ–¹æ¡ˆã€‚ \n",
    "\n",
    "å…¶ä¸»è¦æ€æƒ³æ´å¯Ÿå¦‚ä¸‹ã€‚\n",
    "\n",
    "1ï¼ŒçŸ©é˜µåˆ†è§£ï¼šæ ¹æ®SVDä¸»æˆåˆ†åˆ†æç†è®ºï¼Œåºå¤§çš„äº¤äº’çŸ©é˜µWå¯ä»¥ç”±ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µUå’ŒVçš„ä¹˜ç§¯æ¥è¿‘ä¼¼ã€‚è¿™æ ·å¯ä»¥å°†å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦é™ä½åˆ°O(n)ã€‚\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wid7h72rj209s01ga9v.jpg)\n",
    "\n",
    "2ï¼Œä¸“å®¶èåˆï¼šä½¿ç”¨ä¸¤ä¸ªä½ç§©çŸ©é˜µUå’ŒVçš„ä¹˜ç§¯æ¥è¿‘ä¼¼Wæå‡äº†æ•ˆç‡ä½†æ˜¯é™ä½äº†è¡¨è¾¾èƒ½åŠ›ï¼Œå¯ä»¥ä½¿ç”¨MOEæ€æƒ³(Mixture of Experts)èåˆå¤šä¸ªä½ç§©ç©ºé—´çš„ä¸“å®¶ç½‘ç»œæ¥æå‡è¡¨è¾¾èƒ½åŠ›ã€‚\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wibupquej20eb0ayjrt.jpg)\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wiet82zxj209302ma9y.jpg)\n",
    "\n",
    "3ï¼Œä½ç§©å˜æ¢ï¼šå¯ä»¥åœ¨Uå’ŒVä¹‹é—´åœ¨ä½ç§©ç©ºé—´ä¸Šåšä¸€äº›å˜æ¢é€‚å½“å†æ‹‰ä¸€æ‹‰è¡¨è¾¾èƒ½åŠ›ã€‚\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wifm6877j20ac01d744.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbbc1e",
   "metadata": {},
   "source": [
    "æ•´ä½“æ¥è¯´ï¼ŒDCN-mixç”¨åˆ°çš„è¿™3ä¸ªæŠ€å·§è¿˜æ˜¯æœ‰äº›éš¾åº¦çš„ï¼Œå¯ä»¥ç»“åˆä»£ç ç†è§£ä¸€ä¸‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "class CrossNetMix(nn.Module):\n",
    "    def __init__(self, d_in, n_cross =2, low_rank=32, n_experts=4):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.n_cross = n_cross\n",
    "        self.low_rank = low_rank\n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        # U: (d_in, low_rank)\n",
    "        self.U_list = nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(n_experts, d_in, low_rank))) for i in range(self.n_cross)])\n",
    "\n",
    "        # V: (d_in, low_rank)\n",
    "        self.V_list = nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(n_experts, d_in, low_rank))) for i in range(self.n_cross)])\n",
    "\n",
    "        # C: (low_rank, low_rank)\n",
    "        self.C_list = nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(n_experts, low_rank, low_rank))) for i in range(self.n_cross)])\n",
    "\n",
    "        # G: (d_in, 1)\n",
    "        self.gating = nn.ModuleList([nn.Linear(d_in, 1, bias=False) for i in range(self.n_experts)])\n",
    "\n",
    "        # Bias \n",
    "        self.biases = nn.ParameterList([nn.Parameter(torch.zeros(d_in)) for i in range(self.n_cross)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        xi = x\n",
    "        for i in range(self.n_cross):\n",
    "            output_of_experts = []\n",
    "            gating_score_of_experts = []\n",
    "            for expert_id in range(self.n_experts):\n",
    "\n",
    "                # (1) G(xi)\n",
    "                # compute the gating score by xi\n",
    "                gating_score_of_experts.append(self.gating[expert_id](xi))\n",
    "\n",
    "                # (2) E(xi)\n",
    "                # project the input xi to low_rank space\n",
    "                v_x = xi@(self.V_list[i][expert_id])   # (batch_size, low_rank)\n",
    "\n",
    "                # nonlinear activation in low rank space\n",
    "                v_x = torch.tanh(v_x)\n",
    "                v_x = v_x@self.C_list[i][expert_id]     # (batch_size, low_rank)\n",
    "                v_x = torch.tanh(v_x)\n",
    "\n",
    "                # project back to d_in space\n",
    "                uv_x = v_x@(self.U_list[i][expert_id].T)  # (batch_size, d_in)\n",
    "                expert_out = x0*(uv_x + self.biases[i])\n",
    "                output_of_experts.append(expert_out)\n",
    "\n",
    "            # (3) mixture of low-rank experts\n",
    "            output_of_experts = torch.stack(output_of_experts, 2)  # (batch_size, d_in, n_experts)\n",
    "            gating_score_of_experts = torch.stack(gating_score_of_experts, 1)  # (batch_size, n_experts, 1)\n",
    "            moe_out = torch.bmm(output_of_experts, gating_score_of_experts.softmax(1))\n",
    "            xi = torch.squeeze(moe_out) + xi  # (batch_size, d_in)\n",
    "\n",
    "        return xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac338ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc2fcc0e",
   "metadata": {},
   "source": [
    "## äºŒï¼ŒDeepCrossçš„pytorchå®ç°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff274e",
   "metadata": {},
   "source": [
    "ä¸‹é¢æ˜¯DeepCrossçš„ä¸€ä¸ªpytorchå®Œæ•´å®ç°ã€‚\n",
    "\n",
    "å¯ä»¥é€šè¿‡cross_type æ¥æŒ‡å®šæ˜¯å“ªä¸ªç‰ˆæœ¬çš„DCNæ¨¡å‹ã€‚\n",
    "\n",
    "* cross_type = \"vector\" é€‰æ‹© DCNV1, ä¹Ÿå¯ä»¥ç§°ä¹‹ä¸º DCN-vector \n",
    "\n",
    "* cross_type = \"matrix\" é€‰æ‹© DCNV2, ä¹Ÿå¯ä»¥ç§°ä¹‹ä¸º DCN-matrix \n",
    "\n",
    "* cross_type = \"mix\" é€‰æ‹© DCN-mix \n",
    "\n",
    "é™¤äº†ä¸‰ç§CrossNetç»“æ„çš„å®ç°å¤–ï¼Œè¿˜æœ‰ä¸¤ç‚¹ç»†èŠ‚å€¼å¾—æ³¨æ„ï¼š\n",
    "\n",
    "1ï¼ŒDeepCrosså¯¹ç±»åˆ«ç‰¹å¾çš„å¤„ç†ä¸DeepFMç­‰æ¨¡å‹ä¸å¤ªä¸€æ ·ï¼Œæ˜¯å…¨éƒ¨è½¬æ¢æˆembeddingåå†å’Œæ•°å€¼ç±»ç‰¹å¾æ‹¼åœ¨ä¸€èµ·ï¼Œè¿™ç§ç‰¹æ€§å¯ä»¥è®©å„ä¸ªç±»åˆ«ç‰¹å¾æ ¹æ®å…¶ç±»åˆ«æ•°é‡é€‰æ‹©é€‚å½“çš„embeddingé•¿åº¦ã€‚\n",
    "\n",
    "2ï¼ŒDeepCrossä¸­Deepéƒ¨åˆ†å’ŒCrosséƒ¨åˆ†çš„å…³ç³»å¯ä»¥æ˜¯ä¸²è¡Œçš„(stacked=True)ï¼Œä¹Ÿå¯ä»¥æ˜¯å¹¶è¡Œçš„(stacked=False)ã€‚å…·ä½“å“ªç§æ•ˆæœæ›´å¥½ï¼Œå–å†³äºæ•°æ®é›†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn,Tensor \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#ç¦»æ•£ç‰¹å¾ç¼–ç \n",
    "class CatEmbeddingSqrt(nn.Module):\n",
    "    \"\"\"\n",
    "    ç¦»æ•£ç‰¹å¾ä½¿ç”¨Embeddingå±‚ç¼–ç , d_embedç­‰äºsqrt(category)\n",
    "    è¾“å…¥shape: [batch_size,d_in], \n",
    "    è¾“å‡ºshape: [batch_size,d_out]\n",
    "    \"\"\"\n",
    "    def __init__(self, categories, d_embed_max = 100):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.d_embed_list = [min(max(int(x**0.5), 2),d_embed_max) for x in categories]\n",
    "        self.embedding_list = nn.ModuleList([nn.Embedding(self.categories[i],self.d_embed_list[i])\n",
    "                            for i in range(len(categories))])\n",
    "        self.d_cat_sum = sum(self.d_embed_list)\n",
    "        \n",
    "    def forward(self, x_cat):\n",
    "        \"\"\"\n",
    "        param x_cat: Long tensor of size ``(batch_size, d_in)``\n",
    "        \"\"\"\n",
    "        x_out = torch.cat([self.embedding_list[i](x_cat[:,i]) \n",
    "                           for i in range(len(self.categories)) ],dim=1)\n",
    "        return x_out\n",
    "    \n",
    "#deepéƒ¨åˆ†\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in, d_layers, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for d in d_layers:\n",
    "            layers.append(nn.Linear(d_in, d))\n",
    "            layers.append(nn.BatchNorm1d(d))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = d\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "\n",
    "#3ç§CrossNetçš„å®ç°    \n",
    "class CrossNetVector(nn.Module):\n",
    "    def __init__(self, d_in, n_cross=2):\n",
    "        super().__init__()\n",
    "        self.n_cross = n_cross\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_in,1,bias=False) for i in range(self.n_cross)])\n",
    "        self.biases = nn.ParameterList(\n",
    "            [nn.Parameter(torch.zeros(d_in)) for i in range(self.n_cross)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        xi = x\n",
    "        for i in range(self.n_cross):\n",
    "            xi = x0*self.linears[i](xi)+self.biases[i]+xi\n",
    "        return xi\n",
    "    \n",
    "    \n",
    "class CrossNetMatrix(nn.Module):\n",
    "    def __init__(self, d_in, n_cross=2):\n",
    "        super().__init__()\n",
    "        self.n_cross = n_cross\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_in,d_in) for i in range(self.n_cross)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        xi = x\n",
    "        for i in range(self.n_cross):\n",
    "            xi = x0*self.linears[i](xi)+xi\n",
    "        return xi\n",
    "    \n",
    "\n",
    "class CrossNetMix(nn.Module):\n",
    "    def __init__(self, d_in, n_cross =2, low_rank=32, n_experts=4):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.n_cross = n_cross\n",
    "        self.low_rank = low_rank\n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        # U: (d_in, low_rank)\n",
    "        self.U_list = nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(n_experts, d_in, low_rank))) for i in range(self.n_cross)])\n",
    "        \n",
    "        # V: (d_in, low_rank)\n",
    "        self.V_list = nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(n_experts, d_in, low_rank))) for i in range(self.n_cross)])\n",
    "        \n",
    "        # C: (low_rank, low_rank)\n",
    "        self.C_list = nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(n_experts, low_rank, low_rank))) for i in range(self.n_cross)])\n",
    "        \n",
    "        # G: (d_in, 1)\n",
    "        self.gating = nn.ModuleList([nn.Linear(d_in, 1, bias=False) for i in range(self.n_experts)])\n",
    "\n",
    "        # Bias \n",
    "        self.biases = nn.ParameterList([nn.Parameter(torch.zeros(d_in)) for i in range(self.n_cross)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        xi = x\n",
    "        for i in range(self.n_cross):\n",
    "            output_of_experts = []\n",
    "            gating_score_of_experts = []\n",
    "            for expert_id in range(self.n_experts):\n",
    "                \n",
    "                # (1) G(xi)\n",
    "                # compute the gating score by xi\n",
    "                gating_score_of_experts.append(self.gating[expert_id](xi))\n",
    "\n",
    "                # (2) E(xi)\n",
    "                # project the input xi to low_rank space\n",
    "                v_x = xi@(self.V_list[i][expert_id])   # (batch_size, low_rank)\n",
    "\n",
    "                # nonlinear activation in low rank space\n",
    "                v_x = torch.tanh(v_x)\n",
    "                v_x = v_x@self.C_list[i][expert_id]     # (batch_size, low_rank)\n",
    "                v_x = torch.tanh(v_x)\n",
    "\n",
    "                # project back to d_in space\n",
    "                uv_x = v_x@(self.U_list[i][expert_id].T)  # (batch_size, d_in)\n",
    "                expert_out = x0*(uv_x + self.biases[i])\n",
    "                output_of_experts.append(expert_out)\n",
    "\n",
    "            # (3) mixture of low-rank experts\n",
    "            output_of_experts = torch.stack(output_of_experts, 2)  # (batch_size, d_in, n_experts)\n",
    "            gating_score_of_experts = torch.stack(gating_score_of_experts, 1)  # (batch_size, n_experts, 1)\n",
    "            moe_out = torch.bmm(output_of_experts, gating_score_of_experts.softmax(1))\n",
    "            xi = torch.squeeze(moe_out) + xi  # (batch_size, d_in)\n",
    "            \n",
    "        return xi\n",
    "    \n",
    "    \n",
    "class DeepCross(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepCrossä¸‰ç§æ¨¡å‹(DCN-vec,DCN-matrix,DCN-mix)çš„ç»Ÿä¸€å®ç°ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_numerical, categories, d_embed_max = 8,\n",
    "                 n_cross=2, cross_type = \"matrix\", low_rank=32, n_experts=4, \n",
    "                 mlp_layers = [128,64,32] ,mlp_dropout = 0.25, \n",
    "                 stacked = True, \n",
    "                 n_classes = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if cross_type=='mix':\n",
    "            assert low_rank is not None and n_experts is not None\n",
    "        \n",
    "        if d_numerical is None:\n",
    "            d_numerical = 0\n",
    "        if categories is None:\n",
    "            categories = []\n",
    "            \n",
    "        self.categories = categories\n",
    "        self.n_classes = n_classes\n",
    "        self.stacked = stacked\n",
    "        \n",
    "        \n",
    "        self.cat_embedding = CatEmbeddingSqrt(categories, d_embed_max) if categories else None\n",
    "        \n",
    "        self.d_in = d_numerical \n",
    "        if self.cat_embedding:\n",
    "            self.d_in+=self.cat_embedding.d_cat_sum\n",
    "            \n",
    "        if cross_type==\"vector\":\n",
    "            self.cross_layer = CrossNetVector(self.d_in,n_cross)\n",
    "        elif cross_type==\"matrix\":\n",
    "            self.cross_layer = CrossNetMatrix(self.d_in,n_cross)\n",
    "        elif cross_type==\"mix\":\n",
    "            self.cross_layer = CrossNetMix(self.d_in,n_cross,low_rank,n_experts)\n",
    "        else:\n",
    "            raise NotImplementedError(\"cross_type should  be one of ('vector','matrix','mix') !\")\n",
    "        \n",
    "        self.mlp = MLP(\n",
    "            d_in= self.d_in,\n",
    "            d_layers = mlp_layers,\n",
    "            dropout = mlp_dropout\n",
    "        )\n",
    "        \n",
    "        if self.stacked:\n",
    "            self.last_linear = nn.Linear(mlp_layers[-1],n_classes)\n",
    "        else:\n",
    "            self.last_linear = nn.Linear(self.d_in+mlp_layers[-1],n_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        x_num: numerical features\n",
    "        x_cat: category features\n",
    "        \"\"\"\n",
    "        x_num,x_cat = x\n",
    "        \n",
    "        #embedding \n",
    "        x_total = []\n",
    "        if x_num is not None:\n",
    "            x_total.append(x_num)\n",
    "        if self.cat_embedding is not None:\n",
    "            x_total.append(self.cat_embedding(x_cat))\n",
    "        x_total = torch.cat(x_total, dim=-1)\n",
    "        \n",
    "        \n",
    "        #crosséƒ¨åˆ†\n",
    "        x_cross = self.cross_layer(x_total)\n",
    "        \n",
    "        \n",
    "        #deepéƒ¨åˆ†\n",
    "        if self.stacked:\n",
    "            x_deep = self.mlp(x_cross)\n",
    "            x_out = self.last_linear(x_deep)\n",
    "        else:\n",
    "            x_deep = self.mlp(x_total)\n",
    "            x_deep_cross = torch.cat([x_deep,x_cross],axis = 1)\n",
    "            x_out = self.last_linear(x_deep_cross)\n",
    "            \n",
    "        if self.n_classes==1:\n",
    "            x_out = x_out.squeeze(-1)\n",
    "        \n",
    "        return x_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb91e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##æµ‹è¯• DeepCross\n",
    "\n",
    "x_num = torch.randn(2,3)\n",
    "x_cat = torch.randint(0,2,(2,3))\n",
    "\n",
    "dcn_vec = DeepCross(d_numerical = 3, categories = [4,3,2], d_embed_max = 4,\n",
    "        n_cross=2, cross_type = \"vector\", \n",
    "        mlp_layers = [20,20], mlp_dropout=0.25,\n",
    "        stacked = False,\n",
    "        n_classes = 1)\n",
    "\n",
    "dcn_matrix = DeepCross(d_numerical = 3, categories = [4,3,2], d_embed_max = 4,\n",
    "        n_cross=2, cross_type = \"matrix\", \n",
    "        mlp_layers = [20,20], mlp_dropout=0.25,\n",
    "        stacked = True,\n",
    "        n_classes = 1)\n",
    "\n",
    "dcn_mix = DeepCross(d_numerical = 3, categories = [4,3,2], d_embed_max = 4,\n",
    "        n_cross=2, cross_type = \"mix\", low_rank=32, n_experts=4, \n",
    "        mlp_layers = [20,20], mlp_dropout=0.25,\n",
    "        stacked = False,\n",
    "        n_classes = 1)\n",
    "\n",
    "\n",
    "print(dcn_vec((x_num,x_cat)))\n",
    "print(dcn_matrix((x_num,x_cat)))\n",
    "print(dcn_mix((x_num,x_cat)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b5f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "371196d6",
   "metadata": {},
   "source": [
    "## ä¸‰ï¼ŒCriteoæ•°æ®é›†å®Œæ•´èŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888bd54a",
   "metadata": {},
   "source": [
    "Criteoæ•°æ®é›†æ˜¯ä¸€ä¸ªç»å…¸çš„å¹¿å‘Šç‚¹å‡»ç‡CTRé¢„æµ‹æ•°æ®é›†ã€‚\n",
    "\n",
    "è¿™ä¸ªæ•°æ®é›†çš„ç›®æ ‡æ˜¯é€šè¿‡ç”¨æˆ·ç‰¹å¾å’Œå¹¿å‘Šç‰¹å¾æ¥é¢„æµ‹æŸæ¡å¹¿å‘Šæ˜¯å¦ä¼šä¸ºç”¨æˆ·ç‚¹å‡»ã€‚\n",
    "\n",
    "æ•°æ®é›†æœ‰13ç»´æ•°å€¼ç‰¹å¾(I1-I13)å’Œ26ç»´ç±»åˆ«ç‰¹å¾(C14-C39), å…±39ç»´ç‰¹å¾, ç‰¹å¾ä¸­åŒ…å«ç€è®¸å¤šç¼ºå¤±å€¼ã€‚\n",
    "\n",
    "è®­ç»ƒé›†4000ä¸‡ä¸ªæ ·æœ¬ï¼Œæµ‹è¯•é›†600ä¸‡ä¸ªæ ·æœ¬ã€‚æ•°æ®é›†å¤§å°è¶…è¿‡100G.\n",
    "\n",
    "æ­¤å¤„ä½¿ç”¨çš„æ˜¯é‡‡æ ·100ä¸‡ä¸ªæ ·æœ¬åçš„cretio_smallæ•°æ®é›†ã€‚ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c74075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader  \n",
    "import torch.nn.functional as F \n",
    "import torchkeras \n",
    "\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(info+'...\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79eaa4",
   "metadata": {},
   "source": [
    "### 1ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c810d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "dfdata = pd.read_csv(\"./eat_pytorch_datasets/criteo_small.zip\",sep=\"\\t\",header=None)\n",
    "dfdata.columns = [\"label\"] + [\"I\"+str(x) for x in range(1,14)] + [\n",
    "    \"C\"+str(x) for x in range(14,40)]\n",
    "\n",
    "cat_cols = [x for x in dfdata.columns if x.startswith('C')]\n",
    "num_cols = [x for x in dfdata.columns if x.startswith('I')]\n",
    "num_pipe = Pipeline(steps = [('impute',SimpleImputer()),('quantile',QuantileTransformer())])\n",
    "\n",
    "for col in cat_cols:\n",
    "    dfdata[col]  = LabelEncoder().fit_transform(dfdata[col])\n",
    "\n",
    "dfdata[num_cols] = num_pipe.fit_transform(dfdata[num_cols])\n",
    "\n",
    "categories = [dfdata[col].max()+1 for col in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "#DataFrameè½¬æ¢æˆtorchæ•°æ®é›†Dataset, ç‰¹å¾åˆ†å‰²æˆX_num,X_catæ–¹å¼\n",
    "class DfDataset(Dataset):\n",
    "    def __init__(self,df,\n",
    "                 label_col,\n",
    "                 num_features,\n",
    "                 cat_features,\n",
    "                 categories,\n",
    "                 is_training=True):\n",
    "        \n",
    "        self.X_num = torch.tensor(df[num_features].values).float() if num_features else None\n",
    "        self.X_cat = torch.tensor(df[cat_features].values).long() if cat_features else None\n",
    "        self.Y = torch.tensor(df[label_col].values).float() \n",
    "        self.categories = categories\n",
    "        self.is_training = is_training\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if self.is_training:\n",
    "            return ((self.X_num[index],self.X_cat[index]),self.Y[index])\n",
    "        else:\n",
    "            return (self.X_num[index],self.X_cat[index])\n",
    "    \n",
    "    def get_categories(self):\n",
    "        return self.categories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain_val,dftest = train_test_split(dfdata,test_size=0.2)\n",
    "dftrain,dfval = train_test_split(dftrain_val,test_size=0.2)\n",
    "\n",
    "ds_train = DfDataset(dftrain,label_col = \"label\",num_features = num_cols,cat_features = cat_cols,\n",
    "                    categories = categories, is_training=True)\n",
    "\n",
    "ds_val = DfDataset(dfval,label_col = \"label\",num_features = num_cols,cat_features = cat_cols,\n",
    "                    categories = categories, is_training=True)\n",
    "\n",
    "ds_test = DfDataset(dftest,label_col = \"label\",num_features = num_cols,cat_features = cat_cols,\n",
    "                    categories = categories, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a784c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train,batch_size = 2048,shuffle=True)\n",
    "dl_val = DataLoader(ds_val,batch_size = 2048,shuffle=False)\n",
    "dl_test = DataLoader(ds_test,batch_size = 2048,shuffle=False)\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bec29",
   "metadata": {},
   "source": [
    "### 2ï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63435a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¤å¤„æˆ‘ä»¬é€‰æ‹©çš„äº¤å‰æ¨¡å—æ˜¯CrossNetMatrix, ä¹Ÿå°±æ˜¯æ„å»ºçš„DCNV2æ¨¡å‹ \n",
    "# è¯»è€…ä¹Ÿå¯ä»¥å°è¯•CrossNetVectorå’ŒCrossNetMix \n",
    "def create_net():\n",
    "    net = DeepCross(\n",
    "        d_numerical= ds_train.X_num.shape[1],\n",
    "        categories= ds_train.get_categories(),\n",
    "        d_embed_max = 8, \n",
    "        n_cross = 2, cross_type = \"matrix\",\n",
    "        mlp_layers = [128,64,32], mlp_dropout=0.25,\n",
    "        stacked = True,\n",
    "        n_classes = 1\n",
    "        \n",
    "    )\n",
    "    return net \n",
    "\n",
    "from torchkeras import summary\n",
    "\n",
    "net = create_net()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869f709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b34700",
   "metadata": {},
   "source": [
    "### 3ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from accelerate import Accelerator\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(str(info)+\"\\n\")\n",
    "    \n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn,stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None,\n",
    "                 accelerator = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator\n",
    "    \n",
    "    def __call__(self, features, labels):\n",
    "        #loss\n",
    "        preds = self.net(features)\n",
    "        loss = self.loss_fn(preds,labels)\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            if self.accelerator is  None:\n",
    "                loss.backward()\n",
    "            else:\n",
    "                self.accelerator.backward(loss)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        #metrics\n",
    "        step_metrics = {self.stage+\"_\"+name:metric_fn(preds, labels).item() \n",
    "                        for name,metric_fn in self.metrics_dict.items()}\n",
    "        return loss.item(),step_metrics\n",
    "    \n",
    "    \n",
    "class EpochRunner:\n",
    "    def __init__(self,steprunner):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        self.steprunner.net.train() if self.stage==\"train\" else self.steprunner.net.eval()\n",
    "        \n",
    "    def __call__(self,dataloader):\n",
    "        total_loss,step = 0,0\n",
    "        loop = tqdm(enumerate(dataloader), total =len(dataloader))\n",
    "        for i, batch in loop:\n",
    "            features,labels = batch\n",
    "            if self.stage==\"train\":\n",
    "                loss, step_metrics = self.steprunner(features,labels)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    loss, step_metrics = self.steprunner(features,labels)\n",
    "\n",
    "            step_log = dict({self.stage+\"_loss\":loss},**step_metrics)\n",
    "\n",
    "            total_loss += loss\n",
    "            step+=1\n",
    "            if i!=len(dataloader)-1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss/step\n",
    "                epoch_metrics = {self.stage+\"_\"+name:metric_fn.compute().item() \n",
    "                                 for name,metric_fn in self.steprunner.metrics_dict.items()}\n",
    "                epoch_log = dict({self.stage+\"_loss\":epoch_loss},**epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name,metric_fn in self.steprunner.metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "        return epoch_log\n",
    "\n",
    "class KerasModel(torch.nn.Module):\n",
    "    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler = None):\n",
    "        super().__init__()\n",
    "        self.accelerator = Accelerator()\n",
    "        self.history = {}\n",
    "        \n",
    "        self.net = net\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metrics_dict = nn.ModuleDict(metrics_dict) \n",
    "        \n",
    "        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n",
    "            self.parameters(), lr=1e-2)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        \n",
    "        self.net,self.loss_fn,self.metrics_dict,self.optimizer = self.accelerator.prepare(\n",
    "            self.net,self.loss_fn,self.metrics_dict,self.optimizer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.net:\n",
    "            return self.net.forward(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint.pt', \n",
    "            patience=5, monitor=\"val_loss\", mode=\"min\"):\n",
    "        \n",
    "        train_data = self.accelerator.prepare(train_data)\n",
    "        val_data = self.accelerator.prepare(val_data) if val_data else []\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "            \n",
    "            # 1ï¼Œtrain -------------------------------------------------  \n",
    "            train_step_runner = StepRunner(net = self.net,stage=\"train\",\n",
    "                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n",
    "                    optimizer = self.optimizer, lr_scheduler = self.lr_scheduler,\n",
    "                    accelerator = self.accelerator)\n",
    "            train_epoch_runner = EpochRunner(train_step_runner)\n",
    "            train_metrics = train_epoch_runner(train_data)\n",
    "            \n",
    "            for name, metric in train_metrics.items():\n",
    "                self.history[name] = self.history.get(name, []) + [metric]\n",
    "\n",
    "            # 2ï¼Œvalidate -------------------------------------------------\n",
    "            if val_data:\n",
    "                val_step_runner = StepRunner(net = self.net,stage=\"val\",\n",
    "                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n",
    "                    accelerator = self.accelerator)\n",
    "                val_epoch_runner = EpochRunner(val_step_runner)\n",
    "                with torch.no_grad():\n",
    "                    val_metrics = val_epoch_runner(val_data)\n",
    "                val_metrics[\"epoch\"] = epoch\n",
    "                for name, metric in val_metrics.items():\n",
    "                    self.history[name] = self.history.get(name, []) + [metric]\n",
    "            \n",
    "            # 3ï¼Œearly-stopping -------------------------------------------------\n",
    "            arr_scores = self.history[monitor]\n",
    "            best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "            if best_score_idx==len(arr_scores)-1:\n",
    "                torch.save(self.net.state_dict(),ckpt_path)\n",
    "                print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                     arr_scores[best_score_idx]),file=sys.stderr)\n",
    "            if len(arr_scores)-best_score_idx>patience:\n",
    "                print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "                    monitor,patience),file=sys.stderr)\n",
    "                self.net.load_state_dict(torch.load(ckpt_path))\n",
    "                break \n",
    "            \n",
    "        return pd.DataFrame(self.history)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_data):\n",
    "        val_data = self.accelerator.prepare(val_data)\n",
    "        val_step_runner = StepRunner(net = self.net,stage=\"val\",\n",
    "                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n",
    "                    accelerator = self.accelerator)\n",
    "        val_epoch_runner = EpochRunner(val_step_runner)\n",
    "        val_metrics = val_epoch_runner(val_data)\n",
    "        return val_metrics\n",
    "        \n",
    "       \n",
    "    @torch.no_grad()\n",
    "    def predict(self, dataloader):\n",
    "        dataloader = self.accelerator.prepare(dataloader)\n",
    "        result = torch.cat([self.forward(t[0]) for t in dataloader])\n",
    "        return result.data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32761275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras.metrics import AUC\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "metrics_dict = {\"auc\":AUC()}\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.002, weight_decay=0.001) \n",
    "\n",
    "model = KerasModel(net,\n",
    "                   loss_fn = loss_fn,\n",
    "                   metrics_dict= metrics_dict,\n",
    "                   optimizer = optimizer\n",
    "                  )         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhistory = model.fit(train_data=dl_train,val_data=dl_val,epochs=100, patience=5,\n",
    "                      monitor = \"val_auc\",mode=\"max\",ckpt_path='checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5a1b8",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wiwm0e3gj20mn0f8goa.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bac7d",
   "metadata": {},
   "source": [
    "### 4ï¼Œè¯„ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a577c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[\"train_\"+metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b52108",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc1d36",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wizlzzsij20fv0a73yt.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory,\"auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f540023",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h2wiymw5nzj20fj0aaaad.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cb4cd",
   "metadata": {},
   "source": [
    "### 5ï¼Œä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "preds = torch.sigmoid(model.predict(dl_val))\n",
    "labels = torch.cat([x[-1] for x in dl_val])\n",
    "\n",
    "val_auc = roc_auc_score(labels.cpu().numpy(),preds.cpu().numpy())\n",
    "print(val_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79be59",
   "metadata": {},
   "source": [
    "0.7820486224544348\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d139d55",
   "metadata": {},
   "source": [
    "### 6ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.net.state_dict(),\"best_dcn.pt\")\n",
    "net_clone = create_net()\n",
    "net_clone.load_state_dict(torch.load(\"best_dcn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "net_clone.eval()\n",
    "preds = torch.cat([torch.sigmoid(net_clone(x[0])).data for x in dl_val]) \n",
    "labels = torch.cat([x[-1] for x in dl_val])\n",
    "\n",
    "val_auc = roc_auc_score(labels.cpu().numpy(),preds.cpu().numpy())\n",
    "print(val_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd06a8e",
   "metadata": {},
   "source": [
    "0.7820486196785761\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac90f13",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"ç®—æ³•ç¾é£Ÿå±‹\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚\n",
    "\n",
    "![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
