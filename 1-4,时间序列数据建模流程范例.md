---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# 1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹


2020å¹´å‘ç”Ÿçš„æ–°å† è‚ºç‚ç–«æƒ…ç¾éš¾ç»™å„å›½äººæ°‘çš„ç”Ÿæ´»é€ æˆäº†è¯¸å¤šæ–¹é¢çš„å½±å“ã€‚

æœ‰çš„åŒå­¦æ˜¯æ”¶å…¥ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯æ„Ÿæƒ…ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯å¿ƒç†ä¸Šçš„ï¼Œè¿˜æœ‰çš„åŒå­¦æ˜¯ä½“é‡ä¸Šçš„ã€‚

æœ¬æ–‡åŸºäºä¸­å›½2020å¹´3æœˆä¹‹å‰çš„ç–«æƒ…æ•°æ®ï¼Œå»ºç«‹æ—¶é—´åºåˆ—RNNæ¨¡å‹ï¼Œå¯¹ä¸­å›½çš„æ–°å† è‚ºç‚ç–«æƒ…ç»“æŸæ—¶é—´è¿›è¡Œé¢„æµ‹ã€‚


![](./data/ç–«æƒ…å‰åå¯¹æ¯”.png)

```python
!pip install torch==1.10.0
!pip install pytorch_lightning==1.6.5 

```

```python
import torch 
import pytorch_lightning
print("torch.__version__ = ", torch.__version__)
print("pytorch_lightning.__version__ = ", pytorch_lightning.__version__) 

```

```
torch.__version__ =  1.10.0
pytorch_lightning.__version__ =  1.6.5
```


<br>

<font color="red">
 
å…¬ä¼—å· **ç®—æ³•ç¾é£Ÿå±‹** å›å¤å…³é”®è¯ï¼š**pytorch**ï¼Œ è·å–æœ¬é¡¹ç›®æºç å’Œæ‰€ç”¨æ•°æ®é›†ç™¾åº¦äº‘ç›˜ä¸‹è½½é“¾æ¥ã€‚
    
</font> 


```python
import os
import datetime
import torchkeras

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 

```

```python

```

```python
!pip install torch==1.10.0
!pip install pytorch_lightning==1.6.5 

```

```python
import torch 
import pytorch_lightning
print("torch.__version__ = ", torch.__version__)
print("pytorch_lightning.__version__ = ", pytorch_lightning.__version__) 

```

```
torch.__version__ =  1.10.0
pytorch_lightning.__version__ =  1.6.5
```

```python

```


## ä¸€ï¼Œå‡†å¤‡æ•°æ®


æœ¬æ–‡çš„æ•°æ®é›†å–è‡ªtushareï¼Œè·å–è¯¥æ•°æ®é›†çš„æ–¹æ³•å‚è€ƒäº†ä»¥ä¸‹æ–‡ç« ã€‚

ã€Šhttps://zhuanlan.zhihu.com/p/109556102ã€‹

![](./data/1-4-æ–°å¢äººæ•°.png)


```python
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

df = pd.read_csv("./eat_pytorch_datasets/covid-19.csv",sep = "\t")
df.plot(x = "date",y = ["confirmed_num","cured_num","dead_num"],figsize=(10,6))
plt.xticks(rotation=60);

```

![](./data/1-4-ç´¯ç§¯æ›²çº¿.png)

```python
dfdata = df.set_index("date")
dfdiff = dfdata.diff(periods=1).dropna()
dfdiff = dfdiff.reset_index("date")

dfdiff.plot(x = "date",y = ["confirmed_num","cured_num","dead_num"],figsize=(10,6))
plt.xticks(rotation=60)
dfdiff = dfdiff.drop("date",axis = 1).astype("float32")

```

![](./data/1-4-æ–°å¢æ›²çº¿.png)

```python
dfdiff.head()
```

![](./data/1-4-dfdiff.png)


ä¸‹é¢æˆ‘ä»¬é€šè¿‡ç»§æ‰¿torch.utils.data.Datasetå®ç°è‡ªå®šä¹‰æ—¶é—´åºåˆ—æ•°æ®é›†ã€‚

torch.utils.data.Datasetæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œç”¨æˆ·æƒ³è¦åŠ è½½è‡ªå®šä¹‰çš„æ•°æ®åªéœ€è¦ç»§æ‰¿è¿™ä¸ªç±»ï¼Œå¹¶ä¸”è¦†å†™å…¶ä¸­çš„ä¸¤ä¸ªæ–¹æ³•å³å¯ï¼š

* `__len__`:å®ç°len(dataset)è¿”å›æ•´ä¸ªæ•°æ®é›†çš„å¤§å°ã€‚
* `__getitem__`:ç”¨æ¥è·å–ä¸€äº›ç´¢å¼•çš„æ•°æ®ï¼Œä½¿`dataset[i]`è¿”å›æ•°æ®é›†ä¸­ç¬¬iä¸ªæ ·æœ¬ã€‚

ä¸è¦†å†™è¿™ä¸¤ä¸ªæ–¹æ³•ä¼šç›´æ¥è¿”å›é”™è¯¯ã€‚


```python
import torch 
from torch import nn 
from torch.utils.data import Dataset,DataLoader,TensorDataset


#ç”¨æŸæ—¥å‰8å¤©çª—å£æ•°æ®ä½œä¸ºè¾“å…¥é¢„æµ‹è¯¥æ—¥æ•°æ®
WINDOW_SIZE = 8

class Covid19Dataset(Dataset):
        
    def __len__(self):
        return len(dfdiff) - WINDOW_SIZE
    
    def __getitem__(self,i):
        x = dfdiff.loc[i:i+WINDOW_SIZE-1,:]
        feature = torch.tensor(x.values)
        y = dfdiff.loc[i+WINDOW_SIZE,:]
        label = torch.tensor(y.values)
        return (feature,label)
    
ds_train = Covid19Dataset()

#æ•°æ®è¾ƒå°ï¼Œå¯ä»¥å°†å…¨éƒ¨è®­ç»ƒæ•°æ®æ”¾å…¥åˆ°ä¸€ä¸ªbatchä¸­ï¼Œæå‡æ€§èƒ½
dl_train = DataLoader(ds_train,batch_size = 38)

for features,labels in dl_train:
    break 
    
#dl_trainåŒæ—¶ä½œä¸ºéªŒè¯é›†
dl_val = dl_train

```

## äºŒï¼Œå®šä¹‰æ¨¡å‹


ä½¿ç”¨Pytorché€šå¸¸æœ‰ä¸‰ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ï¼Œç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨è¿›è¡Œå°è£…ã€‚

æ­¤å¤„é€‰æ‹©ç¬¬äºŒç§æ–¹å¼æ„å»ºæ¨¡å‹ã€‚



```python
import torch
from torch import nn 
import importlib 
import torchkeras 

torch.random.seed()

class Block(nn.Module):
    def __init__(self):
        super(Block,self).__init__()
    
    def forward(self,x,x_input):
        x_out = torch.max((1+x)*x_input[:,-1,:],torch.tensor(0.0))
        return x_out
    
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 3å±‚lstm
        self.lstm = nn.LSTM(input_size = 3,hidden_size = 3,num_layers = 5,batch_first = True)
        self.linear = nn.Linear(3,3)
        self.block = Block()
        
    def forward(self,x_input):
        x = self.lstm(x_input)[0][:,-1,:]
        x = self.linear(x)
        y = self.block(x,x_input)
        return y
        
net = Net()
print(net)


```

```
Net(
  (lstm): LSTM(3, 3, num_layers=5, batch_first=True)
  (linear): Linear(in_features=3, out_features=3, bias=True)
  (block): Block()
)
```

```python
from torchkeras import summary
summary(net,input_data=features);
```

```
--------------------------------------------------------------------------
Layer (type)                            Output Shape              Param #
==========================================================================
LSTM-1                                    [-1, 8, 3]                  480
Linear-2                                     [-1, 3]                   12
Block-3                                      [-1, 3]                    0
==========================================================================
Total params: 492
Trainable params: 492
Non-trainable params: 0
--------------------------------------------------------------------------
Input size (MB): 0.000069
Forward/backward pass size (MB): 0.000229
Params size (MB): 0.001877
Estimated Total Size (MB): 0.002174
--------------------------------------------------------------------------

```

```python

```

### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹


è®­ç»ƒPytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼šè„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œå‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œç±»å½¢å¼è®­ç»ƒå¾ªç¯ã€‚

æ­¤å¤„ä»‹ç»ä¸€ç§å¼•è¿›pytorch_lightningåº“å®ç°çš„ç±»å½¢å¼çš„è®­ç»ƒå¾ªç¯ã€‚

è¯¥è®­ç»ƒå¾ªç¯çš„ä»£ç ä¹Ÿæ˜¯torchkerasåº“ä¸­LightModelç±»çš„æ ¸å¿ƒä»£ç ã€‚

torchkerasè¯¦æƒ…:  https://github.com/lyhue1991/torchkeras 

æ³¨ï¼šå¾ªç¯ç¥ç»ç½‘ç»œè°ƒè¯•è¾ƒä¸ºå›°éš¾ï¼Œéœ€è¦è®¾ç½®å¤šä¸ªä¸åŒçš„å­¦ä¹ ç‡å¤šæ¬¡å°è¯•ï¼Œä»¥å–å¾—è¾ƒå¥½çš„æ•ˆæœã€‚


```python

```

```python
import torch 
from torch import nn 
import pytorch_lightning as pl
import datetime
import sys 
import numpy as np
import pandas as pd 
from copy import deepcopy

class LightModel(pl.LightningModule):
    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler=None):
        super().__init__()
        self.net = net
        self.history = {}
        
        self.train_metrics = nn.ModuleDict(metrics_dict)
        self.val_metrics = deepcopy(self.train_metrics)
        self.test_metrics = deepcopy(self.train_metrics)
        
        self.loss_fn = loss_fn
        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(self.parameters(), lr=1e-2)
        self.lr_scheduler = lr_scheduler 
        
        for p in ["net","loss_fn","metrics_dict","optimizer","lr_scheduler"]:
            self.save_hyperparameters(p)
        
    def forward(self,x):
        if self.net:
            return self.net.forward(x)
        else:
            raise NotImplementedError
            
    def shared_step(self,batch,batch_idx):
        x, y = batch
        preds = self(x)
        loss = self.loss_fn(preds,y)
        return {'loss': loss, 'preds': preds.detach(), 'y': y.detach()}
    
    def configure_optimizers(self):
        if self.lr_scheduler is None:
            return self.optimizer
        return {"optimizer":self.optimizer,"lr_scheduler":self.lr_scheduler}
    
    def training_step(self, batch, batch_idx):
        return self.shared_step(batch,batch_idx)
    
    def validation_step(self, batch, batch_idx):
        return self.shared_step(batch,batch_idx)
    
    def test_step(self, batch, batch_idx):
        return self.shared_step(batch,batch_idx)
    
    def predict_step(self, batch, batch_idx):
        if isinstance(batch,list) and len(batch)==2:
            return self(batch[0])
        else:
            return self(batch)
        
    def shared_step_end(self,outputs,stage):
        metrics = self.train_metrics if stage=="train" else (
            self.val_metrics if stage=="val" else self.test_metrics)
        for name in metrics:
            step_metric = metrics[name](outputs['preds'], outputs['y']).item()
            if stage=="train":
                self.log(name,step_metric,prog_bar=True)
        return outputs["loss"].mean()
        
    def training_step_end(self, outputs):
        return {'loss':self.shared_step_end(outputs,"train")}
            
    def validation_step_end(self, outputs):
        return {'val_loss':self.shared_step_end(outputs,"val")}
            
    def test_step_end(self, outputs):
        return {'test_loss':self.shared_step_end(outputs,"test")}
            
    def shared_epoch_end(self,outputs,stage="train"):
        metrics = self.train_metrics if stage=="train" else (
            self.val_metrics if stage=="val" else self.test_metrics)
        
        epoch = self.trainer.current_epoch
        stage_loss = torch.mean(torch.tensor([t[(stage+"_loss").replace('train_','')] for t in outputs])).item()
        dic = {"epoch":epoch,stage+"_loss":stage_loss}
        
        for name in metrics:
            epoch_metric = metrics[name].compute().item() 
            metrics[name].reset()
            dic[stage+"_"+name] = epoch_metric 
        if stage!='test':
            self.history[epoch] = dict(self.history.get(epoch,{}),**dic)    
        return dic 
    
    def training_epoch_end(self, outputs):
        dic = self.shared_epoch_end(outputs,stage="train")
        self.print(dic)
        dic.pop("epoch",None)
        self.log_dict(dic, logger=True)

    def validation_epoch_end(self, outputs):
        dic = self.shared_epoch_end(outputs,stage="val")
        self.print_bar()
        self.print(dic)
        dic.pop("epoch",None)
        self.log_dict(dic, logger=True)
        
        #log when reach best score
        ckpt_cb = self.trainer.checkpoint_callback
        monitor = ckpt_cb.monitor 
        mode = ckpt_cb.mode 
        arr_scores = self.get_history()[monitor]
        best_score_idx = np.argmax(arr_scores) if mode=="max" else np.argmin(arr_scores)
        if best_score_idx==len(arr_scores)-1:   
            self.print("<<<<<< reach best {0} : {1} >>>>>>".format(monitor,
                 arr_scores[best_score_idx]),file=sys.stderr)
            
    
    def test_epoch_end(self, outputs):
        dic = self.shared_epoch_end(outputs,stage="test")
        dic.pop("epoch",None)
        self.print(dic)
        self.log_dict(dic, logger=True)
        
    def get_history(self):
        return pd.DataFrame(self.history.values()) 
    
    def print_bar(self): 
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        self.print("\n"+"="*80 + "%s"%nowtime)
        
```

```python
from torchmetrics.regression import MeanAbsolutePercentageError

def mspe(y_pred,y_true):
    err_percent = (y_true - y_pred)**2/(torch.max(y_true**2,torch.tensor(1e-7)))
    return torch.mean(err_percent)


net = Net() 
loss_fn = mspe
metric_dict = {"mape":MeanAbsolutePercentageError()}

optimizer = torch.optim.Adam(net.parameters(), lr=0.03)
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.0001)

model = LightModel(net,
       loss_fn = loss_fn,
       metrics_dict= metric_dict,
       optimizer = optimizer,
       lr_scheduler = lr_scheduler)       

```

```python
import pytorch_lightning as pl     

#1ï¼Œè®¾ç½®å›è°ƒå‡½æ•°

model_ckpt = pl.callbacks.ModelCheckpoint(
    monitor='val_mape',
    save_top_k=1,
    mode='min'
)

early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_mape',
                           patience=3,
                           mode = 'min'
                          )

#2ï¼Œè®¾ç½®è®­ç»ƒå‚æ•°
# gpus=0 åˆ™ä½¿ç”¨cpuè®­ç»ƒï¼Œgpus=1åˆ™ä½¿ç”¨1ä¸ªgpuè®­ç»ƒï¼Œgpus=2åˆ™ä½¿ç”¨2ä¸ªgpuè®­ç»ƒï¼Œgpus=-1åˆ™ä½¿ç”¨æ‰€æœ‰gpuè®­ç»ƒï¼Œ
# gpus=[0,1]åˆ™æŒ‡å®šä½¿ç”¨0å·å’Œ1å·gpuè®­ç»ƒï¼Œ gpus="0,1,2,3"åˆ™ä½¿ç”¨0,1,2,3å·gpuè®­ç»ƒ
# tpus=1 åˆ™ä½¿ç”¨1ä¸ªtpuè®­ç»ƒ
trainer = pl.Trainer(logger=True,
                     min_epochs=3,max_epochs=30,
                     gpus=0,
                     callbacks = [model_ckpt,early_stopping],
                     enable_progress_bar = True) 


##3ï¼Œå¯åŠ¨è®­ç»ƒå¾ªç¯
trainer.fit(model,dl_train,dl_val)

```

```python
dfhistory = model.get_history() 
dfhistory 
```

```
epoch	val_loss	val_mape	train_loss	train_mape
0	0	5.974455	0.661542	6.936645	0.737031
1	1	5.086240	0.590996	5.974455	0.661542
2	2	4.237600	0.524000	5.086240	0.590996
3	3	3.408828	0.463179	4.237600	0.524000
4	4	2.614143	0.422679	3.408828	0.463179
5	5	1.896354	0.413116	2.614143	0.422679
6	6	1.304007	0.437700	1.896354	0.413116
7	7	0.866170	0.474878	1.304007	0.437700
8	8	0.585183	0.517995	0.866170	0.474878
	
```

```python

```

### å››ï¼Œè¯„ä¼°æ¨¡å‹


è¯„ä¼°æ¨¡å‹ä¸€èˆ¬è¦è®¾ç½®éªŒè¯é›†æˆ–è€…æµ‹è¯•é›†ï¼Œç”±äºæ­¤ä¾‹æ•°æ®è¾ƒå°‘ï¼Œæˆ‘ä»¬ä»…ä»…å¯è§†åŒ–æŸå¤±å‡½æ•°åœ¨è®­ç»ƒé›†ä¸Šçš„è¿­ä»£æƒ…å†µã€‚

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory["train_"+metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()

```

```python
#ä½¿ç”¨æœ€ä½³ä¿å­˜ç‚¹è¿›è¡Œè¯„ä¼°
trainer.test(ckpt_path='best', dataloaders=dl_val,verbose = False)

```

```
{'test_loss': 1.8963541984558105, 'test_mape': 0.4131162464618683}
```

```python
plot_metric(dfhistory,"loss")
```

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h48rhp9jpqj20ej0acwer.jpg)

```python
plot_metric(dfhistory,"mape")
```

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h48rj15ctvj20f70ag3yv.jpg)

```python

```

### äº”ï¼Œä½¿ç”¨æ¨¡å‹


æ­¤å¤„æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹é¢„æµ‹ç–«æƒ…ç»“æŸæ—¶é—´ï¼Œå³ æ–°å¢ç¡®è¯Šç—…ä¾‹ä¸º0 çš„æ—¶é—´ã€‚

```python
#ä½¿ç”¨dfresultè®°å½•ç°æœ‰æ•°æ®ä»¥åŠæ­¤åé¢„æµ‹çš„ç–«æƒ…æ•°æ®
dfresult = dfdiff[["confirmed_num","cured_num","dead_num"]].copy()
dfresult.tail()
```

![](./data/1-4-æ—¥æœŸ3æœˆ10.png)

```python
#é¢„æµ‹æ­¤å1000å¤©çš„æ–°å¢èµ°åŠ¿,å°†å…¶ç»“æœæ·»åŠ åˆ°dfresultä¸­
for i in range(1000):
    arr_input = torch.unsqueeze(torch.from_numpy(dfresult.values[-38:,:]),axis=0)
    arr_predict = model.forward(arr_input)

    dfpredict = pd.DataFrame(torch.floor(arr_predict).data.numpy(),
                columns = dfresult.columns)
    dfresult = pd.concat([dfresult,dfpredict],ignore_index=True)
```

```python
dfresult.query("confirmed_num==0").head()

# ç¬¬50å¤©å¼€å§‹æ–°å¢ç¡®è¯Šé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯5å¤©åï¼Œå³é¢„è®¡3æœˆ15æ—¥æ–°å¢ç¡®è¯Šé™ä¸º0
# æ³¨ï¼šè¯¥é¢„æµ‹åä¹è§‚
```

![](./data/1-4-torché¢„æµ‹ç¡®è¯Š.png)

```python

```

```python
dfresult.query("cured_num==0").head()

# ç¬¬132å¤©å¼€å§‹æ–°å¢æ²»æ„ˆé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚3ä¸ªæœˆåï¼Œå³6æœˆ10æ—¥å·¦å³å…¨éƒ¨æ²»æ„ˆã€‚
# æ³¨: è¯¥é¢„æµ‹åæ‚²è§‚ï¼Œå¹¶ä¸”å­˜åœ¨é—®é¢˜ï¼Œå¦‚æœå°†æ¯å¤©æ–°å¢æ²»æ„ˆäººæ•°åŠ èµ·æ¥ï¼Œå°†è¶…è¿‡ç´¯è®¡ç¡®è¯Šäººæ•°ã€‚
```

![](./data/1-4-torché¢„æµ‹æ²»æ„ˆ.png)

```python

```

### å…­ï¼Œä¿å­˜æ¨¡å‹


æ¨¡å‹ä¿å­˜åœ¨äº†trainer.checkpoint_callback.best_model_pathè·¯å¾„ã€‚


```python
print(trainer.checkpoint_callback.best_model_path)
print(trainer.checkpoint_callback.best_model_score)
```

```python
#pytorch_lightningä¸ä»…ä¿ç•™äº†æ¨¡å‹å‚æ•°ï¼Œè¿˜ä¿å­˜äº†æ¨¡å‹ç»“æ„ï¼Œå¯ä»¥ç”¨LightModelé‡æ–°åŠ è½½
model_loaded = LightModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)
```

```python
trainer.predict(model_loaded,dataloaders=dl_val)
```

```
[tensor([[1.4974e+03, 8.5825e+01, 2.3000e+01],
         [1.8469e+03, 6.7295e+01, 4.6768e+01],
         [2.0153e+03, 1.4337e+02, 4.3701e+01],
         [2.3063e+03, 1.5312e+02, 4.9068e+01],
         [2.7721e+03, 2.5358e+02, 4.9834e+01],
         [2.6352e+03, 2.5455e+02, 5.5968e+01],
         [2.2421e+03, 3.7744e+02, 5.5968e+01],
         [2.4147e+03, 4.9740e+02, 6.5935e+01],
         [1.8918e+03, 5.8420e+02, 6.8235e+01],
         [2.1208e+03, 6.1638e+02, 7.4368e+01],
         [1.7599e+03, 6.9733e+02, 8.2802e+01],
         [1.4374e+03, 7.2561e+02, 7.4368e+01],
         [1.0808e+04, 1.1421e+03, 1.9474e+02],
         [2.8870e+03, 7.9193e+02, 9.9669e+00],
         [1.8840e+03, 1.3391e+03, 1.0964e+02],
         [1.4324e+03, 1.2903e+03, 1.0887e+02],
         [1.4610e+03, 1.3898e+03, 8.0502e+01],
         [1.3468e+03, 1.6658e+03, 7.5135e+01],
         [1.2477e+03, 1.7789e+03, 1.0427e+02],
         [2.7895e+02, 1.7353e+03, 8.7424e+01],
         [6.3425e+02, 2.0573e+03, 9.0511e+01],
         [5.8716e+02, 2.3365e+03, 8.3624e+01],
         [4.6230e+02, 2.1747e+03, 7.4431e+01],
         [1.5267e+02, 1.8011e+03, 1.1512e+02],
         [3.6241e+02, 2.5261e+03, 5.4495e+01],
         [2.8963e+02, 2.3633e+03, 3.9916e+01],
         [3.0889e+02, 2.6834e+03, 2.2262e+01],
         [2.3327e+02, 3.5342e+03, 3.3777e+01],
         [3.0461e+02, 2.8151e+03, 3.6080e+01],
         [4.0876e+02, 2.5594e+03, 2.6868e+01],
         [1.4410e+02, 2.7683e+03, 3.2242e+01],
         [8.9172e+01, 2.6756e+03, 2.3798e+01],
         [8.4892e+01, 2.5877e+03, 2.9171e+01],
         [9.9159e+01, 2.1360e+03, 2.3798e+01],
         [1.0201e+02, 1.6403e+03, 2.3030e+01],
         [7.0624e+01, 1.6373e+03, 2.1495e+01],
         [3.1389e+01, 1.6208e+03, 2.0727e+01],
         [2.8535e+01, 1.4978e+03, 1.6889e+01]])]
```


**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)
