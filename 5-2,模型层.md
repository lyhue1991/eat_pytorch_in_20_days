---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# 5-2,æ¨¡å‹å±‚layers

æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€èˆ¬ç”±å„ç§æ¨¡å‹å±‚ç»„åˆè€Œæˆã€‚

torch.nnä¸­å†…ç½®äº†éå¸¸ä¸°å¯Œçš„å„ç§æ¨¡å‹å±‚ã€‚å®ƒä»¬éƒ½å±äºnn.Moduleçš„å­ç±»ï¼Œå…·å¤‡å‚æ•°ç®¡ç†åŠŸèƒ½ã€‚

ä¾‹å¦‚ï¼š

* nn.Linear, nn.Flatten, nn.Dropout, nn.BatchNorm2d, nn.Embedding

* nn.Conv2d,nn.AvgPool2d,nn.Conv1d,nn.ConvTranspose2d

* nn.GRU,nn.LSTM

* nn.Transformer

å¦‚æœè¿™äº›å†…ç½®æ¨¡å‹å±‚ä¸èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰çš„æ¨¡å‹å±‚ã€‚

å®é™…ä¸Šï¼Œpytorchä¸åŒºåˆ†æ¨¡å‹å’Œæ¨¡å‹å±‚ï¼Œéƒ½æ˜¯é€šè¿‡ç»§æ‰¿nn.Moduleè¿›è¡Œæ„å»ºã€‚

å› æ­¤ï¼Œæˆ‘ä»¬åªè¦ç»§æ‰¿nn.ModuleåŸºç±»å¹¶å®ç°forwardæ–¹æ³•å³å¯è‡ªå®šä¹‰æ¨¡å‹å±‚ã€‚


```python

```

## ä¸€ï¼ŒåŸºç¡€å±‚

<!-- #region -->
ä¸€äº›åŸºç¡€çš„å†…ç½®æ¨¡å‹å±‚ç®€å•ä»‹ç»å¦‚ä¸‹ã€‚

* nn.Linearï¼šå…¨è¿æ¥å±‚ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥å±‚ç‰¹å¾æ•°Ã— è¾“å‡ºå±‚ç‰¹å¾æ•°(weight)ï¼‹ è¾“å‡ºå±‚ç‰¹å¾æ•°(bias)

* nn.Embeddingï¼šåµŒå…¥å±‚ã€‚ä¸€ç§æ¯”Onehotæ›´åŠ æœ‰æ•ˆçš„å¯¹ç¦»æ•£ç‰¹å¾è¿›è¡Œç¼–ç çš„æ–¹æ³•ã€‚ä¸€èˆ¬ç”¨äºå°†è¾“å…¥ä¸­çš„å•è¯æ˜ å°„ä¸ºç¨ å¯†å‘é‡ã€‚åµŒå…¥å±‚çš„å‚æ•°éœ€è¦å­¦ä¹ ã€‚

* nn.Flattenï¼šå‹å¹³å±‚ï¼Œç”¨äºå°†å¤šç»´å¼ é‡æ ·æœ¬å‹æˆä¸€ç»´å¼ é‡æ ·æœ¬ã€‚

* nn.BatchNorm1dï¼šä¸€ç»´æ‰¹æ ‡å‡†åŒ–å±‚ã€‚é€šè¿‡çº¿æ€§å˜æ¢å°†è¾“å…¥æ‰¹æ¬¡ç¼©æ”¾å¹³ç§»åˆ°ç¨³å®šçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚å¯ä»¥å¢å¼ºæ¨¡å‹å¯¹è¾“å…¥ä¸åŒåˆ†å¸ƒçš„é€‚åº”æ€§ï¼ŒåŠ å¿«æ¨¡å‹è®­ç»ƒé€Ÿåº¦ï¼Œæœ‰è½»å¾®æ­£åˆ™åŒ–æ•ˆæœã€‚ä¸€èˆ¬åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰ä½¿ç”¨ã€‚å¯ä»¥ç”¨afineå‚æ•°è®¾ç½®è¯¥å±‚æ˜¯å¦å«æœ‰å¯ä»¥è®­ç»ƒçš„å‚æ•°ã€‚

* nn.BatchNorm2dï¼šäºŒç»´æ‰¹æ ‡å‡†åŒ–å±‚ã€‚ å¸¸ç”¨äºCVé¢†åŸŸã€‚

* nn.BatchNorm3dï¼šä¸‰ç»´æ‰¹æ ‡å‡†åŒ–å±‚ã€‚

* nn.Dropoutï¼šä¸€ç»´éšæœºä¸¢å¼ƒå±‚ã€‚ä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µã€‚

* nn.Dropout2dï¼šäºŒç»´éšæœºä¸¢å¼ƒå±‚ã€‚

* nn.Dropout3dï¼šä¸‰ç»´éšæœºä¸¢å¼ƒå±‚ã€‚

* nn.Thresholdï¼šé™å¹…å±‚ã€‚å½“è¾“å…¥å¤§äºæˆ–å°äºé˜ˆå€¼èŒƒå›´æ—¶ï¼Œæˆªæ–­ä¹‹ã€‚

* nn.ConstantPad2dï¼š äºŒç»´å¸¸æ•°å¡«å……å±‚ã€‚å¯¹äºŒç»´å¼ é‡æ ·æœ¬å¡«å……å¸¸æ•°æ‰©å±•é•¿åº¦ã€‚

* nn.ReplicationPad1dï¼š ä¸€ç»´å¤åˆ¶å¡«å……å±‚ã€‚å¯¹ä¸€ç»´å¼ é‡æ ·æœ¬é€šè¿‡å¤åˆ¶è¾¹ç¼˜å€¼å¡«å……æ‰©å±•é•¿åº¦ã€‚

* nn.ZeroPad2dï¼šäºŒç»´é›¶å€¼å¡«å……å±‚ã€‚å¯¹äºŒç»´å¼ é‡æ ·æœ¬åœ¨è¾¹ç¼˜å¡«å……0å€¼.

* nn.GroupNormï¼šç»„å½’ä¸€åŒ–ã€‚ä¸€ç§æ›¿ä»£æ‰¹å½’ä¸€åŒ–çš„æ–¹æ³•ï¼Œå°†é€šé“åˆ†æˆè‹¥å¹²ç»„è¿›è¡Œå½’ä¸€ã€‚ä¸å—batchå¤§å°é™åˆ¶ã€‚

* nn.LayerNormï¼šå±‚å½’ä¸€åŒ–ã€‚å¸¸ç”¨äºNLPé¢†åŸŸï¼Œä¸å—åºåˆ—é•¿åº¦ä¸ä¸€è‡´å½±å“ã€‚

* nn.InstanceNorm2d: æ ·æœ¬å½’ä¸€åŒ–ã€‚ä¸€èˆ¬åœ¨å›¾åƒé£æ ¼è¿ç§»ä»»åŠ¡ä¸­æ•ˆæœè¾ƒå¥½ã€‚



é‡ç‚¹è¯´è¯´å„ç§å½’ä¸€åŒ–å±‚ï¼š

$$y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$$



* ç»“æ„åŒ–æ•°æ®çš„BatchNorm1Då½’ä¸€åŒ– ã€ç»“æ„åŒ–æ•°æ®çš„ä¸»è¦åŒºåˆ†åº¦æ¥è‡ªæ¯ä¸ªæ ·æœ¬ç‰¹å¾åœ¨å…¨ä½“æ ·æœ¬ä¸­çš„æ’åºï¼Œå°†å…¨éƒ¨æ ·æœ¬çš„æŸä¸ªç‰¹å¾éƒ½è¿›è¡Œç›¸åŒçš„æ”¾å¤§ç¼©å°å¹³ç§»æ“ä½œï¼Œæ ·æœ¬é—´çš„åŒºåˆ†åº¦åŸºæœ¬ä¿æŒä¸å˜ï¼Œæ‰€ä»¥ç»“æ„åŒ–æ•°æ®å¯ä»¥åšBatchNormï¼Œä½†LayerNormä¼šæ‰“ä¹±å…¨ä½“æ ·æœ¬æ ¹æ®æŸä¸ªç‰¹å¾çš„æ’åºå…³ç³»ï¼Œå¼•èµ·åŒºåˆ†åº¦ä¸‹é™ã€‘


![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5mbd2ill5j20a808z0ta.jpg)


* å›¾ç‰‡æ•°æ®çš„å„ç§å½’ä¸€åŒ–(ä¸€èˆ¬å¸¸ç”¨BatchNorm2D)ã€å›¾ç‰‡æ•°æ®çš„ä¸»è¦åŒºåˆ†åº¦æ¥è‡ªå›¾ç‰‡ä¸­çš„çº¹ç†ç»“æ„ï¼Œæ‰€ä»¥å›¾ç‰‡æ•°æ®çš„å½’ä¸€åŒ–ä¸€å®šè¦åœ¨å›¾ç‰‡çš„å®½é«˜æ–¹å‘ä¸Šæ“ä½œä»¥ä¿æŒçº¹ç†ç»“æ„ï¼Œæ­¤å¤–åœ¨Batchç»´åº¦ä¸Šæ“ä½œè¿˜èƒ½å¤Ÿå¼•å…¥å°‘è®¸çš„æ­£åˆ™åŒ–ï¼Œå¯¹æå‡ç²¾åº¦æœ‰è¿›ä¸€æ­¥çš„å¸®åŠ©ã€‚ã€‘


![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5m92dtnd0j20tn07ztab.jpg)





* æ–‡æœ¬æ•°æ®çš„LayerNormå½’ä¸€åŒ– ã€æ–‡æœ¬æ•°æ®çš„ä¸»è¦åŒºåˆ†åº¦æ¥è‡ªäºè¯å‘é‡(Embeddingå‘é‡)çš„æ–¹å‘ï¼Œæ‰€ä»¥æ–‡æœ¬æ•°æ®çš„å½’ä¸€åŒ–ä¸€å®šè¦åœ¨ ç‰¹å¾(é€šé“)ç»´åº¦ä¸Šæ“ä½œ ä»¥ä¿æŒ è¯å‘é‡æ–¹å‘ä¸å˜ã€‚æ­¤å¤–æ–‡æœ¬æ•°æ®è¿˜æœ‰ä¸€ä¸ªé‡è¦çš„ç‰¹ç‚¹æ˜¯ä¸åŒæ ·æœ¬çš„åºåˆ—é•¿åº¦å¾€å¾€ä¸ä¸€æ ·ï¼Œæ‰€ä»¥ä¸å¯ä»¥åœ¨Sequenceå’ŒBatchç»´åº¦ä¸Šåšå½’ä¸€åŒ–ï¼Œå¦åˆ™å°†ä¸å¯é¿å…åœ°è®©paddingä½ç½®å¯¹åº”çš„å‘é‡å˜æˆéé›¶å‘é‡ã€‘

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5m903lv0nj20jc0iawfx.jpg)



* æ­¤å¤–ï¼Œæœ‰è®ºæ–‡æå‡ºäº†ä¸€ç§å¯è‡ªé€‚åº”å­¦ä¹ çš„å½’ä¸€åŒ–ï¼šSwitchableNormï¼Œå¯åº”ç”¨äºå„ç§åœºæ™¯ä¸”æœ‰ä¸€å®šçš„æ•ˆæœæå‡ã€‚ã€SwitchableNormæ˜¯å°†BNã€LNã€INç»“åˆï¼Œèµ‹äºˆæƒé‡ï¼Œè®©ç½‘ç»œè‡ªå·±å»å­¦ä¹ å½’ä¸€åŒ–å±‚åº”è¯¥ä½¿ç”¨ä»€ä¹ˆæ–¹æ³•ã€‚ã€‘

å‚è€ƒè®ºæ–‡ï¼šhttps://arxiv.org/pdf/1806.10779.pdf




å¯¹BatchNorméœ€è¦æ³¨æ„çš„å‡ ç‚¹ï¼š

(1)BatchNormæ”¾åœ¨æ¿€æ´»å‡½æ•°å‰è¿˜æ˜¯æ¿€æ´»å‡½æ•°åï¼Ÿ

åŸå§‹è®ºæ–‡è®¤ä¸ºå°†BatchNormæ”¾åœ¨æ¿€æ´»å‡½æ•°å‰æ•ˆæœè¾ƒå¥½ï¼Œåé¢çš„ç ”ç©¶ä¸€èˆ¬è®¤ä¸ºå°†BatchNormæ”¾åœ¨æ¿€æ´»å‡½æ•°ä¹‹åæ›´å¥½ã€‚

(2)BatchNormåœ¨è®­ç»ƒè¿‡ç¨‹å’Œæ¨ç†è¿‡ç¨‹çš„é€»è¾‘æ˜¯å¦ä¸€æ ·ï¼Ÿ

ä¸ä¸€æ ·ï¼è®­ç»ƒè¿‡ç¨‹BatchNormçš„å‡å€¼å’Œæ–¹å·®å’Œæ ¹æ®mini-batchä¸­çš„æ•°æ®ä¼°è®¡çš„ï¼Œè€Œæ¨ç†è¿‡ç¨‹ä¸­BatchNormçš„å‡å€¼å’Œæ–¹å·®æ˜¯ç”¨çš„è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…¨ä½“æ ·æœ¬ä¼°è®¡çš„ã€‚å› æ­¤é¢„æµ‹è¿‡ç¨‹æ˜¯ç¨³å®šçš„ï¼Œç›¸åŒçš„æ ·æœ¬ä¸ä¼šå› ä¸ºæ‰€åœ¨æ‰¹æ¬¡çš„å·®å¼‚å¾—åˆ°ä¸åŒçš„ç»“æœï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä¸­åˆ™ä¼šå—åˆ°æ‰¹æ¬¡ä¸­å…¶ä»–æ ·æœ¬çš„å½±å“æ‰€ä»¥æœ‰æ­£åˆ™åŒ–æ•ˆæœã€‚

(3)BatchNormçš„ç²¾åº¦æ•ˆæœä¸batch_sizeå¤§å°æœ‰ä½•å…³ç³»? 

å¦‚æœå—åˆ°GPUå†…å­˜é™åˆ¶ï¼Œä¸å¾—ä¸ä½¿ç”¨å¾ˆå°çš„batch_sizeï¼Œè®­ç»ƒé˜¶æ®µæ—¶ä½¿ç”¨çš„mini-batchä¸Šçš„å‡å€¼å’Œæ–¹å·®çš„ä¼°è®¡å’Œé¢„æµ‹é˜¶æ®µæ—¶ä½¿ç”¨çš„å…¨ä½“æ ·æœ¬ä¸Šçš„å‡å€¼å’Œæ–¹å·®çš„ä¼°è®¡å·®å¼‚å¯èƒ½ä¼šè¾ƒå¤§ï¼Œæ•ˆæœä¼šå˜å·®ã€‚è¿™æ—¶å€™ï¼Œå¯ä»¥å°è¯•LayerNormæˆ–è€…GroupNormç­‰å½’ä¸€åŒ–æ–¹æ³•ã€‚


<!-- #endregion -->

```python
import torch 
from torch import nn 

batch_size, channel, height, width = 32, 16, 128, 128

tensor = torch.arange(0,32*16*128*128).view(32,16,128,128).float() 

bn = nn.BatchNorm2d(num_features=channel,affine=False)
bn_out = bn(tensor)


channel_mean = torch.mean(bn_out[:,0,:,:]) 
channel_std = torch.std(bn_out[:,0,:,:])
print("channel mean:",channel_mean.item())
print("channel std:",channel_std.item())


```

```python
import torch 
from torch import nn 

batch_size, sequence, features = 32, 100, 2048
tensor = torch.arange(0,32*100*2048).view(32,100,2048).float() 

ln = nn.LayerNorm(normalized_shape=[features],
                  elementwise_affine = False)

ln_out = ln(tensor)

token_mean = torch.mean(ln_out[0,0,:]) 
token_std = torch.std(ln_out[0,0,:])
print("token_mean:",token_mean.item())
print("token_mean:",token_std.item())


```

```python

```

## äºŒï¼Œå·ç§¯ç½‘ç»œç›¸å…³å±‚



ä¸€äº›ä¸å·ç§¯ç›¸å…³çš„å†…ç½®å±‚ä»‹ç»å¦‚ä¸‹

* nn.Conv1dï¼šæ™®é€šä¸€ç»´å·ç§¯ï¼Œå¸¸ç”¨äºæ–‡æœ¬ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3)Ã—å·ç§¯æ ¸ä¸ªæ•° + å·ç§¯æ ¸å°ºå¯¸(å¦‚3ï¼‰
  
* nn.Conv2dï¼šæ™®é€šäºŒç»´å·ç§¯ï¼Œå¸¸ç”¨äºå›¾åƒã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•° + å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3)ã€‚) é€šè¿‡è°ƒæ•´dilationå‚æ•°å¤§äº1ï¼Œå¯ä»¥å˜æˆç©ºæ´å·ç§¯ï¼Œå¢åŠ æ„Ÿå—é‡ã€‚ é€šè¿‡è°ƒæ•´groupså‚æ•°ä¸ä¸º1ï¼Œå¯ä»¥å˜æˆåˆ†ç»„å·ç§¯ã€‚åˆ†ç»„å·ç§¯ä¸­æ¯ä¸ªå·ç§¯æ ¸ä»…å¯¹å…¶å¯¹åº”çš„ä¸€ä¸ªåˆ†ç»„è¿›è¡Œæ“ä½œã€‚ å½“groupså‚æ•°æ•°é‡ç­‰äºè¾“å…¥é€šé“æ•°æ—¶ï¼Œç›¸å½“äºtensorflowä¸­çš„äºŒç»´æ·±åº¦å·ç§¯å±‚tf.keras.layers.DepthwiseConv2Dã€‚ åˆ©ç”¨åˆ†ç»„å·ç§¯å’Œ1ä¹˜1å·ç§¯çš„ç»„åˆæ“ä½œï¼Œå¯ä»¥æ„é€ ç›¸å½“äºKerasä¸­çš„äºŒç»´æ·±åº¦å¯åˆ†ç¦»å·ç§¯å±‚tf.keras.layers.SeparableConv2Dã€‚

* nn.Conv3dï¼šæ™®é€šä¸‰ç»´å·ç§¯ï¼Œå¸¸ç”¨äºè§†é¢‘ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•° + å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3ä¹˜3) ã€‚

* nn.MaxPool1d: ä¸€ç»´æœ€å¤§æ± åŒ–ã€‚

* nn.MaxPool2dï¼šäºŒç»´æœ€å¤§æ± åŒ–ã€‚ä¸€ç§ä¸‹é‡‡æ ·æ–¹å¼ã€‚æ²¡æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°ã€‚

* nn.MaxPool3dï¼šä¸‰ç»´æœ€å¤§æ± åŒ–ã€‚

* nn.AdaptiveMaxPool2dï¼šäºŒç»´è‡ªé€‚åº”æœ€å¤§æ± åŒ–ã€‚æ— è®ºè¾“å…¥å›¾åƒçš„å°ºå¯¸å¦‚ä½•å˜åŒ–ï¼Œè¾“å‡ºçš„å›¾åƒå°ºå¯¸æ˜¯å›ºå®šçš„ã€‚
  è¯¥å‡½æ•°çš„å®ç°åŸç†ï¼Œå¤§æ¦‚æ˜¯é€šè¿‡è¾“å…¥å›¾åƒçš„å°ºå¯¸å’Œè¦å¾—åˆ°çš„è¾“å‡ºå›¾åƒçš„å°ºå¯¸æ¥åå‘æ¨ç®—æ± åŒ–ç®—å­çš„padding,strideç­‰å‚æ•°ã€‚
  
* nn.FractionalMaxPool2dï¼šäºŒç»´åˆ†æ•°æœ€å¤§æ± åŒ–ã€‚æ™®é€šæœ€å¤§æ± åŒ–é€šå¸¸è¾“å…¥å°ºå¯¸æ˜¯è¾“å‡ºçš„æ•´æ•°å€ã€‚è€Œåˆ†æ•°æœ€å¤§æ± åŒ–åˆ™å¯ä»¥ä¸å¿…æ˜¯æ•´æ•°ã€‚åˆ†æ•°æœ€å¤§æ± åŒ–ä½¿ç”¨äº†ä¸€äº›éšæœºé‡‡æ ·ç­–ç•¥ï¼Œæœ‰ä¸€å®šçš„æ­£åˆ™æ•ˆæœï¼Œå¯ä»¥ç”¨å®ƒæ¥ä»£æ›¿æ™®é€šæœ€å¤§æ± åŒ–å’ŒDropoutå±‚ã€‚

* nn.AvgPool2dï¼šäºŒç»´å¹³å‡æ± åŒ–ã€‚

* nn.AdaptiveAvgPool2dï¼šäºŒç»´è‡ªé€‚åº”å¹³å‡æ± åŒ–ã€‚æ— è®ºè¾“å…¥çš„ç»´åº¦å¦‚ä½•å˜åŒ–ï¼Œè¾“å‡ºçš„ç»´åº¦æ˜¯å›ºå®šçš„ã€‚

* nn.ConvTranspose2dï¼šäºŒç»´å·ç§¯è½¬ç½®å±‚ï¼Œä¿—ç§°åå·ç§¯å±‚ã€‚å¹¶éå·ç§¯çš„é€†æ“ä½œï¼Œä½†åœ¨å·ç§¯æ ¸ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå½“å…¶è¾“å…¥å°ºå¯¸æ˜¯å·ç§¯æ“ä½œè¾“å‡ºå°ºå¯¸çš„æƒ…å†µä¸‹ï¼Œå·ç§¯è½¬ç½®çš„è¾“å‡ºå°ºå¯¸æ°å¥½æ˜¯å·ç§¯æ“ä½œçš„è¾“å…¥å°ºå¯¸ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä¸­å¯ç”¨äºä¸Šé‡‡æ ·ã€‚

* nn.Upsampleï¼šä¸Šé‡‡æ ·å±‚ï¼Œæ“ä½œæ•ˆæœå’Œæ± åŒ–ç›¸åã€‚å¯ä»¥é€šè¿‡modeå‚æ•°æ§åˆ¶ä¸Šé‡‡æ ·ç­–ç•¥ä¸º"nearest"æœ€é‚»è¿‘ç­–ç•¥æˆ–"linear"çº¿æ€§æ’å€¼ç­–ç•¥ã€‚

* nn.Unfoldï¼šæ»‘åŠ¨çª—å£æå–å±‚ã€‚å…¶å‚æ•°å’Œå·ç§¯æ“ä½œnn.Conv2dç›¸åŒã€‚å®é™…ä¸Šï¼Œå·ç§¯æ“ä½œå¯ä»¥ç­‰ä»·äºnn.Unfoldå’Œnn.Linearä»¥åŠnn.Foldçš„ä¸€ä¸ªç»„åˆã€‚
  å…¶ä¸­nn.Unfoldæ“ä½œå¯ä»¥ä»è¾“å…¥ä¸­æå–å„ä¸ªæ»‘åŠ¨çª—å£çš„æ•°å€¼çŸ©é˜µï¼Œå¹¶å°†å…¶å‹å¹³æˆä¸€ç»´ã€‚åˆ©ç”¨nn.Linearå°†nn.Unfoldçš„è¾“å‡ºå’Œå·ç§¯æ ¸åšä¹˜æ³•åï¼Œå†ä½¿ç”¨
  nn.Foldæ“ä½œå°†ç»“æœè½¬æ¢æˆè¾“å‡ºå›¾ç‰‡å½¢çŠ¶ã€‚

* nn.Foldï¼šé€†æ»‘åŠ¨çª—å£æå–å±‚ã€‚


```python

```

<!-- #region -->
é‡ç‚¹è¯´è¯´å„ç§å¸¸ç”¨çš„å·ç§¯å±‚å’Œä¸Šé‡‡æ ·å±‚ï¼š

* æ™®é€šå·ç§¯ã€æ™®é€šå·ç§¯çš„æ“ä½œåˆ†æˆ3ä¸ªç»´åº¦ï¼Œåœ¨ç©ºé—´ç»´åº¦(Hå’ŒWç»´åº¦)æ˜¯å…±äº«å·ç§¯æ ¸æƒé‡æ»‘çª—ç›¸ä¹˜æ±‚å’Œ(èåˆç©ºé—´ä¿¡æ¯)ï¼Œåœ¨è¾“å…¥é€šé“ç»´åº¦æ˜¯æ¯ä¸€ä¸ªé€šé“ä½¿ç”¨ä¸åŒçš„å·ç§¯æ ¸å‚æ•°å¹¶å¯¹è¾“å…¥é€šé“ç»´åº¦æ±‚å’Œ(èåˆé€šé“ä¿¡æ¯)ï¼Œåœ¨è¾“å‡ºé€šé“ç»´åº¦æ“ä½œæ–¹å¼æ˜¯å¹¶è¡Œå †å (å¤šç§)ï¼Œæœ‰å¤šå°‘ä¸ªå·ç§¯æ ¸å°±æœ‰å¤šå°‘ä¸ªè¾“å‡ºé€šé“ã€‘

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5nhe0lsutg20az0aln03.gif)



* ç©ºæ´å·ç§¯ã€å’Œæ™®é€šå·ç§¯ç›¸æ¯”ï¼Œç©ºæ´å·ç§¯å¯ä»¥åœ¨ä¿æŒè¾ƒå°å‚æ•°è§„æ¨¡çš„æ¡ä»¶ä¸‹å¢å¤§æ„Ÿå—é‡ï¼Œå¸¸ç”¨äºå›¾åƒåˆ†å‰²é¢†åŸŸã€‚å…¶ç¼ºç‚¹æ˜¯å¯èƒ½äº§ç”Ÿç½‘æ ¼æ•ˆåº”ï¼Œå³æœ‰äº›åƒç´ è¢«ç©ºæ´æ¼è¿‡æ— æ³•åˆ©ç”¨åˆ°ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨ä¸åŒè†¨èƒ€å› å­çš„ç©ºæ´å·ç§¯çš„ç»„åˆæ¥å…‹æœè¯¥é—®é¢˜ï¼Œå‚è€ƒæ–‡ç« ï¼šhttps://developer.orbbec.com.cn/v/blog_detail/892 ã€‘ 

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5nhe0y7x1g20az0al0vu.gif)


* åˆ†ç»„å·ç§¯ ã€å’Œæ™®é€šå·ç§¯ç›¸æ¯”ï¼Œåˆ†ç»„å·ç§¯å°†è¾“å…¥é€šé“åˆ†æˆgç»„ï¼Œå·ç§¯æ ¸ä¹Ÿåˆ†æˆå¯¹åº”çš„gç»„ï¼Œæ¯ä¸ªå·ç§¯æ ¸åªåœ¨å…¶å¯¹åº”çš„é‚£ç»„è¾“å…¥é€šé“ä¸Šåšå·ç§¯ï¼Œæœ€åå°†gç»„ç»“æœå †å æ‹¼æ¥ã€‚ç”±äºæ¯ä¸ªå·ç§¯æ ¸åªéœ€è¦åœ¨å…¨éƒ¨è¾“å…¥é€šé“çš„1/gä¸ªé€šé“ä¸Šåšå·ç§¯ï¼Œå‚æ•°é‡é™ä½ä¸ºæ™®é€šå·ç§¯çš„1/gã€‚åˆ†ç»„å·ç§¯è¦æ±‚è¾“å…¥é€šé“å’Œè¾“å‡ºé€šé“æ•°éƒ½æ˜¯gçš„æ•´æ•°å€ã€‚å‚è€ƒæ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/65377955 ã€‘
![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5npy1zyalj20ie0erwf8.jpg)



* æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€æ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„æ€æƒ³æ˜¯å…ˆç”¨g=m(è¾“å…¥é€šé“æ•°)çš„åˆ†ç»„å·ç§¯é€é€šé“ä½œç”¨èåˆç©ºé—´ä¿¡æ¯ï¼Œå†ç”¨n(è¾“å‡ºé€šé“æ•°)ä¸ª1ä¹˜1å·ç§¯èåˆé€šé“ä¿¡æ¯ã€‚ å…¶å‚æ•°é‡ä¸º (mÃ—kÃ—k)+ nÃ—m, ç›¸æ¯”æ™®é€šå·ç§¯çš„å‚æ•°é‡ mÃ—nÃ—kÃ—k æ˜¾è‘—å‡å° ã€‘
![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5npbiuvzvj20uq0e7dge.jpg)


* è½¬ç½®å·ç§¯ ã€ä¸€èˆ¬çš„å·ç§¯æ“ä½œåä¼šè®©ç‰¹å¾å›¾å°ºå¯¸å˜å°ï¼Œä½†è½¬ç½®å·ç§¯(ä¹Ÿè¢«ç§°ä¸ºåå·ç§¯)å¯ä»¥å®ç°ç›¸åçš„æ•ˆæœï¼Œå³æ”¾å¤§ç‰¹å¾å›¾å°ºå¯¸ã€‚å¯¹ä¸¤ç§æ–¹å¼ç†è§£è½¬ç½®å·ç§¯ï¼Œç¬¬ä¸€ç§æ–¹å¼æ˜¯è½¬ç½®å·ç§¯æ˜¯ä¸€ç§ç‰¹æ®Šçš„å·ç§¯ï¼Œé€šè¿‡è®¾ç½®åˆé€‚çš„paddingçš„å¤§å°æ¥æ¢å¤ç‰¹å¾å›¾å°ºå¯¸ã€‚ç¬¬äºŒç§ç†è§£åŸºäºå·ç§¯è¿ç®—çš„çŸ©é˜µä¹˜æ³•è¡¨ç¤ºæ–¹æ³•ï¼Œè½¬ç½®å·ç§¯ç›¸å½“äºå°†å·ç§¯æ ¸å¯¹åº”çš„è¡¨ç¤ºçŸ©é˜µåšè½¬ç½®ï¼Œç„¶åä¹˜ä¸Šè¾“å‡ºç‰¹å¾å›¾å‹å¹³çš„ä¸€ç»´å‘é‡ï¼Œå³å¯æ¢å¤åŸå§‹è¾“å…¥ç‰¹å¾å›¾çš„å¤§å°ã€‚
å‚è€ƒæ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/115070523ã€‘

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5ns98iiamj20v70u075e.jpg)






* ä¸Šé‡‡æ ·å±‚ ã€é™¤äº†ä½¿ç”¨è½¬ç½®å·ç§¯è¿›è¡Œä¸Šé‡‡æ ·å¤–ï¼Œåœ¨å›¾åƒåˆ†å‰²é¢†åŸŸæ›´å¤šçš„æ—¶å€™ä¸€èˆ¬æ˜¯ä½¿ç”¨åŒçº¿æ€§æ’å€¼çš„æ–¹å¼è¿›è¡Œä¸Šé‡‡æ ·ï¼Œè¯¥æ–¹æ³•æ²¡æœ‰éœ€è¦å­¦ä¹ çš„å‚æ•°ï¼Œé€šå¸¸æ•ˆæœä¹Ÿæ›´å¥½ï¼Œé™¤äº†åŒçº¿æ€§æ’å€¼ä¹‹å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨æœ€é‚»è¿‘æ’å€¼çš„æ–¹å¼è¿›è¡Œä¸Šé‡‡æ ·ï¼Œä½†ä½¿ç”¨è¾ƒå°‘ã€‚ã€‘

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5nsi5pt4eg20na0co74k.gif)





<!-- #endregion -->

```python
import torch 
from torch import nn 
import torch.nn.functional as F 

# å·ç§¯è¾“å‡ºå°ºå¯¸è®¡ç®—å…¬å¼ o = (i + 2*p -k')//s  + 1 
# å¯¹ç©ºæ´å·ç§¯ k' = d(k-1) + 1
# oæ˜¯è¾“å‡ºå°ºå¯¸ï¼Œi æ˜¯è¾“å…¥å°ºå¯¸ï¼Œpæ˜¯ paddingå¤§å°ï¼Œ k æ˜¯å·ç§¯æ ¸å°ºå¯¸ï¼Œ sæ˜¯strideæ­¥é•¿, dæ˜¯dilationç©ºæ´å‚æ•°

inputs = torch.arange(0,25).view(1,1,5,5).float() # i= 5
filters = torch.tensor([[[[1.0,1],[1,1]]]]) # k = 2

outputs = F.conv2d(inputs, filters) # o = (5+2*0-2)//1+1 = 4
outputs_s2 = F.conv2d(inputs, filters, stride=2)  #o = (5+2*0-2)//2+1 = 2
outputs_p1 = F.conv2d(inputs, filters, padding=1) #o = (5+2*1-2)//1+1 = 6
outputs_d2 = F.conv2d(inputs,filters, dilation=2) #o = (5+2*0-(2(2-1)+1))//1+1 = 3

print("--inputs--")
print(inputs)
print("--filters--")
print(filters)

print("--outputs--")
print(outputs,"\n")

print("--outputs(stride=2)--")
print(outputs_s2,"\n")

print("--outputs(padding=1)--")
print(outputs_p1,"\n")

print("--outputs(dilation=2)--")
print(outputs_d2,"\n")


```

```python
import torch 
from torch import nn 

features = torch.randn(8,64,128,128)
print("features.shape:",features.shape)
print("\n")

#æ™®é€šå·ç§¯
print("--conv--")
conv = nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3)
conv_out = conv(features)
print("conv_out.shape:",conv_out.shape) 
print("conv.weight.shape:",conv.weight.shape)
print("\n")

#åˆ†ç»„å·ç§¯
print("--group conv--")
conv_group = nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3,groups=8)
group_out = conv_group(features)
print("group_out.shape:",group_out.shape) 
print("conv_group.weight.shape:",conv_group.weight.shape)
print("\n")

#æ·±åº¦å¯åˆ†ç¦»å·ç§¯
print("--separable conv--")
depth_conv = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,groups=64)
oneone_conv = nn.Conv2d(in_channels=64,out_channels=32,kernel_size=1)
separable_conv = nn.Sequential(depth_conv,oneone_conv)
separable_out = separable_conv(features)
print("separable_out.shape:",separable_out.shape) 
print("depth_conv.weight.shape:",depth_conv.weight.shape)
print("oneone_conv.weight.shape:",oneone_conv.weight.shape)
print("\n")

#è½¬ç½®å·ç§¯
print("--conv transpose--")
conv_t = nn.ConvTranspose2d(in_channels=32,out_channels=64,kernel_size=3)
features_like = conv_t(conv_out)
print("features_like.shape:",features_like.shape)
print("conv_t.weight.shape:",conv_t.weight.shape)



```

```python
import torch 
from torch import nn 

inputs = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
print("inputs:")
print(inputs)
print("\n")

nearest = nn.Upsample(scale_factor=2, mode='nearest')
bilinear = nn.Upsample(scale_factor=2,mode="bilinear",align_corners=True)

print("nearest(inputs)ï¼š")
print(nearest(inputs))
print("\n")
print("bilinear(inputs)ï¼š")
print(bilinear(inputs)) 

```

```python

```

## ä¸‰ï¼Œå¾ªç¯ç½‘ç»œç›¸å…³å±‚

<!-- #region -->


* nn.LSTMï¼šé•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå±‚ã€æ”¯æŒå¤šå±‚ã€‘ã€‚æœ€æ™®éä½¿ç”¨çš„å¾ªç¯ç½‘ç»œå±‚ã€‚å…·æœ‰æºå¸¦è½¨é“ï¼Œé—å¿˜é—¨ï¼Œæ›´æ–°é—¨ï¼Œè¾“å‡ºé—¨ã€‚å¯ä»¥è¾ƒä¸ºæœ‰æ•ˆåœ°ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚è®¾ç½®bidirectional = Trueæ—¶å¯ä»¥å¾—åˆ°åŒå‘LSTMã€‚éœ€è¦æ³¨æ„çš„æ—¶ï¼Œé»˜è®¤çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶æ˜¯(seq,batch,feature), å¦‚æœéœ€è¦å°†batchç»´åº¦æ”¾åœ¨ç¬¬0ç»´ï¼Œåˆ™è¦è®¾ç½®batch_firstå‚æ•°è®¾ç½®ä¸ºTrueã€‚

* nn.GRUï¼šé—¨æ§å¾ªç¯ç½‘ç»œå±‚ã€æ”¯æŒå¤šå±‚ã€‘ã€‚LSTMçš„ä½é…ç‰ˆï¼Œä¸å…·æœ‰æºå¸¦è½¨é“ï¼Œå‚æ•°æ•°é‡å°‘äºLSTMï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

* nn.RNNï¼šç®€å•å¾ªç¯ç½‘ç»œå±‚ã€æ”¯æŒå¤šå±‚ã€‘ã€‚å®¹æ˜“å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±ï¼Œä¸èƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

* nn.LSTMCellï¼šé•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå•å…ƒã€‚å’Œnn.LSTMåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

* nn.GRUCellï¼šé—¨æ§å¾ªç¯ç½‘ç»œå•å…ƒã€‚å’Œnn.GRUåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

* nn.RNNCellï¼šç®€å•å¾ªç¯ç½‘ç»œå•å…ƒã€‚å’Œnn.RNNåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

<!-- #endregion -->

<!-- #region -->
é‡ç‚¹ä»‹ç»ä¸€ä¸‹LSTMå’ŒGRU 


![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5rzqa54z6j20u00j30u3.jpg)

ä¸€èˆ¬åœ°ï¼Œå„ç§RNNåºåˆ—æ¨¡å‹å±‚(RNN,GRU,LSTMç­‰)å¯ä»¥ç”¨å‡½æ•°è¡¨ç¤ºå¦‚ä¸‹:

$$h_t = f(h_{t-1},x_t)$$

è¿™ä¸ªå…¬å¼çš„å«ä¹‰æ˜¯ï¼štæ—¶åˆ»å¾ªç¯ç¥ç»ç½‘ç»œçš„è¾“å‡ºå‘é‡$h_t$ç”±t-1æ—¶åˆ»çš„è¾“å‡ºå‘é‡$h_{t-1}$å’Œtæ—¶åˆ»çš„è¾“å…¥$i_t$å˜æ¢è€Œæ¥ã€‚


<!-- #endregion -->

<!-- #region -->
* LSTM ç»“æ„è§£æ 

å‚è€ƒæ–‡ç« ï¼šã€Šäººäººéƒ½èƒ½çœ‹æ‡‚çš„LSTMã€‹https://zhuanlan.zhihu.com/p/32085405

LSTMé€šè¿‡å¼•å…¥äº†ä¸‰ä¸ªé—¨æ¥æ§åˆ¶ä¿¡æ¯çš„ä¼ é€’ï¼Œåˆ†åˆ«æ˜¯é—å¿˜é—¨ï¼Œè¾“å…¥é—¨ å’Œè¾“å‡ºé—¨ ã€‚ä¸‰ä¸ªé—¨çš„ä½œç”¨ä¸ºï¼š

ï¼ˆ1ï¼‰é—å¿˜é—¨: é—å¿˜é—¨$f_t$æ§åˆ¶ä¸Šä¸€æ—¶åˆ»çš„å†…éƒ¨çŠ¶æ€  éœ€è¦é—å¿˜å¤šå°‘ä¿¡æ¯ï¼›

ï¼ˆ2ï¼‰è¾“å…¥é—¨: è¾“å…¥é—¨$i_t$æ§åˆ¶å½“å‰æ—¶åˆ»çš„å€™é€‰çŠ¶æ€  æœ‰å¤šå°‘ä¿¡æ¯éœ€è¦ä¿å­˜ï¼›

ï¼ˆ3ï¼‰è¾“å‡ºé—¨: è¾“å‡ºé—¨$o_t$æ§åˆ¶å½“å‰æ—¶åˆ»çš„å†…éƒ¨çŠ¶æ€  æœ‰å¤šå°‘ä¿¡æ¯éœ€è¦è¾“å‡ºç»™å¤–éƒ¨çŠ¶æ€  ï¼›


$$
\begin{align}
i_{t}=\sigma\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right) \tag{1} \\
f_{t}=\sigma\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right) \tag{2} \\
o_{t}=\sigma\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right) \tag{3} \\
\tilde{c}_{t}=\tanh \left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\right) \tag{4} \\
c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \tag{5} \\
h_{t}=o_{t} \odot \tanh \left(c_{t}\right) \tag{6}
\end{align}
$$




<!-- #endregion -->

<!-- #region -->
* GRU ç»“æ„è§£æ

å‚è€ƒæ–‡ç« ï¼šã€Šäººäººéƒ½èƒ½çœ‹æ‡‚çš„GRUã€‹https://zhuanlan.zhihu.com/p/32481747

GRUçš„ç»“æ„æ¯”LSTMæ›´ä¸ºç®€å•ä¸€äº›ï¼ŒGRUåªæœ‰ä¸¤ä¸ªé—¨ï¼Œæ›´æ–°é—¨å’Œé‡ç½®é—¨  ã€‚

ï¼ˆ1ï¼‰æ›´æ–°é—¨ï¼šæ›´æ–°é—¨ç”¨äºæ§åˆ¶æ¯ä¸€æ­¥$h_t$è¢«æ›´æ–°çš„æ¯”ä¾‹ï¼Œæ›´æ–°é—¨è¶Šå¤§ï¼Œ$h_t$æ›´æ–°å¹…åº¦è¶Šå¤§ã€‚

ï¼ˆ2ï¼‰é‡ç½®é—¨ï¼šé‡ç½®é—¨ç”¨äºæ§åˆ¶æ›´æ–°å€™é€‰å‘é‡$\tilde{h}_{t}$ä¸­å‰ä¸€æ­¥çš„çŠ¶æ€$h_{t-1}$è¢«é‡æ–°æ”¾å…¥çš„æ¯”ä¾‹ï¼Œé‡ç½®é—¨è¶Šå¤§ï¼Œæ›´æ–°å€™é€‰å‘é‡ä¸­$h_{t-1}$è¢«é‡æ–°æ”¾è¿›æ¥çš„æ¯”ä¾‹è¶Šå¤§ã€‚




å…¬å¼ä¸­çš„å°åœˆè¡¨ç¤ºå“ˆè¾¾ç›ç§¯ï¼Œä¹Ÿå°±æ˜¯ä¸¤ä¸ªå‘é‡é€ä½ç›¸ä¹˜ã€‚

å…¶ä¸­(1)å¼å’Œ(2)å¼è®¡ç®—çš„æ˜¯æ›´æ–°é—¨$u_t$å’Œé‡ç½®é—¨$r_t$ï¼Œæ˜¯ä¸¤ä¸ªé•¿åº¦å’Œ$h_t$ç›¸åŒçš„å‘é‡ã€‚


æ³¨æ„åˆ°(4)å¼ å®é™…ä¸Šå’ŒResNetçš„æ®‹å·®ç»“æ„æ˜¯ç›¸ä¼¼çš„ï¼Œéƒ½æ˜¯ f(x) = x + g(x) çš„å½¢å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢é•¿åºåˆ—å­¦ä¹ åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚



$$
\begin{align}
z_{t}=\sigma\left(W_{z} x_{t}+U_{z} h_{t-1}+b_{z}\right)\tag{1} \\
r_{t}=\sigma\left(W_{r} x_{t}+U_{r} h_{t-1}+b_{r}\right) \tag{2}\\
\tilde{h}_{t}=\tanh \left(W_{h} x_{t}+U_{h}\left(r_{t} \odot h_{t-1}\right)+b_{h}\right) \tag{3}\\
h_{t}= h_{t-1} - z_{t}\odot h_{t-1}  + z_{t} \odot  \tilde{h}_{t} \tag{4}
\end{align}
$$
GRUçš„å‚æ•°æ•°é‡ä¸ºLSTMçš„3/4.


<!-- #endregion -->

```python
import torch 
from torch import nn 

inputs = torch.randn(8,200,64) #batch_size, seq_length, features

gru = nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)
gru_output,gru_hn = gru(inputs)
print("--GRU--")
print("gru_output.shape:",gru_output.shape)
print("gru_hn.shape:",gru_hn.shape)
print("\n")


print("--LSTM--")
lstm = nn.LSTM(input_size=64,hidden_size=32,num_layers=1,batch_first=True)
lstm_output,(lstm_hn,lstm_cn) = lstm(inputs)
print("lstm_output.shape:",lstm_output.shape)
print("lstm_hn.shape:",lstm_hn.shape)
print("lstm_cn.shape:",lstm_cn.shape)


from torchkeras import summary
summary(gru,input_data=inputs);
summary(lstm,input_data=inputs);


```

```python
9408/12544 
```

```python

```

## å››ï¼ŒTransformerç›¸å…³å±‚

<!-- #region -->
* nn.Transformerï¼šTransformerç½‘ç»œç»“æ„ã€‚Transformerç½‘ç»œç»“æ„æ˜¯æ›¿ä»£å¾ªç¯ç½‘ç»œçš„ä¸€ç§ç»“æ„ï¼Œè§£å†³äº†å¾ªç¯ç½‘ç»œéš¾ä»¥å¹¶è¡Œï¼Œéš¾ä»¥æ•æ‰é•¿æœŸä¾èµ–çš„ç¼ºé™·ã€‚å®ƒæ˜¯ç›®å‰NLPä»»åŠ¡çš„ä¸»æµæ¨¡å‹çš„ä¸»è¦æ„æˆéƒ¨åˆ†ã€‚

* nn.TransformerEncoderï¼šTransformerç¼–ç å™¨ç»“æ„ã€‚ç”±å¤šä¸ª nn.TransformerEncoderLayerç¼–ç å™¨å±‚ç»„æˆã€‚

* nn.TransformerDecoderï¼šTransformerè§£ç å™¨ç»“æ„ã€‚ç”±å¤šä¸ª nn.TransformerDecoderLayerè§£ç å™¨å±‚ç»„æˆã€‚

* nn.TransformerEncoderLayerï¼šTransformerçš„ç¼–ç å™¨å±‚ã€‚ä¸»è¦ç”±Multi-Head self-Attention, Feed-Forwardå‰é¦ˆç½‘ç»œ, LayerNormå½’ä¸€åŒ–å±‚, ä»¥åŠæ®‹å·®è¿æ¥å±‚ç»„æˆã€‚

* nn.TransformerDecoderLayerï¼šTransformerçš„è§£ç å™¨å±‚ã€‚ä¸»è¦ç”±Masked Multi-Head self-Attention, Multi-Head cross-Attention, Feed-Forwardå‰é¦ˆç½‘ç»œ, LayerNormå½’ä¸€åŒ–å±‚, ä»¥åŠæ®‹å·®è¿æ¥å±‚ç»„æˆã€‚

* nn.MultiheadAttentionï¼šå¤šå¤´æ³¨æ„åŠ›å±‚ã€‚ç”¨äºåœ¨åºåˆ—æ–¹å‘ä¸Šèåˆç‰¹å¾ã€‚ä½¿ç”¨çš„æ˜¯Scaled Dot Production Attentionï¼Œå¹¶å¼•å…¥äº†å¤šä¸ªæ³¨æ„åŠ›å¤´ã€‚


$$\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$$ 
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &=\operatorname{Concat}\left(\operatorname{head}_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\
\text { where }\, head_{i} &=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}


![](./data/5-2-Transformerç»“æ„.jpg)



<!-- #endregion -->

å‚è€ƒé˜…è¯»ææ–™ï¼š 

TransformerçŸ¥ä¹åŸç†è®²è§£ï¼šhttps://zhuanlan.zhihu.com/p/48508221

Transformerå“ˆä½›åšå®¢ä»£ç è®²è§£ï¼šhttp://nlp.seas.harvard.edu/annotated-transformer/ 


<!-- #region -->
ä¸»è¦å¯¹Transformerçš„è¦ç‚¹é—®é¢˜åšä¸€äº›æ¢³ç†ï¼š


1ï¼ŒTransformeræ˜¯å¦‚ä½•è§£å†³é•¿è·ç¦»ä¾èµ–çš„é—®é¢˜çš„ï¼Ÿ

Transformeræ˜¯é€šè¿‡å¼•å…¥Scale-Dot-Productæ³¨æ„åŠ›æœºåˆ¶æ¥èåˆåºåˆ—ä¸Šä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œä»è€Œè§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜ã€‚ä»¥æ–‡æœ¬æ•°æ®ä¸ºä¾‹ï¼Œåœ¨å¾ªç¯ç¥ç»ç½‘ç»œLSTMç»“æ„ä¸­ï¼Œè¾“å…¥åºåˆ—ä¸Šç›¸è·å¾ˆè¿œçš„ä¸¤ä¸ªå•è¯æ— æ³•ç›´æ¥å‘ç”Ÿäº¤äº’ï¼Œåªèƒ½é€šè¿‡éšè—å±‚è¾“å‡ºæˆ–è€…ç»†èƒçŠ¶æ€æŒ‰ç…§æ—¶é—´æ­¥éª¤ä¸€ä¸ªä¸€ä¸ªå‘åè¿›è¡Œä¼ é€’ã€‚å¯¹äºä¸¤ä¸ªåœ¨åºåˆ—ä¸Šç›¸è·éå¸¸è¿œçš„å•è¯ï¼Œä¸­é—´ç»è¿‡çš„å…¶å®ƒå•è¯è®©éšè—å±‚è¾“å‡ºå’Œç»†èƒçŠ¶æ€æ··å…¥äº†å¤ªå¤šçš„ä¿¡æ¯ï¼Œå¾ˆéš¾æœ‰æ•ˆåœ°æ•æ‰è¿™ç§é•¿è·ç¦»ä¾èµ–ç‰¹å¾ã€‚ä½†æ˜¯åœ¨Scale-Dot-Productæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œåºåˆ—ä¸Šçš„æ¯ä¸ªå•è¯éƒ½ä¼šå’Œå…¶å®ƒæ‰€æœ‰å•è¯åšä¸€æ¬¡ç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œè¿™ç§æ³¨æ„åŠ›æœºåˆ¶ä¸­å•è¯ä¹‹é—´çš„äº¤äº’æ˜¯å¼ºåˆ¶çš„ä¸å—è·ç¦»å½±å“çš„ï¼Œæ‰€ä»¥å¯ä»¥è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜ã€‚


2ï¼ŒTransformeråœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µå¯ä»¥åœ¨æ—¶é—´(åºåˆ—)ç»´åº¦ä¸Šè¿›è¡Œå¹¶è¡Œå—ï¼Ÿ

åœ¨è®­ç»ƒé˜¶æ®µï¼ŒEncoderå’ŒDecoderåœ¨æ—¶é—´(åºåˆ—)ç»´åº¦éƒ½æ˜¯å¹¶è¡Œçš„ï¼Œåœ¨æµ‹è¯•é˜¶æ®µï¼ŒEncoderåœ¨åºåˆ—ç»´åº¦æ˜¯å¹¶è¡Œçš„ï¼ŒDecoderæ˜¯ä¸²è¡Œçš„ã€‚

é¦–å…ˆï¼ŒEncoderéƒ¨åˆ†åœ¨è®­ç»ƒé˜¶æ®µå’Œé¢„æµ‹é˜¶æ®µéƒ½å¯ä»¥å¹¶è¡Œæ¯”è¾ƒå¥½ç†è§£ï¼Œæ— è®ºåœ¨è®­ç»ƒè¿˜æ˜¯é¢„æµ‹é˜¶æ®µï¼Œå®ƒå¹²çš„äº‹æƒ…éƒ½æ˜¯æŠŠå·²çŸ¥çš„å®Œæ•´è¾“å…¥ç¼–ç æˆmemoryï¼Œåœ¨åºåˆ—ç»´åº¦å¯ä»¥å¹¶è¡Œã€‚

å¯¹äºDecoderéƒ¨åˆ†æœ‰äº›å¾®å¦™ã€‚åœ¨é¢„æµ‹é˜¶æ®µDecoderè‚¯å®šæ˜¯ä¸èƒ½å¹¶è¡Œçš„ï¼Œå› ä¸ºDecoderå®é™…ä¸Šæ˜¯ä¸€ä¸ªè‡ªå›å½’ï¼Œå®ƒå‰é¢k-1ä½ç½®çš„è¾“å‡ºä¼šå˜æˆç¬¬kä½çš„è¾“å…¥çš„ã€‚å‰é¢æ²¡æœ‰è®¡ç®—å®Œï¼Œåé¢æ˜¯æ‹¿ä¸åˆ°è¾“å…¥çš„ï¼Œè‚¯å®šä¸å¯ä»¥å¹¶è¡Œã€‚é‚£ä¹ˆè®­ç»ƒé˜¶æ®µèƒ½å¦å¹¶è¡Œå‘¢ï¼Ÿè™½ç„¶è®­ç»ƒé˜¶æ®µçŸ¥é“äº†å…¨éƒ¨çš„è§£ç ç»“æœï¼Œä½†æ˜¯è®­ç»ƒé˜¶æ®µè¦å’Œé¢„æµ‹é˜¶æ®µä¸€è‡´å•Šï¼Œå‰é¢çš„è§£ç è¾“å‡ºä¸èƒ½å—åˆ°åé¢è§£ç ç»“æœçš„å½±å“å•Šã€‚ä½†Transformeré€šè¿‡åœ¨Decoderä¸­å·§å¦™åœ°å¼•å…¥MaskæŠ€å·§ï¼Œä½¿å¾—åœ¨ç”¨Attentionæœºåˆ¶åšåºåˆ—ç‰¹å¾èåˆçš„æ—¶å€™ï¼Œæ¯ä¸ªå•è¯å¯¹ä½äºå®ƒä¹‹åçš„å•è¯çš„æ³¨æ„åŠ›å¾—åˆ†éƒ½ä¸º0ï¼Œè¿™æ ·å°±ä¿è¯äº†å‰é¢çš„è§£ç è¾“å‡ºä¸ä¼šå—åˆ°åé¢è§£ç ç»“æœçš„å½±å“ï¼Œå› æ­¤Decoderåœ¨è®­ç»ƒé˜¶æ®µå¯ä»¥åœ¨åºåˆ—ç»´åº¦åšå¹¶è¡Œã€‚


3ï¼ŒScaled-Dot Product Attentionä¸ºä»€ä¹ˆè¦é™¤ä»¥$\sqrt{d_k}$?

ä¸ºäº†é¿å…$d_k$å˜å¾—å¾ˆå¤§æ—¶softmaxå‡½æ•°çš„æ¢¯åº¦è¶‹äº0ã€‚å‡è®¾Qå’ŒKä¸­çš„å–å‡ºçš„ä¸¤ä¸ªå‘é‡$q$å’Œ$k$çš„æ¯ä¸ªå…ƒç´ å€¼éƒ½æ˜¯æ­£æ€éšæœºåˆ†å¸ƒï¼Œæ•°å­¦ä¸Šå¯ä»¥è¯æ˜ä¸¤ä¸ªç‹¬ç«‹çš„æ­£æ€éšæœºå˜é‡çš„ç§¯ä¾ç„¶æ˜¯ä¸€ä¸ªæ­£æ€éšæœºå˜é‡ï¼Œé‚£ä¹ˆä¸¤ä¸ªå‘é‡åšç‚¹ç§¯ï¼Œä¼šå¾—åˆ°$d_k$ä¸ªæ­£æ€éšæœºå˜é‡çš„å’Œï¼Œæ•°å­¦ä¸Š$d_k$ä¸ªæ­£æ€éšæœºå˜é‡çš„å’Œä¾ç„¶æ˜¯ä¸€ä¸ªæ­£æ€éšæœºå˜é‡ï¼Œå…¶æ–¹å·®æ˜¯åŸæ¥çš„$d_k$å€ï¼Œæ ‡å‡†å·®æ˜¯åŸæ¥çš„$\sqrt{d_k}$å€ã€‚å¦‚æœä¸åšscale, å½“$d_k$å¾ˆå¤§æ—¶ï¼Œæ±‚å¾—çš„$QK^T$å…ƒç´ çš„ç»å¯¹å€¼å®¹æ˜“å¾ˆå¤§ï¼Œå¯¼è‡´è½åœ¨softmaxçš„æç«¯åŒºåŸŸ(è¶‹äº0æˆ–è€…1)ï¼Œæç«¯åŒºåŸŸsoftmaxå‡½æ•°çš„æ¢¯åº¦å€¼è¶‹äº0ï¼Œä¸åˆ©äºæ¨¡å‹å­¦ä¹ ã€‚é™¤ä»¥$\sqrt{d_k}$ï¼Œæ°å¥½åšäº†å½’ä¸€ï¼Œä¸å—$d_k$å˜åŒ–å½±å“ã€‚


4ï¼ŒMultiHeadAttentionçš„å‚æ•°æ•°é‡å’Œheadæ•°é‡æœ‰ä½•å…³ç³»?

MultiHeadAttentionçš„å‚æ•°æ•°é‡å’Œheadæ•°é‡æ— å…³ã€‚å¤šå¤´æ³¨æ„åŠ›çš„å‚æ•°æ¥è‡ªå¯¹QKVçš„ä¸‰ä¸ªå˜æ¢çŸ©é˜µä»¥åŠå¤šå¤´ç»“æœconcatåçš„è¾“å‡ºå˜æ¢çŸ©é˜µã€‚å‡è®¾åµŒå…¥å‘é‡çš„é•¿åº¦æ˜¯d_model, ä¸€å…±æœ‰hä¸ªhead. å¯¹æ¯ä¸ªheadï¼Œ$W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ è¿™ä¸‰ä¸ªå˜æ¢çŸ©é˜µçš„å°ºå¯¸éƒ½æ˜¯ d_modelÃ—(d_model/h)ï¼Œæ‰€ä»¥hä¸ªheadæ€»çš„å‚æ•°æ•°é‡å°±æ˜¯3Ã—d_modelÃ—(d_model/h)Ã—h = 3Ã—d_modelÃ—d_modelã€‚å®ƒä»¬çš„è¾“å‡ºå‘é‡é•¿åº¦éƒ½å˜æˆ d_model/hï¼Œç»è¿‡attentionä½œç”¨åå‘é‡é•¿åº¦ä¿æŒï¼Œhä¸ªheadçš„è¾“å‡ºæ‹¼æ¥åˆ°ä¸€èµ·åå‘é‡é•¿åº¦è¿˜æ˜¯d_modelï¼Œæ‰€ä»¥æœ€åè¾“å‡ºå˜æ¢çŸ©é˜µçš„å°ºå¯¸æ˜¯d_modelÃ—d_modelã€‚å› æ­¤ï¼ŒMultiHeadAttentionçš„å‚æ•°æ•°é‡ä¸º 4Ã—d_modelÃ—d_modelï¼Œå’Œheadæ•°é‡æ— å…³ã€‚


5ï¼ŒTransformeræœ‰ä»€ä¹ˆç¼ºç‚¹ï¼Ÿ

Transformerä¸»è¦çš„ç¼ºç‚¹æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯æ³¨æ„åŠ›æœºåˆ¶ç›¸å¯¹åºåˆ—é•¿åº¦çš„å¤æ‚åº¦æ˜¯O(n^2)ï¼Œç¬¬äºŒä¸ªæ˜¯å¯¹ä½ç½®ä¿¡æ¯çš„ã€‚
ç¬¬ä¸€ï¼ŒTransformeråœ¨ç”¨Attentionæœºåˆ¶åšåºåˆ—ç‰¹å¾èåˆçš„æ—¶å€™ï¼Œæ¯ä¸¤ä¸ªå•è¯ä¹‹é—´éƒ½è¦è®¡ç®—ç‚¹ç§¯è·å¾—æ³¨æ„åŠ›å¾—åˆ†ï¼Œè¿™ä¸ªè®¡ç®—å¤æ‚åº¦å’Œåºåˆ—çš„é•¿åº¦å¹³æ–¹æˆæ­£æ¯”ï¼Œå¯¹äºä¸€äº›ç‰¹åˆ«é•¿çš„åºåˆ—ï¼Œå¯èƒ½å­˜åœ¨ç€æ€§èƒ½ç“¶é¢ˆï¼Œæœ‰ä¸€äº›é’ˆå¯¹è¿™ä¸ªé—®é¢˜çš„æ”¹è¿›æ–¹æ¡ˆå¦‚Linformerã€‚
ç¬¬äºŒä¸ªæ˜¯Transformeré€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ä¸¤ä¸¤ä½ç½®åšç‚¹ä¹˜æ¥èåˆåºåˆ—ç‰¹å¾ï¼Œè€Œä¸æ˜¯åƒå¾ªç¯ç¥ç»ç½‘ç»œé‚£æ ·ç”±å…ˆåˆ°ååœ°å¤„ç†åºåˆ—ä¸­çš„æ•°æ®ï¼Œå¯¼è‡´ä¸¢å¤±äº†å•è¯ä¹‹é—´çš„ä½ç½®ä¿¡æ¯å…³ç³»ï¼Œé€šè¿‡åœ¨è¾“å…¥ä¸­å¼•å…¥æ­£ä½™å¼¦å‡½æ•°æ„é€ çš„ä½ç½®ç¼–ç PositionEncodingä¸€å®šç¨‹åº¦ä¸Šè¡¥å……äº†ä½ç½®ä¿¡æ¯ï¼Œä½†è¿˜æ˜¯ä¸å¦‚å¾ªç¯ç¥ç»ç½‘ç»œé‚£æ ·è‡ªç„¶å’Œé«˜æ•ˆã€‚






<!-- #endregion -->

```python
import torch 
from torch import nn 

#éªŒè¯MultiheadAttentionå’Œheadæ•°é‡æ— å…³
inputs = torch.randn(8,200,64) #batch_size, seq_length, features

attention_h8 = nn.MultiheadAttention(
    embed_dim = 64,
    num_heads = 8,
    bias=True,
    batch_first=True
)

attention_h16 = nn.MultiheadAttention(
    embed_dim = 64,
    num_heads = 16,
    bias=True,
    batch_first=True
)


out_h8 = attention_h8(inputs,inputs,inputs)
out_h16 = attention_h16(inputs,inputs,inputs)

from torchkeras import summary 
summary(attention_h8,input_data_args=(inputs,inputs,inputs));

summary(attention_h16,input_data_args=(inputs,inputs,inputs));

```

```python
import torch 
from torch import nn 
from copy import deepcopy

#å¤šå¤´æ³¨æ„åŠ›çš„ä¸€ç§ç®€æ´å®ç°

class ScaledDotProductAttention(nn.Module):
    "Compute 'Scaled Dot Product Attention'"
    def __init__(self):
        super(ScaledDotProductAttention, self).__init__()

    def forward(self,query, key, value, mask=None, dropout=None):
        d_k = query.size(-1)
        scores = query@key.transpose(-2,-1) / d_k**0.5     
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e20)
        p_attn = F.softmax(scores, dim = -1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return p_attn@value, p_attn
    
class MultiHeadAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        "Take in model size and number of heads."
        super(MultiHeadAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = nn.ModuleList([deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])
        
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        self.attention = ScaledDotProductAttention()
        
    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        
        # 1) Do all the linear projections in batch from d_model => h x d_k 
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
        
        # 2) Apply attention on all the projected vectors in batch. 
        x, self.attn = self.attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # 3) "Concat" using a view and apply a final linear. 
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
    
```

```python

```

```python

```

## äº”ï¼Œè‡ªå®šä¹‰æ¨¡å‹å±‚


å¦‚æœPytorchçš„å†…ç½®æ¨¡å‹å±‚ä¸èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰çš„æ¨¡å‹å±‚ã€‚

å®é™…ä¸Šï¼Œpytorchä¸åŒºåˆ†æ¨¡å‹å’Œæ¨¡å‹å±‚ï¼Œéƒ½æ˜¯é€šè¿‡ç»§æ‰¿nn.Moduleè¿›è¡Œæ„å»ºã€‚

å› æ­¤ï¼Œæˆ‘ä»¬åªè¦ç»§æ‰¿nn.ModuleåŸºç±»å¹¶å®ç°forwardæ–¹æ³•å³å¯è‡ªå®šä¹‰æ¨¡å‹å±‚ã€‚

ä¸‹é¢æ˜¯Pytorchçš„nn.Linearå±‚çš„æºç ï¼Œæˆ‘ä»¬å¯ä»¥ä»¿ç…§å®ƒæ¥è‡ªå®šä¹‰æ¨¡å‹å±‚ã€‚


```python
import torch
from torch import nn
import torch.nn.functional as F

class Linear(nn.Module):
    __constants__ = ['in_features', 'out_features']

    def __init__(self, in_features, out_features, bias=True):
        super(Linear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, input):
        return F.linear(input, self.weight, self.bias)

    def extra_repr(self):
        return 'in_features={}, out_features={}, bias={}'.format(
            self.in_features, self.out_features, self.bias is not None
        )
```

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)
