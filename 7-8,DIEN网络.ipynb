{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949e8e0e",
   "metadata": {},
   "source": [
    "\n",
    "# 7-8ï¼ŒDIENç½‘ç»œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d35e35",
   "metadata": {},
   "source": [
    "é˜¿é‡Œå¦ˆå¦ˆåœ¨CTRé¢„ä¼°é¢†åŸŸæœ‰3ç¯‡æ¯”è¾ƒæœ‰åçš„æ–‡ç« ã€‚\n",
    "\n",
    "2017å¹´çš„æ·±åº¦å…´è¶£ç½‘ç»œ, DIN(DeepInterestNetwork)ã€‚ \n",
    "\n",
    "2018å¹´çš„æ·±åº¦å…´è¶£æ¼”åŒ–ç½‘ç»œ, DIEN(DeepInterestEvolutionNetWork)ã€‚\n",
    "\n",
    "2019å¹´çš„æ·±åº¦ä¼šè¯å…´è¶£ç½‘ç»œ, DSIN(DeepSessionInterestNetWork)ã€‚\n",
    "\n",
    "è¿™3ç¯‡æ–‡ç« çš„ä¸»è¦æ€æƒ³å’Œç›¸äº’å…³ç³»ç”¨ä¸€å¥è¯åˆ†åˆ«æ¦‚æ‹¬å¦‚ä¸‹ï¼š\n",
    "\n",
    "ç¬¬1ç¯‡DINè¯´ï¼Œç”¨æˆ·çš„è¡Œä¸ºæ—¥å¿—ä¸­åªæœ‰ä¸€éƒ¨åˆ†å’Œå½“å‰å€™é€‰å¹¿å‘Šæœ‰å…³ã€‚å¯ä»¥åˆ©ç”¨Attentionæœºåˆ¶ä»ç”¨æˆ·è¡Œä¸ºæ—¥å¿—ä¸­å»ºæ¨¡å‡ºå’Œå½“å‰å€™é€‰å¹¿å‘Šç›¸å…³çš„ç”¨æˆ·å…´è¶£è¡¨ç¤ºã€‚æˆ‘ä»¬è¯•è¿‡æ¶¨ç‚¹äº†å˜»å˜»å˜»ã€‚\n",
    "\n",
    "ç¬¬2ç¯‡DIENè¯´ï¼Œç”¨æˆ·æœ€è¿‘çš„è¡Œä¸ºå¯èƒ½æ¯”è¾ƒè¿œçš„è¡Œä¸ºæ›´åŠ é‡è¦ã€‚å¯ä»¥ç”¨å¾ªç¯ç¥ç»ç½‘ç»œGRUå»ºæ¨¡ç”¨æˆ·å…´è¶£éšæ—¶é—´çš„æ¼”åŒ–ã€‚æˆ‘ä»¬è¯•è¿‡ä¹Ÿæ¶¨ç‚¹äº†å˜¿å˜¿å˜¿ã€‚\n",
    "\n",
    "ç¬¬3ç¯‡DSINè¯´ï¼Œç”¨æˆ·åœ¨åŒä¸€æ¬¡ä¼šè¯ä¸­çš„è¡Œä¸ºé«˜åº¦ç›¸å…³ï¼Œåœ¨ä¸åŒä¼šè¯é—´çš„è¡Œä¸ºåˆ™ç›¸å¯¹ç‹¬ç«‹ã€‚å¯ä»¥æŠŠç”¨æˆ·è¡Œä¸ºæ—¥å¿—æŒ‰ç…§æ—¶é—´é—´éš”åˆ†å‰²æˆä¼šè¯å¹¶ç”¨SelfAttentionæœºåˆ¶å»ºæ¨¡å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚æˆ‘ä»¬è¯•è¿‡åˆæ¶¨ç‚¹äº†å“ˆå“ˆå“ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af6364",
   "metadata": {},
   "source": [
    "å‚è€ƒææ–™ï¼š\n",
    "\n",
    "* DIENè®ºæ–‡ï¼š https://arxiv.org/pdf/1809.03672.pdf \n",
    "\n",
    "* DIN+DIENï¼Œæœºå™¨å­¦ä¹ å”¯ä¸€æŒ‡å®šæ¶¨ç‚¹æŠ€Attentionï¼š https://zhuanlan.zhihu.com/p/431131396\n",
    "\n",
    "* ä»DINåˆ°DIENçœ‹é˜¿é‡ŒCTRç®—æ³•çš„è¿›åŒ–è„‰ç»œï¼š https://zhuanlan.zhihu.com/p/78365283\n",
    "\n",
    "* ä»£ç å®ç°å‚è€ƒï¼š https://github.com/GitHub-HongweiZhang/prediction-flow\n",
    "\n",
    "ä¸Šä¸€ç¯‡æ–‡ç« æˆ‘ä»¬ä»‹ç»äº†DIN, æœ¬ç¯‡æ–‡ç« æˆ‘ä»¬ä»‹ç»DIENã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1be357",
   "metadata": {},
   "source": [
    "DIENè¿™ç¯‡æ–‡ç« çš„ä¸»è¦åˆ›æ–°ä¹‹å¤„æœ‰3ç‚¹ï¼š\n",
    "\n",
    "* ä¸€æ˜¯å¼•å…¥GRUæ¥ä»ç”¨æˆ·è¡Œä¸ºæ—¥å¿—åºåˆ—ä¸­è‡ªç„¶åœ°æŠ½å–æ¯ä¸ªè¡Œä¸ºæ—¥å¿—å¯¹åº”çš„ç”¨æˆ·å…´è¶£è¡¨ç¤º(å…´è¶£æŠ½å–å±‚)ã€‚\n",
    "\n",
    "* äºŒæ˜¯è®¾è®¡äº†ä¸€ä¸ªè¾…åŠ©losså±‚ï¼Œé€šè¿‡åšä¸€ä¸ªè¾…åŠ©ä»»åŠ¡(åŒºåˆ†çœŸå®çš„ç”¨æˆ·å†å²ç‚¹å‡»è¡Œä¸ºå’Œè´Ÿé‡‡æ ·çš„éç”¨æˆ·ç‚¹å‡»è¡Œä¸º)æ¥å¼ºåŒ–ç”¨æˆ·å…´è¶£è¡¨ç¤ºçš„å­¦ä¹ ã€‚\n",
    "\n",
    "* ä¸‰æ˜¯å°†æ³¨æ„åŠ›æœºåˆ¶å’ŒGRUç»“æ„ç»“åˆèµ·æ¥(AUGRU: Attention UPdate GRU)ï¼Œæ¥å»ºæ¨¡ç”¨æˆ·å…´è¶£çš„æ—¶é—´æ¼”åŒ–å¾—åˆ°æœ€ç»ˆçš„ç”¨æˆ·è¡¨ç¤º(å…´è¶£æ¼”åŒ–å±‚)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd12a20",
   "metadata": {},
   "source": [
    "å…¶ä¸­å¼•å…¥è¾…åŠ©Lossçš„æŠ€å·§æ˜¯ç¥ç»ç½‘ç»œæ¶¨ç‚¹éå¸¸é€šç”¨çš„ä¸€ç§é«˜çº§æŠ€å·§ï¼Œå€¼å¾—æˆ‘ä»¬å­¦ä¹ ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3fb51",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<font color=\"red\">\n",
    " \n",
    "å…¬ä¼—å· **ç®—æ³•ç¾é£Ÿå±‹** å›å¤å…³é”®è¯ï¼š**pytorch**ï¼Œ è·å–æœ¬é¡¹ç›®æºç å’Œæ‰€ç”¨æ•°æ®é›†ç™¾åº¦äº‘ç›˜ä¸‹è½½é“¾æ¥ã€‚\n",
    "    \n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f76cb",
   "metadata": {},
   "source": [
    "## ä¸€ï¼ŒDIENåŸç†è§£æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffc518",
   "metadata": {},
   "source": [
    "DIENçš„ä¸»è¦å‡ºå‘ç‚¹æ˜¯ï¼Œç”¨æˆ·æœ€è¿‘çš„è¡Œä¸ºå¯èƒ½æ¯”è¾ƒè¿œçš„è¡Œä¸ºæ›´åŠ é‡è¦ã€‚å¯ä»¥ç”¨å¾ªç¯ç¥ç»ç½‘ç»œGRUå»ºæ¨¡ç”¨æˆ·å…´è¶£éšæ—¶é—´çš„æ¼”åŒ–ã€‚\n",
    "\n",
    "DIENé€‰æ‹©çš„æ˜¯ä¸å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±ä¸”è¾ƒå¿«çš„GRUã€‚\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h3x1brptqij20k10b8jsp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c1015",
   "metadata": {},
   "source": [
    "### 1, å…´è¶£æŠ½å–å±‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef24e5",
   "metadata": {},
   "source": [
    "å›¾ä¸­çš„ $b(t)$ æ˜¯ç”¨æˆ·çš„è¡Œä¸ºåºåˆ—ï¼Œè€Œ $e(t)$æ˜¯å¯¹åº”çš„embeddingã€‚éšç€è‡ªç„¶å‘ç”Ÿçš„é¡ºåºï¼Œ $e(t)$è¢«è¾“å…¥GRUä¸­ï¼Œè¿™å°±æ˜¯å…´è¶£æŠ½å–å±‚ã€‚\n",
    "\n",
    "ä¹Ÿæ˜¯DIENçš„ç¬¬ä¸€æ¡åˆ›æ–°ï¼šå¼•å…¥GRUæ¥ä»ç”¨æˆ·è¡Œä¸ºæ—¥å¿—åºåˆ—ä¸­è‡ªç„¶åœ°æŠ½å–æ¯ä¸ªè¡Œä¸ºæ—¥å¿—å¯¹åº”çš„ç”¨æˆ·å…´è¶£è¡¨ç¤º(å…´è¶£æŠ½å–å±‚)ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cabf40",
   "metadata": {},
   "source": [
    "### 2ï¼Œè¾…åŠ©loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e181c",
   "metadata": {},
   "source": [
    "å¦‚æœå¿½ç•¥ä¸Šé¢çš„AUGRUç¯èŠ‚ï¼ŒGRUä¸­çš„éšçŠ¶æ€ $h(t)$å°±åº”è¯¥æˆä¸ºç”¨æˆ·çš„è¡Œä¸ºåºåˆ—æœ€åçš„è¡¨ç¤ºã€‚\n",
    "\n",
    "å¦‚æœç›´æ¥å°±è¿™æ ·åšï¼Œä¹Ÿä¸æ˜¯ä¸å¯ä»¥ï¼Œä½†æ˜¯$h(t)$å­¦ä¹ åˆ°çš„ä¸œè¥¿å¯èƒ½ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ç”¨æˆ·å…´è¶£è¡¨ç¤ºï¼Œæˆ–è€…è¯´$h(t)$å¾ˆéš¾å­¦ä¹ åˆ°æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "å› ä¸º$h(t)$ çš„è¿­ä»£ç»è¿‡äº†å¾ˆå¤šæ­¥ï¼Œç„¶åè¿˜è¦å’Œå…¶ä»–ç‰¹å¾åšæ‹¼æ¥ï¼Œç„¶åè¿˜è¦ç»è¿‡MLPï¼Œæœ€åæ‰å¾—åˆ°è¾“å‡ºå»è®¡ç®—Lossã€‚\n",
    "\n",
    "è¿™æ ·çš„ç»“æœå°±æ˜¯æœ€åæ¥äº†ä¸€ä¸ªæ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬ï¼Œåå‘ä¼ æ’­å¾ˆéš¾å½’å› åˆ° $h(t)$ ä¸Šã€‚\n",
    "\n",
    "åŸºäºæ­¤DIENç»™å‡ºäº†ç¬¬äºŒä¸ªè¦ç‚¹ï¼šä½¿ç”¨è¾…åŠ©Lossæ¥å¼ºåŒ–$h(t)$çš„å­¦ä¹ ã€‚\n",
    "\n",
    "æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªè¾…åŠ©Lossæ˜¯æ€ä¹ˆåšçš„ï¼Ÿè¿™é‡Œè®¾è®¡äº†ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ï¼Œä½¿ç”¨$h(t)$æ¥åŒºåˆ†çœŸå®çš„ç”¨æˆ·å†å²ç‚¹å‡»è¡Œä¸ºå’Œè´Ÿé‡‡æ ·çš„éç”¨æˆ·ç‚¹å‡»è¡Œä¸ºã€‚\n",
    "\n",
    "ç”±äº$h(t)$ ä»£è¡¨ç€ t æ—¶åˆ»çš„ç”¨æˆ·å…´è¶£è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥é¢„æµ‹ t+1æ—¶åˆ»çš„å¹¿å‘Šç”¨æˆ·æ˜¯å¦ç‚¹å‡»ã€‚\n",
    "\n",
    "å› ä¸ºç”¨æˆ·è¡Œä¸ºæ—¥å¿—ä¸­éƒ½æ˜¯ç”¨æˆ·ç‚¹å‡»è¿‡çš„å¹¿å‘Š(æ­£æ ·æœ¬, $e(t)$)ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä»å…¨éƒ¨çš„å¹¿å‘Šä¸­ç»™ç”¨æˆ·é‡‡æ ·åŒæ ·æ•°é‡çš„ç”¨æˆ·æ²¡æœ‰ç‚¹å‡»è¿‡çš„å¹¿å‘Šä½œä¸ºè´Ÿæ ·æœ¬$e'(t)$ã€‚\n",
    "\n",
    "ç»“åˆ$h(t)$å’Œ $e(t)$, $e'(t)$ä½œä¸ºè¾“å…¥, æˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªäºŒåˆ†ç±»çš„è¾…åŠ©ä»»åŠ¡ã€‚\n",
    "\n",
    "è¿™ä¸ªè¾…åŠ©ä»»åŠ¡ç»™$h(t)$åœ¨æ¯ä¸ªtæ—¶åˆ»éƒ½æä¾›äº†ä¸€ä¸ªç›‘ç£ä¿¡å·ï¼Œä½¿å¾—$h(t)$èƒ½å¤Ÿæ›´å¥½åœ°æˆä¸ºç”¨æˆ·å…´è¶£çš„æŠ½å–è¡¨ç¤ºã€‚\n",
    "\n",
    "çœŸå®åº”ç”¨åœºåˆä¸‹ï¼Œä½ æŠŠå¼€å§‹çš„è¾“å…¥å’Œæœ€åçš„è¦æ±‚å‘Šè¯‰ç½‘ç»œï¼Œå®ƒå°±èƒ½ç»™ä½ ä¸€ä¸ªå¥½çš„ç»“æœçš„æƒ…å†µéå¸¸å°‘ã€‚\n",
    "\n",
    "å¤§å¤šæ•°æ—¶å€™æ˜¯éœ€è¦ä½ å»æ§åˆ¶æ¯ä¸€æ­¥çš„è¾“å…¥è¾“å‡ºï¼Œæ¯ä¸€æ­¥çš„lossæ‰èƒ½é˜²æ­¢ç½‘ç»œå„ç§å·æ‡’ä½œå¼Šã€‚\n",
    "\n",
    "è¾…åŠ©lossèƒ½å¤Ÿä½¿å¾—ç½‘ç»œæ›´å—æ§åˆ¶ï¼Œå‘æˆ‘ä»¬éœ€è¦çš„æ–¹å‘å‘å±•ï¼Œéå¸¸å»ºè®®å¤§å®¶åœ¨å®é™…ä¸šåŠ¡ä¸­å¤šè¯•è¯•è¾…åŠ©lossã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e6f486",
   "metadata": {},
   "source": [
    "### 3ï¼Œå…´è¶£æ¼”åŒ–å±‚\n",
    "\n",
    "é€šè¿‡å…´è¶£æŠ½å–å±‚å’Œè¾…åŠ©lossï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ¯ä¸ªtæ—¶åˆ»ç”¨æˆ·çš„ä¸€èˆ¬å…´è¶£è¡¨ç¤ºã€‚\n",
    "\n",
    "æ³¨æ„è¿™ä¸ªå…´è¶£è¡¨ç¤ºæ˜¯ä¸€èˆ¬æ€§çš„ï¼Œè¿˜æ²¡æœ‰å’Œæˆ‘ä»¬çš„å€™é€‰å¹¿å‘ŠåšAttentionå…³è”ã€‚\n",
    "\n",
    "åœ¨DINä¸­ï¼Œæˆ‘ä»¬é€šè¿‡Attentionæœºåˆ¶æ„å»ºäº†å’Œå€™é€‰å¹¿å‘Šç›¸å…³çš„ç”¨æˆ·å…´è¶£è¡¨ç¤ºã€‚\n",
    "\n",
    "è€Œåœ¨DIENä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å»ºç«‹çš„æ˜¯å’Œå’Œå€™é€‰å¹¿å‘Šç›¸å…³ï¼Œå¹¶ä¸”å’Œæ—¶é—´æ¼”åŒ–ç›¸å…³çš„ç”¨æˆ·å…´è¶£è¡¨ç¤ºã€‚\n",
    "\n",
    "DIENé€šè¿‡ç»“åˆAttentionæœºåˆ¶å’ŒGRUç»“æ„æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿™å°±æ˜¯ç¬¬ä¸‰ç‚¹åˆ›æ–°AUGRU : Attention UPdate Gate GRUã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬è¿›è¡Œè¯¦ç»†è®²è§£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0c97d",
   "metadata": {},
   "source": [
    "ä¸€èˆ¬åœ°ï¼Œå„ç§RNNåºåˆ—æ¨¡å‹å±‚(SimpleRNN,GRU,LSTMç­‰)å¯ä»¥ç”¨å‡½æ•°è¡¨ç¤ºå¦‚ä¸‹:\n",
    "\n",
    "$$h_t = f(h_{t-1},i_t)$$\n",
    "\n",
    "è¿™ä¸ªå…¬å¼çš„å«ä¹‰æ˜¯ï¼štæ—¶åˆ»å¾ªç¯ç¥ç»ç½‘ç»œçš„è¾“å‡ºå‘é‡$h_t$ç”±t-1æ—¶åˆ»çš„è¾“å‡ºå‘é‡$h_{t-1}$å’Œtæ—¶åˆ»çš„è¾“å…¥$i_t$å˜æ¢è€Œæ¥ã€‚\n",
    "\n",
    "ä¸ºäº†ç»“åˆAttentionæœºåˆ¶å’ŒGRUç»“æ„ï¼Œæˆ‘ä»¬éœ€è¦è®¾è®¡è¿™æ ·çš„ä¸€ä¸ªæœ‰ä¸‰ç§è¾“å…¥çš„åºåˆ—æ¨¡å‹\n",
    "\n",
    "$$h_t = g(h_{t-1},i_t, a_t)$$\n",
    "\n",
    "è¿™é‡Œçš„$a_t$æ˜¯ tæ—¶åˆ»çš„ç”¨æˆ·å…´è¶£è¡¨ç¤ºè¾“å…¥ $i_t$å’Œå€™é€‰å¹¿å‘Šè®¡ç®—å‡ºçš„attention å¾—åˆ†ï¼Œæ˜¯ä¸ªæ ‡é‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d78bd1",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å…ˆçœ‹çœ‹ GRUçš„ å…·ä½“å‡½æ•°å½¢å¼ï¼š \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_t &= \\sigma(W^u i_t + U^u h_{t-1} + b^u) \\tag{1} \\\\\n",
    "r_t &= \\sigma(W^r i_t + U^r h_{t-1} + b^r) \\tag{2} \\\\\n",
    "n_t &= \\tanh(W^n i_t + r_t \\circ U^n h_{t-1} + b^n) \\tag{3} \\\\\n",
    "h_t &= h_{t-1} - u_t \t\\circ h_{t-1} + u_t \\circ n_t \\tag{4} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28e848",
   "metadata": {},
   "source": [
    "å…¬å¼ä¸­çš„å°åœˆè¡¨ç¤ºå“ˆè¾¾ç›ç§¯ï¼Œä¹Ÿå°±æ˜¯ä¸¤ä¸ªå‘é‡é€ä½ç›¸ä¹˜ã€‚\n",
    "\n",
    "å…¶ä¸­(1)å¼å’Œ(2)å¼è®¡ç®—çš„æ˜¯æ›´æ–°é—¨$u_t$å’Œé‡ç½®é—¨$r_t$ï¼Œæ˜¯ä¸¤ä¸ªé•¿åº¦å’Œ$h_t$ç›¸åŒçš„å‘é‡ã€‚\n",
    "\n",
    "æ›´æ–°é—¨ç”¨äºæ§åˆ¶æ¯ä¸€æ­¥$h_t$è¢«æ›´æ–°çš„æ¯”ä¾‹ï¼Œæ›´æ–°é—¨è¶Šå¤§ï¼Œ$h_t$æ›´æ–°å¹…åº¦è¶Šå¤§ã€‚\n",
    "\n",
    "é‡ç½®é—¨ç”¨äºæ§åˆ¶æ›´æ–°å€™é€‰å‘é‡$n_t$ä¸­å‰ä¸€æ­¥çš„çŠ¶æ€$h_{t-1}$è¢«é‡æ–°æ”¾å…¥çš„æ¯”ä¾‹ï¼Œé‡ç½®é—¨è¶Šå¤§ï¼Œæ›´æ–°å€™é€‰å‘é‡ä¸­$h_{t-1}$è¢«é‡æ–°æ”¾è¿›æ¥çš„æ¯”ä¾‹è¶Šå¤§ã€‚\n",
    "\n",
    "æ³¨æ„åˆ°(4)å¼ å®é™…ä¸Šå’ŒResNetçš„æ®‹å·®ç»“æ„æ˜¯ç›¸ä¼¼çš„ï¼Œéƒ½æ˜¯ f(x) = x + g(x) çš„å½¢å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢é•¿åºåˆ—å­¦ä¹ åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚\n",
    "\n",
    "å¦‚ä½•åœ¨GRUçš„åŸºç¡€ä¸ŠæŠŠattentionå¾—åˆ†èå…¥è¿›æ¥å‘¢ï¼Ÿæœ‰ä»¥ä¸‹ä¸€äº›éå¸¸è‡ªç„¶çš„æƒ³æ³•ï¼š\n",
    "\n",
    "* 1ï¼Œ ç”¨$a_t$ç¼©æ”¾è¾“å…¥$i_t$, è¿™å°±æ˜¯AIGRU: Attention Input GRUã€‚å…¶å«ä¹‰æ˜¯ç›¸å…³æ€§é«˜çš„åœ¨è¾“å…¥ç«¯è¿›è¡Œæ”¾å¤§ã€‚\n",
    "\n",
    "* 2ï¼Œ ç”¨$a_t$ä»£æ›¿GRUçš„æ›´æ–°é—¨ï¼Œè¿™å°±æ˜¯AGRU: Attention based GRUã€‚å…¶å«ä¹‰æ˜¯ç”¨ç›´æ¥ç”¨ç›¸å…³æ€§ä½œä¸ºæ›´æ–°å¹…åº¦ã€‚\n",
    "\n",
    "* 3ï¼Œ ç”¨$a_t$ç¼©æ”¾GRUçš„æ›´æ–°é—¨$u_t$ï¼Œè¿™å°±æ˜¯AUGRU:  Attention Update Gate GRUã€‚å…¶å«ä¹‰æ˜¯ç”¨ç”¨ç›¸å…³æ€§ç¼©æ”¾æ›´æ–°å¹…åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb903e31",
   "metadata": {},
   "source": [
    "AIGRUå®é™…ä¸Šå¹¶æ²¡æœ‰æ”¹å˜GRUçš„ç»“æ„ï¼Œåªæ˜¯æ”¹å˜äº†å…¶è¾“å…¥ï¼Œè¿™ç§æ–¹å¼å¯¹Attentionçš„ä½¿ç”¨æ¯”è¾ƒå«è“„ï¼Œæˆ‘æŠŠæ¯ä¸ªå†å²å¹¿å‘Šçš„ç›¸å…³æ€§å¼ºå¼±é€šè¿‡è¾“å…¥å‘Šè¯‰GRUï¼ŒGRUä½ å°±ç»™æˆ‘å¥½å¥½å­¦å§ï¼Œå¸Œæœ›ä½ æŠŠç›¸å…³æ€§å¼ºçš„å¹¿å‘Šå¤šé•¿ç‚¹åˆ°è„‘å­é‡Œã€‚ä½†æ˜¯è¿™ç§æ–¹å¼æ•ˆæœä¸æ˜¯å¾ˆç†æƒ³ï¼Œå³ä½¿æ˜¯ç›¸å…³æ€§ä¸º0çš„å†å²å¹¿å‘Šï¼Œä¹Ÿä¼šå¯¹è¿›è¡Œæ›´æ–°ã€‚\n",
    "\n",
    "AGRUæ˜¯æ”¹å˜äº†GRUçš„ç»“æ„çš„ï¼Œå¹¶ä¸”å¯¹Attentionçš„ä½¿ç”¨éå¸¸æ¿€è¿›ï¼Œå®Œå…¨åˆ æ‰äº†GRUåŸæœ‰çš„çš„æ›´æ–°é—¨ï¼ŒGRUä½ çš„è„‘å­å½’Attentionç®¡äº†ï¼Œé‡åˆ°ç›¸å…³æ€§é«˜çš„å¹¿å‘Šï¼Œä¸€å®šå¤§å¤§åœ°è®°ä¸Šä¸€ç¬”ã€‚ä¸è¿‡AGRUä¹Ÿæœ‰ä¸€ä¸ªç¼ºé™·ï¼Œé‚£å°±æ˜¯Attentionå¾—åˆ†å®é™…ä¸Šæ˜¯ä¸ªæ ‡é‡ï¼Œæ— æ³•ååº”ä¸åŒç»´åº¦çš„å·®å¼‚ã€‚\n",
    "\n",
    "AUGRUä¹Ÿæ˜¯æ”¹å˜äº†GRUçš„ç»“æ„çš„ï¼Œå¹¶ä¸”å¯¹Attentionçš„ä½¿ç”¨æ¯”è¾ƒæŠ˜è¡·ï¼Œè®©Attentionç¼©æ”¾GRUåŸæœ‰çš„æ›´æ–°å¹…åº¦ã€‚GRUæˆ‘ç»™ä½ æ‰¾äº†ä¸ªæ­æ¡£Attentionï¼Œä½ æ›´æ–°å‰å…ˆé—®é—®å®ƒï¼Œä½ ä¸¤ä¸€èµ·å†³å®šè¯¥è¿ˆå¤šå¤§çš„æ­¥å­å§ã€‚\n",
    "\n",
    "DIENè®ºæ–‡ä¸­é€šè¿‡å¯¹æ¯”å®éªŒå‘ç°AUGRUçš„æ•ˆæœæœ€å¥½ã€‚\n",
    "\n",
    "æˆ‘ä»¬çœ‹çœ‹AUGRUçš„æ ¸å¿ƒå®ç°ä»£ç ã€‚åŸºæœ¬ä¸Šå’Œå…¬å¼æ˜¯ä¸€è‡´çš„ï¼Œåº”ç”¨äº†F.linearå‡½æ•°æ¥å®ç°çŸ©é˜µä¹˜æ³•å’ŒåŠ åç½®ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950dd47b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "class AttentionUpdateGateGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # (Wu|Wr|Wn)\n",
    "        self.weight_ih = nn.Parameter(\n",
    "            torch.Tensor(3 * hidden_size, input_size))\n",
    "        # (Uu|Ur|Un)\n",
    "        self.weight_hh = nn.Parameter(\n",
    "            torch.Tensor(3 * hidden_size, hidden_size))\n",
    "        if bias:\n",
    "            # (b_iu|b_ir|b_in)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            # (b_hu|b_hr|b_hn)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / (self.hidden_size)**0.5\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "            \n",
    "    def forward(self, x, hx, att_score):\n",
    "        gi = F.linear(x, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_r, i_u, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_u, h_n = gh.chunk(3, 1)\n",
    "\n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        updategate = torch.sigmoid(i_u + h_u)\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
    "\n",
    "        updategate = att_score.view(-1, 1) * updategate\n",
    "        hy = (1-updategate)*hx +  updategate*newgate\n",
    "\n",
    "        return hy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b9c3e",
   "metadata": {},
   "source": [
    "## äºŒï¼ŒDIENçš„pytorchå®ç°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cee47",
   "metadata": {},
   "source": [
    "ä¸‹é¢æ˜¯ä¸€ä¸ªDIENæ¨¡å‹çš„å®Œæ•´pytorchå®ç°ã€‚è®¸å¤šä»£ç å’ŒDINçš„å®ç°æ˜¯ä¸€æ ·çš„ã€‚\n",
    "\n",
    "è¿™é‡Œçš„AttentionGroupç±»ç”¨æ¥å»ºç«‹å€™é€‰å¹¿å‘Šå±æ€§ï¼Œå†å²å¹¿å‘Šå±æ€§ï¼Œä»¥åŠè´Ÿé‡‡æ ·çš„å¹¿å‘Šå±æ€§çš„pairå…³ç³»ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from collections import OrderedDict\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.max(input, self.dim)[0]\n",
    "\n",
    "\n",
    "class SumPooling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SumPooling, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.sum(input, self.dim)\n",
    "\n",
    "class Dice(nn.Module):\n",
    "    \"\"\"\n",
    "    The Data Adaptive Activation Function in DIN, a generalization of PReLu.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size, dim=2, epsilon=1e-8):\n",
    "        super(Dice, self).__init__()\n",
    "        assert dim == 2 or dim == 3\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(emb_size, eps=epsilon)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # wrap alpha in nn.Parameter to make it trainable\n",
    "        self.alpha = nn.Parameter(torch.zeros((emb_size,))) if self.dim == 2 else nn.Parameter(\n",
    "            torch.zeros((emb_size, 1)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == self.dim\n",
    "        if self.dim == 2:\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "        else:\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "def get_activation_layer(name, hidden_size=None, dice_dim=2):\n",
    "    name = name.lower()\n",
    "    name_dict = {x.lower():x for x in dir(nn) if '__' not in x and 'Z'>=x[0]>='A'}\n",
    "    if name==\"linear\":\n",
    "        return Identity()\n",
    "    elif name==\"dice\":\n",
    "        assert dice_dim\n",
    "        return Dice(hidden_size, dice_dim)\n",
    "    else:\n",
    "        assert name in name_dict, f'activation type {name} not supported!'\n",
    "        return getattr(nn,name_dict[name])()\n",
    "    \n",
    "def init_weights(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        if model.weight is not None:\n",
    "            nn.init.kaiming_uniform_(model.weight.data)\n",
    "        if model.bias is not None:\n",
    "            nn.init.normal_(model.bias.data)\n",
    "    elif isinstance(model, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):\n",
    "        if model.weight is not None:\n",
    "            nn.init.normal_(model.weight.data, mean=1, std=0.02)\n",
    "        if model.bias is not None:\n",
    "            nn.init.constant_(model.bias.data, 0)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers,\n",
    "                 dropout=0.0, batchnorm=True, activation='relu'):\n",
    "        super(MLP, self).__init__()\n",
    "        modules = OrderedDict()\n",
    "        previous_size = input_size\n",
    "        for index, hidden_layer in enumerate(hidden_layers):\n",
    "            modules[f\"dense{index}\"] = nn.Linear(previous_size, hidden_layer)\n",
    "            if batchnorm:\n",
    "                modules[f\"batchnorm{index}\"] = nn.BatchNorm1d(hidden_layer)\n",
    "            if activation:\n",
    "                modules[f\"activation{index}\"] = get_activation_layer(activation,hidden_layer,2)\n",
    "            if dropout:\n",
    "                modules[f\"dropout{index}\"] = nn.Dropout(dropout)\n",
    "            previous_size = hidden_layer\n",
    "        self.mlp = nn.Sequential(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class AttentionGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # (Wr|Wn)\n",
    "        self.weight_ih = nn.Parameter(\n",
    "            torch.Tensor(2 * hidden_size, input_size))\n",
    "        # (Ur|Un)\n",
    "        self.weight_hh = nn.Parameter(\n",
    "            torch.Tensor(2 * hidden_size, hidden_size))\n",
    "        if bias:\n",
    "            # (b_ir|b_in)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(2 * hidden_size))\n",
    "            # (b_hr|b_hn)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(2 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / (self.hidden_size)**0.5\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x, hx, att_score):\n",
    "\n",
    "        gi = F.linear(x, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_r, i_n = gi.chunk(2, 1)\n",
    "        h_r, h_n = gh.chunk(2, 1)\n",
    "\n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
    "        att_score = att_score.view(-1, 1)\n",
    "        hy = (1. - att_score) * hx + att_score * newgate\n",
    "        \n",
    "        return hy\n",
    "\n",
    "\n",
    "class AttentionUpdateGateGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # (Wu|Wr|Wn)\n",
    "        self.weight_ih = nn.Parameter(\n",
    "            torch.Tensor(3 * hidden_size, input_size))\n",
    "        # (Uu|Ur|Un)\n",
    "        self.weight_hh = nn.Parameter(\n",
    "            torch.Tensor(3 * hidden_size, hidden_size))\n",
    "        if bias:\n",
    "            # (b_iu|b_ir|b_in)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            # (b_hu|b_hr|b_hn)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / (self.hidden_size)**0.5\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "            \n",
    "    def forward(self, x, hx, att_score):\n",
    "        gi = F.linear(x, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_u,i_r, i_n = gi.chunk(3, 1)\n",
    "        h_u,h_r, h_n = gh.chunk(3, 1)\n",
    "\n",
    "        updategate = torch.sigmoid(i_u + h_u)\n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
    "\n",
    "        updategate = att_score.view(-1, 1) * updategate\n",
    "        hy = (1-updategate)*hx +  updategate*newgate\n",
    "\n",
    "        return hy\n",
    "\n",
    "\n",
    "\n",
    "class DynamicGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True, gru_type='AGRU'):\n",
    "        super(DynamicGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if gru_type == 'AGRU':\n",
    "            self.rnn = AttentionGRUCell(input_size, hidden_size, bias)\n",
    "        elif gru_type == 'AUGRU':\n",
    "            self.rnn = AttentionUpdateGateGRUCell(\n",
    "                input_size, hidden_size, bias)\n",
    "\n",
    "    def forward(self, x, att_scores, hx=None):\n",
    "        is_packed_input = isinstance(x, nn.utils.rnn.PackedSequence)\n",
    "        if not is_packed_input:\n",
    "            raise NotImplementedError(\n",
    "                \"DynamicGRU only supports packed input\")\n",
    "\n",
    "        is_packed_att_scores = isinstance(att_scores, nn.utils.rnn.PackedSequence)\n",
    "        if not is_packed_att_scores:\n",
    "            raise NotImplementedError(\n",
    "                \"DynamicGRU only supports packed att_scores\")\n",
    "\n",
    "        x, batch_sizes, sorted_indices, unsorted_indices = x\n",
    "        att_scores, _, _, _ = att_scores\n",
    "\n",
    "        max_batch_size = batch_sizes[0]\n",
    "        max_batch_size = int(max_batch_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(\n",
    "                max_batch_size, self.hidden_size,\n",
    "                dtype=x.dtype, device=x.device)\n",
    "\n",
    "        outputs = torch.zeros(\n",
    "            x.size(0), self.hidden_size,\n",
    "            dtype=x.dtype, device=x.device)\n",
    "\n",
    "        begin = 0\n",
    "        for batch in batch_sizes:\n",
    "            new_hx = self.rnn(\n",
    "                x[begin: begin + batch],\n",
    "                hx[0:batch],\n",
    "                att_scores[begin: begin + batch])\n",
    "            outputs[begin: begin + batch] = new_hx\n",
    "            hx = new_hx\n",
    "            begin += batch\n",
    "\n",
    "        return nn.utils.rnn.PackedSequence(\n",
    "            outputs, batch_sizes, sorted_indices, unsorted_indices)\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_layers,\n",
    "            dropout=0.0,\n",
    "            batchnorm=True,\n",
    "            activation='prelu',\n",
    "            return_scores=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.return_scores = return_scores\n",
    "        \n",
    "        self.mlp = MLP(\n",
    "            input_size=input_size * 4,\n",
    "            hidden_layers=hidden_layers,\n",
    "            dropout=dropout,\n",
    "            batchnorm=batchnorm,\n",
    "            activation=activation)\n",
    "        self.fc = nn.Linear(hidden_layers[-1], 1)\n",
    "\n",
    "    def forward(self, query, keys, keys_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: 2D tensor, [Batch, Hidden]\n",
    "        keys: 3D tensor, [Batch, Time, Hidden]\n",
    "        keys_length: 1D tensor, [Batch]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: 2D tensor, [Batch, Hidden]\n",
    "        \"\"\"\n",
    "        batch_size, max_length, dim = keys.size()\n",
    "\n",
    "        query = query.unsqueeze(1).expand(-1, max_length, -1)\n",
    "\n",
    "        din_all = torch.cat(\n",
    "            [query, keys, query - keys, query * keys], dim=-1)\n",
    "\n",
    "        din_all = din_all.view(batch_size * max_length, -1)\n",
    "\n",
    "        outputs = self.mlp(din_all)\n",
    "\n",
    "        outputs = self.fc(outputs).view(batch_size, max_length)  # [B, T]\n",
    "\n",
    "        # Scale\n",
    "        outputs = outputs / (dim ** 0.5)\n",
    "\n",
    "        # Mask\n",
    "        mask = (torch.arange(max_length, device=keys_length.device).repeat(\n",
    "            batch_size, 1) < keys_length.view(-1, 1))\n",
    "        outputs[~mask] = -np.inf\n",
    "\n",
    "        # Activation\n",
    "        outputs = F.softmax(outputs, dim=1)  #DIN uses sigmoid,DIEN uses softmax; [B, T]\n",
    "\n",
    "        if not self.return_scores:\n",
    "            # Weighted sum\n",
    "            outputs = torch.matmul(\n",
    "                outputs.unsqueeze(1), keys).squeeze()  # [B, H]\n",
    "        return outputs \n",
    "    \n",
    "class AuxiliaryNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        modules = OrderedDict()\n",
    "        previous_size = input_size\n",
    "        for index, hidden_layer in enumerate(hidden_layers):\n",
    "            modules[f\"dense{index}\"] = nn.Linear(previous_size, hidden_layer)\n",
    "            if activation:\n",
    "                modules[f\"activation{index}\"] = get_activation_layer(activation)\n",
    "            previous_size = hidden_layer\n",
    "        modules[\"final_layer\"] = nn.Linear(previous_size, 1)\n",
    "        self.mlp = nn.Sequential(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.mlp(x))\n",
    "\n",
    "\n",
    "class Interest(nn.Module):\n",
    "    SUPPORTED_GRU_TYPE = ['GRU', 'AIGRU', 'AGRU', 'AUGRU']\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            gru_type='AUGRU',\n",
    "            gru_dropout=0.0,\n",
    "            att_hidden_layers=[80, 40],\n",
    "            att_dropout=0.0,\n",
    "            att_batchnorm=True,\n",
    "            att_activation='prelu',\n",
    "            use_negsampling=False):\n",
    "        super(Interest, self).__init__()\n",
    "        if gru_type not in Interest.SUPPORTED_GRU_TYPE:\n",
    "            raise NotImplementedError(f\"gru_type: {gru_type} is not supported\")\n",
    "\n",
    "        self.gru_type = gru_type\n",
    "        self.use_negsampling = use_negsampling\n",
    "\n",
    "        self.interest_extractor = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=input_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=False)\n",
    "\n",
    "        if self.use_negsampling:\n",
    "            self.auxiliary_net = AuxiliaryNet(\n",
    "                input_size * 2, hidden_layers=[100, 50])\n",
    "\n",
    "        if gru_type == 'GRU':\n",
    "            self.attention = Attention(\n",
    "                input_size=input_size,\n",
    "                hidden_layers=att_hidden_layers,\n",
    "                dropout=att_dropout,\n",
    "                batchnorm=att_batchnorm,\n",
    "                activation=att_activation)\n",
    "            \n",
    "            self.interest_evolution = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=input_size,\n",
    "                batch_first=True,\n",
    "                bidirectional=False)\n",
    "                \n",
    "        elif gru_type == 'AIGRU':\n",
    "            self.attention = Attention(\n",
    "                input_size=input_size,\n",
    "                hidden_layers=att_hidden_layers,\n",
    "                dropout=att_dropout,\n",
    "                batchnorm=att_batchnorm,\n",
    "                activation=att_activation,\n",
    "                return_scores=True)\n",
    "\n",
    "            self.interest_evolution = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=input_size,\n",
    "                batch_first=True,\n",
    "                bidirectional=False)\n",
    "            \n",
    "        elif gru_type == 'AGRU' or gru_type == 'AUGRU':\n",
    "            self.attention = Attention(\n",
    "                input_size=input_size,\n",
    "                hidden_layers=att_hidden_layers,\n",
    "                dropout=att_dropout,\n",
    "                batchnorm=att_batchnorm,\n",
    "                activation=att_activation,\n",
    "                return_scores=True)\n",
    "\n",
    "            self.interest_evolution = DynamicGRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=input_size,\n",
    "                gru_type=gru_type)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_last_state(states, keys_length):\n",
    "        # states [B, T, H]\n",
    "        batch_size, max_seq_length, hidden_size = states.size()\n",
    "\n",
    "        mask = (torch.arange(max_seq_length, device=keys_length.device).repeat(\n",
    "            batch_size, 1) == (keys_length.view(-1, 1) - 1))\n",
    "\n",
    "        return states[mask]\n",
    "\n",
    "    def cal_auxiliary_loss(\n",
    "            self, states, click_seq, noclick_seq, keys_length):\n",
    "        # states [B, T, H]\n",
    "        # click_seq [B, T, H]\n",
    "        # noclick_seq [B, T, H]\n",
    "        # keys_length [B]\n",
    "        batch_size, max_seq_length, embedding_size = states.size()\n",
    "\n",
    "        mask = (torch.arange(max_seq_length, device=states.device).repeat(\n",
    "            batch_size, 1) < keys_length.view(-1, 1)).float()\n",
    "\n",
    "        click_input = torch.cat([states, click_seq], dim=-1)\n",
    "        noclick_input = torch.cat([states, noclick_seq], dim=-1)\n",
    "        embedding_size = embedding_size * 2\n",
    "\n",
    "        click_p = self.auxiliary_net(\n",
    "            click_input.view(\n",
    "                batch_size * max_seq_length, embedding_size)).view(\n",
    "                    batch_size, max_seq_length)[mask > 0].view(-1, 1)\n",
    "        click_target = torch.ones(\n",
    "            click_p.size(), dtype=torch.float, device=click_p.device)\n",
    "\n",
    "        noclick_p = self.auxiliary_net(\n",
    "            noclick_input.view(\n",
    "                batch_size * max_seq_length, embedding_size)).view(\n",
    "                    batch_size, max_seq_length)[mask > 0].view(-1, 1)\n",
    "        noclick_target = torch.zeros(\n",
    "            noclick_p.size(), dtype=torch.float, device=noclick_p.device)\n",
    "\n",
    "        loss = F.binary_cross_entropy(\n",
    "            torch.cat([click_p, noclick_p], dim=0),\n",
    "            torch.cat([click_target, noclick_target], dim=0))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, query, keys, keys_length, neg_keys=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: 2D tensor, [Batch, Hidden]\n",
    "        keys: 3D tensor, [Batch, Time, Hidden]\n",
    "        keys_length: 1D tensor, [Batch]\n",
    "        neg_keys: 3D tensor, [Batch, Time, Hidden]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: 2D tensor, [Batch, Hidden]\n",
    "        \"\"\"\n",
    "        batch_size, max_length, dim = keys.size()\n",
    "\n",
    "        packed_keys = pack_padded_sequence(\n",
    "            keys,\n",
    "            lengths=keys_length.squeeze().cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False)\n",
    "\n",
    "        packed_interests, _ = self.interest_extractor(packed_keys)\n",
    "\n",
    "        aloss = None\n",
    "        if (self.gru_type != 'GRU') or self.use_negsampling:\n",
    "            interests, _ = pad_packed_sequence(\n",
    "                packed_interests,\n",
    "                batch_first=True,\n",
    "                padding_value=0.0,\n",
    "                total_length=max_length)\n",
    "\n",
    "            if self.use_negsampling:\n",
    "                aloss = self.cal_auxiliary_loss(\n",
    "                    interests[:, :-1, :],\n",
    "                    keys[:, 1:, :],\n",
    "                    neg_keys[:, 1:, :],\n",
    "                    keys_length - 1)\n",
    "\n",
    "        if self.gru_type == 'GRU':\n",
    "            packed_interests, _ = self.interest_evolution(packed_interests)\n",
    "\n",
    "            interests, _ = pad_packed_sequence(\n",
    "                packed_interests,\n",
    "                batch_first=True,\n",
    "                padding_value=0.0,\n",
    "                total_length=max_length)\n",
    "\n",
    "            outputs = self.attention(query, interests, keys_length)\n",
    "\n",
    "        elif self.gru_type == 'AIGRU':\n",
    "            # attention\n",
    "            scores = self.attention(query, interests, keys_length)\n",
    "            interests = interests * scores.unsqueeze(-1)\n",
    "\n",
    "            packed_interests = pack_padded_sequence(\n",
    "                interests,\n",
    "                lengths=keys_length.squeeze().cpu(),\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False)\n",
    "            _, outputs = self.interest_evolution(packed_interests)\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "        elif self.gru_type == 'AGRU' or self.gru_type == 'AUGRU':\n",
    "            # attention\n",
    "            scores = self.attention(query, interests, keys_length)\n",
    "\n",
    "            packed_interests = pack_padded_sequence(\n",
    "                interests,\n",
    "                lengths=keys_length.squeeze().cpu(),\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False)\n",
    "\n",
    "            packed_scores = pack_padded_sequence(\n",
    "                scores,\n",
    "                lengths=keys_length.squeeze().cpu(),\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False)\n",
    "\n",
    "            outputs, _ = pad_packed_sequence(\n",
    "                self.interest_evolution(\n",
    "                    packed_interests, packed_scores), batch_first=True)\n",
    "            # pick last state\n",
    "            outputs = Interest.get_last_state(\n",
    "                outputs, keys_length.squeeze())\n",
    "\n",
    "        return outputs, aloss\n",
    "    \n",
    "class AttentionGroup(object):\n",
    "    def __init__(self, name, pairs,\n",
    "                 hidden_layers, activation='dice', att_dropout=0.0,\n",
    "                 gru_type='AUGRU', gru_dropout=0.0):\n",
    "        self.name = name\n",
    "        self.pairs = pairs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.att_dropout = att_dropout\n",
    "        self.gru_type = gru_type\n",
    "        self.gru_dropout = gru_dropout\n",
    "\n",
    "        self.related_feature_names = set()\n",
    "        self.neg_feature_names = set()\n",
    "        for pair in pairs:\n",
    "            self.related_feature_names.add(pair['ad'])\n",
    "            self.related_feature_names.add(pair['pos_hist'])\n",
    "            if 'neg_hist' in pair:\n",
    "                self.related_feature_names.add(pair['neg_hist'])\n",
    "                self.neg_feature_names.add(pair['neg_hist'])\n",
    "\n",
    "    def is_attention_feature(self, feature_name):\n",
    "        if feature_name in self.related_feature_names:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_neg_sampling_feature(self, feature_name):\n",
    "        if feature_name in self.neg_feature_names:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def pairs_count(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "class DIEN(nn.Module):\n",
    "    def __init__(self, num_features,cat_features,seq_features, \n",
    "                 cat_nums,embedding_size, attention_groups,\n",
    "                 mlp_hidden_layers, mlp_activation='prelu', mlp_dropout=0.0,\n",
    "                 use_negsampling = False,\n",
    "                 d_out = 1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.seq_features = seq_features\n",
    "        self.cat_nums = cat_nums \n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.attention_groups = attention_groups\n",
    "        \n",
    "        self.mlp_hidden_layers = mlp_hidden_layers\n",
    "        self.mlp_activation = mlp_activation\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.use_negsampling = use_negsampling\n",
    "        \n",
    "        #embedding\n",
    "        self.embeddings = OrderedDict()\n",
    "        for feature in self.cat_features+self.seq_features:\n",
    "            self.embeddings[feature] = nn.Embedding(\n",
    "                self.cat_nums[feature], self.embedding_size, padding_idx=0)\n",
    "            self.add_module(f\"embedding:{feature}\",self.embeddings[feature])\n",
    "\n",
    "        self.sequence_poolings = OrderedDict()\n",
    "        self.attention_poolings = OrderedDict()\n",
    "        total_embedding_sizes = 0\n",
    "        for feature in self.cat_features:\n",
    "            total_embedding_sizes += self.embedding_size\n",
    "        for feature in self.seq_features:\n",
    "            if not self.is_neg_sampling_feature(feature):\n",
    "                total_embedding_sizes += self.embedding_size\n",
    "        \n",
    "        #sequence_pooling\n",
    "        for feature in self.seq_features:\n",
    "            if not self.is_attention_feature(feature):\n",
    "                self.sequence_poolings[feature] = MaxPooling(1)\n",
    "                self.add_module(f\"pooling:{feature}\",self.sequence_poolings[feature])\n",
    "\n",
    "        #attention_pooling\n",
    "        for attention_group in self.attention_groups:\n",
    "            self.attention_poolings[attention_group.name] = (\n",
    "                self.create_attention_fn(attention_group))\n",
    "            self.add_module(f\"attention_pooling:{attention_group.name}\",\n",
    "                self.attention_poolings[attention_group.name])\n",
    "\n",
    "        total_input_size = total_embedding_sizes+len(self.num_features)\n",
    "        \n",
    "        self.mlp = MLP(\n",
    "            total_input_size,\n",
    "            mlp_hidden_layers,\n",
    "            dropout=mlp_dropout, batchnorm=True, activation=mlp_activation)\n",
    "        \n",
    "        self.final_layer = nn.Linear(mlp_hidden_layers[-1], self.d_out)\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        final_layer_inputs = list()\n",
    "\n",
    "        # linear\n",
    "        number_inputs = list()\n",
    "        for feature in self.num_features:\n",
    "            number_inputs.append(x[feature].view(-1, 1))\n",
    "\n",
    "        embeddings = OrderedDict()\n",
    "        for feature in self.cat_features:\n",
    "            embeddings[feature] = self.embeddings[feature](x[feature])\n",
    "\n",
    "        for feature in self.seq_features:\n",
    "            if not self.is_attention_feature(feature):\n",
    "                embeddings[feature] = self.sequence_poolings[feature](\n",
    "                    self.embeddings[feature](x[feature]))\n",
    "\n",
    "        auxiliary_losses = []\n",
    "        for attention_group in self.attention_groups:\n",
    "            query = torch.cat(\n",
    "                [embeddings[pair['ad']]\n",
    "                 for pair in attention_group.pairs],\n",
    "                dim=-1)\n",
    "            pos_hist = torch.cat(\n",
    "                [self.embeddings[pair['pos_hist']](\n",
    "                    x[pair['pos_hist']]) for pair in attention_group.pairs],\n",
    "                dim=-1)\n",
    "            \n",
    "            #hist_length = torch.sum(hist>0,axis=1)\n",
    "            keys_length = torch.min(torch.cat(\n",
    "                [torch.sum(x[pair['pos_hist']]>0,axis=1).view(-1, 1)\n",
    "                 for pair in attention_group.pairs],\n",
    "                dim=-1), dim=-1)[0]\n",
    "    \n",
    "            neg_hist = None\n",
    "            if self.use_negsampling:\n",
    "                neg_hist = torch.cat(\n",
    "                    [self.embeddings[pair['neg_hist']](\n",
    "                        x[pair['neg_hist']])\n",
    "                     for pair in attention_group.pairs],\n",
    "                    dim=-1)\n",
    "                \n",
    "            embeddings[attention_group.name], tmp_loss = (\n",
    "                self.attention_poolings[attention_group.name](\n",
    "                    query, pos_hist, keys_length, neg_hist))\n",
    "            if tmp_loss is not None:\n",
    "                auxiliary_losses.append(tmp_loss)\n",
    "\n",
    "        emb_concat = torch.cat(number_inputs + [\n",
    "            emb for emb in embeddings.values()], dim=-1)\n",
    "\n",
    "        final_layer_inputs = self.mlp(emb_concat)\n",
    "\n",
    "        output = self.final_layer(final_layer_inputs)\n",
    "        \n",
    "        auxiliary_avg_loss = None\n",
    "        if auxiliary_losses:\n",
    "            auxiliary_avg_loss = auxiliary_losses[0]\n",
    "            size = len(auxiliary_losses)\n",
    "            for i in range(1, size):\n",
    "                auxiliary_avg_loss += auxiliary_losses[i]\n",
    "            auxiliary_avg_loss /= size\n",
    "            \n",
    "        if  self.d_out==1:\n",
    "            output = output.squeeze() \n",
    "            \n",
    "        return output, auxiliary_avg_loss\n",
    "\n",
    "    def create_attention_fn(self, attention_group):\n",
    "        return Interest(\n",
    "            attention_group.pairs_count * self.embedding_size,\n",
    "            gru_type=attention_group.gru_type,\n",
    "            gru_dropout=attention_group.gru_dropout,\n",
    "            att_hidden_layers=attention_group.hidden_layers,\n",
    "            att_dropout=attention_group.att_dropout,\n",
    "            att_activation=attention_group.activation,\n",
    "            use_negsampling=self.use_negsampling)\n",
    "    \n",
    "    def is_attention_feature(self, feature):\n",
    "        for group in self.attention_groups:\n",
    "            if group.is_attention_feature(feature):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_neg_sampling_feature(self, feature):\n",
    "        for group in self.attention_groups:\n",
    "            if group.is_neg_sampling_feature(feature):\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298364a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ce7fd9e",
   "metadata": {},
   "source": [
    "## ä¸‰ï¼ŒMovielensæ•°æ®é›†å®Œæ•´èŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47364ceb",
   "metadata": {},
   "source": [
    "ä¸‹é¢æ˜¯ä¸€ä¸ªåŸºäºMovielensè¯„ä»·æ•°æ®é›†çš„DIENå®Œæ•´èŒƒä¾‹ï¼Œæ ¹æ®ç”¨æˆ·è¿‡å»å¯¹ä¸€äº›ç”µå½±çš„è¯„ä»·ç»“æœï¼Œæ¥é¢„æµ‹ç”¨æˆ·å¯¹å€™é€‰ç”µå½±æ˜¯å¦ä¼šç»™å¥½è¯„ã€‚\n",
    "\n",
    "è¿™ä¸ªæ•°æ®é›†ä¸å¤§ï¼Œç”¨CPUå°±èƒ½è·‘ã€‚ğŸ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11dd6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d8d160",
   "metadata": {},
   "source": [
    "### 1ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "from sklearn.impute import SimpleImputer \n",
    "from collections import Counter\n",
    "\n",
    "class CategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, min_cnt=5, word2idx=None, idx2word=None):\n",
    "        super().__init__() \n",
    "        self.min_cnt = min_cnt\n",
    "        self.word2idx = word2idx if word2idx else dict()\n",
    "        self.idx2word = idx2word if idx2word else dict()\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        if not self.word2idx:\n",
    "            counter = Counter(np.asarray(x).ravel())\n",
    "\n",
    "            selected_terms = sorted(\n",
    "                list(filter(lambda x: counter[x] >= self.min_cnt, counter)))\n",
    "\n",
    "            self.word2idx = dict(\n",
    "                zip(selected_terms, range(1, len(selected_terms) + 1)))\n",
    "            self.word2idx['__PAD__'] = 0\n",
    "            if '__UNKNOWN__' not in self.word2idx:\n",
    "                self.word2idx['__UNKNOWN__'] = len(self.word2idx)\n",
    "\n",
    "        if not self.idx2word:\n",
    "            self.idx2word = {\n",
    "                index: word for word, index in self.word2idx.items()}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        transformed_x = list()\n",
    "        for term in np.asarray(x).ravel():\n",
    "            try:\n",
    "                transformed_x.append(self.word2idx[term])\n",
    "            except KeyError:\n",
    "                transformed_x.append(self.word2idx['__UNKNOWN__'])\n",
    "\n",
    "        return np.asarray(transformed_x, dtype=np.int64)\n",
    "\n",
    "    def dimension(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class SequenceEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sep=' ', min_cnt=5, max_len=None,\n",
    "                 word2idx=None, idx2word=None):\n",
    "        super().__init__() \n",
    "        self.sep = sep\n",
    "        self.min_cnt = min_cnt\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.word2idx = word2idx if word2idx else dict()\n",
    "        self.idx2word = idx2word if idx2word else dict()\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        if not self.word2idx:\n",
    "            counter = Counter()\n",
    "\n",
    "            max_len = 0\n",
    "            for sequence in np.array(x).ravel():\n",
    "                words = sequence.split(self.sep)\n",
    "                counter.update(words)\n",
    "                max_len = max(max_len, len(words))\n",
    "\n",
    "            if self.max_len is None:\n",
    "                self.max_len = max_len\n",
    "\n",
    "            # drop rare words\n",
    "            words = sorted(\n",
    "                list(filter(lambda x: counter[x] >= self.min_cnt, counter)))\n",
    "\n",
    "            self.word2idx = dict(zip(words, range(1, len(words) + 1)))\n",
    "            self.word2idx['__PAD__'] = 0\n",
    "            if '__UNKNOWN__' not in self.word2idx:\n",
    "                self.word2idx['__UNKNOWN__'] = len(self.word2idx)\n",
    "\n",
    "        if not self.idx2word:\n",
    "            self.idx2word = {\n",
    "                index: word for word, index in self.word2idx.items()}\n",
    "\n",
    "        if not self.max_len:\n",
    "            max_len = 0\n",
    "            for sequence in np.array(x).ravel():\n",
    "                words = sequence.split(self.sep)\n",
    "                max_len = max(max_len, len(words))\n",
    "            self.max_len = max_len\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        transformed_x = list()\n",
    "\n",
    "        for sequence in np.asarray(x).ravel():\n",
    "            words = list()\n",
    "            for word in sequence.split(self.sep):\n",
    "                try:\n",
    "                    words.append(self.word2idx[word])\n",
    "                except KeyError:\n",
    "                    words.append(self.word2idx['__UNKNOWN__'])\n",
    "\n",
    "            transformed_x.append(\n",
    "                np.asarray(words[0:self.max_len], dtype=np.int64))\n",
    "\n",
    "        return np.asarray(transformed_x, dtype=object)\n",
    "    \n",
    "    def dimension(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def max_length(self):\n",
    "        return self.max_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07aff3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2549a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.impute import SimpleImputer \n",
    "from tqdm import tqdm \n",
    "\n",
    "dftrain = pd.read_csv(\"./eat_pytorch_datasets/ml_1m/train.csv\")\n",
    "dfval = pd.read_csv(\"./eat_pytorch_datasets/ml_1m/test.csv\")\n",
    "\n",
    "for col in [\"movieId\",\"histHighRatedMovieIds\",\"negHistMovieIds\",\"genres\"]:\n",
    "    dftrain[col] = dftrain[col].astype(str)\n",
    "    dfval[col] = dfval[col].astype(str)\n",
    "\n",
    "num_features = ['age']\n",
    "cat_features = ['gender', 'movieId', 'occupation', 'zipCode']\n",
    "seq_features = ['genres', 'histHighRatedMovieIds', 'negHistMovieIds']\n",
    "\n",
    "num_pipe = Pipeline(steps = [('impute',SimpleImputer()),('quantile',QuantileTransformer())])\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "print(\"preprocess number features...\")\n",
    "dftrain[num_features] = num_pipe.fit_transform(dftrain[num_features]).astype(np.float32)\n",
    "dfval[num_features] = num_pipe.transform(dfval[num_features]).astype(np.float32)\n",
    "\n",
    "print(\"preprocess category features...\")\n",
    "for col in tqdm(cat_features):\n",
    "    encoders[col] = CategoryEncoder(min_cnt=5)\n",
    "    dftrain[col]  = encoders[col].fit_transform(dftrain[col])\n",
    "    dfval[col] =  encoders[col].transform(dfval[col])\n",
    "    \n",
    "print(\"preprocess sequence features...\")\n",
    "for col in tqdm(seq_features):\n",
    "    encoders[col] = SequenceEncoder(sep=\"|\",min_cnt=5)\n",
    "    dftrain[col]  = encoders[col].fit_transform(dftrain[col])\n",
    "    dfval[col] =  encoders[col].transform(dfval[col])\n",
    "    \n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "class Df2Dataset(Dataset):\n",
    "    def __init__(self, dfdata, num_features, cat_features,\n",
    "                 seq_features, encoders, label_col=\"label\"):\n",
    "        self.dfdata = dfdata\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features \n",
    "        self.seq_features = seq_features\n",
    "        self.encoders = encoders\n",
    "        self.label_col = label_col\n",
    "        self.size = len(self.dfdata)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence,max_length):\n",
    "        #zero is special index for padding\n",
    "        padded_seq = np.zeros(max_length, np.int32)\n",
    "        padded_seq[0: sequence.shape[0]] = sequence\n",
    "        return padded_seq\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = OrderedDict()\n",
    "        for col in self.num_features:\n",
    "            record[col] = self.dfdata[col].iloc[idx].astype(np.float32)\n",
    "            \n",
    "        for col in self.cat_features:\n",
    "            record[col] = self.dfdata[col].iloc[idx].astype(np.int64)\n",
    "            \n",
    "        for col in self.seq_features:\n",
    "            seq = self.dfdata[col].iloc[idx]\n",
    "            max_length = self.encoders[col].max_length()\n",
    "            record[col] = Df2Dataset.pad_sequence(seq,max_length)\n",
    "\n",
    "        if self.label_col is not None:\n",
    "            record['label'] = self.dfdata[self.label_col].iloc[idx].astype(np.float32)\n",
    "        return record\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return np.ceil(self.size / batch_size)\n",
    "    \n",
    "ds_train = Df2Dataset(dftrain, num_features, cat_features, seq_features, encoders)\n",
    "ds_val = Df2Dataset(dfval,num_features, cat_features, seq_features, encoders)\n",
    "dl_train = DataLoader(ds_train, batch_size=128,shuffle=True)\n",
    "dl_val = DataLoader(ds_val,batch_size=128,shuffle=False)\n",
    "\n",
    "cat_nums = {k:v.dimension() for k,v in encoders.items()} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl_train:\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73039199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbbfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a9ce12",
   "metadata": {},
   "source": [
    "### 2ï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_net():\n",
    "    augru_attention_groups_with_neg = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'movieId', 'pos_hist': 'histHighRatedMovieIds', 'neg_hist': 'negHistMovieIds'}],\n",
    "        hidden_layers=[16, 8], att_dropout=0.1, gru_type='AUGRU')\n",
    "    ]\n",
    "\n",
    "    net = DIEN(num_features=num_features,\n",
    "           cat_features=cat_features,\n",
    "           seq_features=seq_features,\n",
    "           cat_nums = cat_nums,\n",
    "           embedding_size=16,\n",
    "           attention_groups=augru_attention_groups_with_neg,\n",
    "           mlp_hidden_layers=[32,16],\n",
    "           mlp_activation=\"prelu\",\n",
    "           mlp_dropout=0.25,\n",
    "           use_negsampling=True,\n",
    "           d_out=1\n",
    "           )\n",
    "    \n",
    "    return net \n",
    "\n",
    "net = create_net() \n",
    "\n",
    "out,aloss = net.forward(batch)\n",
    "\n",
    "from torchkeras.summary import summary \n",
    "summary(net,input_data=batch);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15722f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10909969",
   "metadata": {},
   "source": [
    "### 3ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64381bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ab7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from accelerate import Accelerator\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(str(info)+\"\\n\")\n",
    "    \n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn,stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None,\n",
    "                 accelerator = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        #loss\n",
    "        preds,aloss = self.net(batch)\n",
    "        loss = self.loss_fn(preds,batch[\"label\"])+aloss\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            if self.accelerator is  None:\n",
    "                loss.backward()\n",
    "            else:\n",
    "                self.accelerator.backward(loss)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        #metrics\n",
    "        step_metrics = {self.stage+\"_\"+name:metric_fn(preds, batch[\"label\"]).item() \n",
    "                        for name,metric_fn in self.metrics_dict.items()}\n",
    "        return loss.item(),step_metrics\n",
    "    \n",
    "    \n",
    "class EpochRunner:\n",
    "    def __init__(self,steprunner):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        self.steprunner.net.train() if self.stage==\"train\" else self.steprunner.net.eval()\n",
    "        \n",
    "    def __call__(self,dataloader):\n",
    "        total_loss,step = 0,0\n",
    "        loop = tqdm(enumerate(dataloader), total =len(dataloader))\n",
    "        for i, batch in loop:\n",
    "            if self.stage==\"train\":\n",
    "                loss, step_metrics = self.steprunner(batch)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    loss, step_metrics = self.steprunner(batch)\n",
    "\n",
    "            step_log = dict({self.stage+\"_loss\":loss},**step_metrics)\n",
    "\n",
    "            total_loss += loss\n",
    "            step+=1\n",
    "            if i!=len(dataloader)-1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss/step\n",
    "                epoch_metrics = {self.stage+\"_\"+name:metric_fn.compute().item() \n",
    "                                 for name,metric_fn in self.steprunner.metrics_dict.items()}\n",
    "                epoch_log = dict({self.stage+\"_loss\":epoch_loss},**epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name,metric_fn in self.steprunner.metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "        return epoch_log\n",
    "\n",
    "class KerasModel(torch.nn.Module):\n",
    "    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler = None):\n",
    "        super().__init__()\n",
    "        self.accelerator = Accelerator()\n",
    "        self.history = {}\n",
    "        \n",
    "        self.net = net\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metrics_dict = nn.ModuleDict(metrics_dict) \n",
    "        \n",
    "        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n",
    "            self.parameters(), lr=1e-2)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        \n",
    "        self.net,self.loss_fn,self.metrics_dict,self.optimizer = self.accelerator.prepare(\n",
    "            self.net,self.loss_fn,self.metrics_dict,self.optimizer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.net:\n",
    "            return self.net.forward(x)[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint.pt', \n",
    "            patience=5, monitor=\"val_loss\", mode=\"min\"):\n",
    "        \n",
    "        train_data = self.accelerator.prepare(train_data)\n",
    "        val_data = self.accelerator.prepare(val_data) if val_data else []\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "            \n",
    "            # 1ï¼Œtrain -------------------------------------------------  \n",
    "            train_step_runner = StepRunner(net = self.net,stage=\"train\",\n",
    "                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n",
    "                    optimizer = self.optimizer, lr_scheduler = self.lr_scheduler,\n",
    "                    accelerator = self.accelerator)\n",
    "            train_epoch_runner = EpochRunner(train_step_runner)\n",
    "            train_metrics = train_epoch_runner(train_data)\n",
    "            \n",
    "            for name, metric in train_metrics.items():\n",
    "                self.history[name] = self.history.get(name, []) + [metric]\n",
    "\n",
    "            # 2ï¼Œvalidate -------------------------------------------------\n",
    "            if val_data:\n",
    "                val_step_runner = StepRunner(net = self.net,stage=\"val\",\n",
    "                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n",
    "                    accelerator = self.accelerator)\n",
    "                val_epoch_runner = EpochRunner(val_step_runner)\n",
    "                with torch.no_grad():\n",
    "                    val_metrics = val_epoch_runner(val_data)\n",
    "                val_metrics[\"epoch\"] = epoch\n",
    "                for name, metric in val_metrics.items():\n",
    "                    self.history[name] = self.history.get(name, []) + [metric]\n",
    "            \n",
    "            # 3ï¼Œearly-stopping -------------------------------------------------\n",
    "            arr_scores = self.history[monitor]\n",
    "            best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "            if best_score_idx==len(arr_scores)-1:\n",
    "                torch.save(self.net.state_dict(),ckpt_path)\n",
    "                print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                     arr_scores[best_score_idx]),file=sys.stderr)\n",
    "            if len(arr_scores)-best_score_idx>patience:\n",
    "                print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "                    monitor,patience),file=sys.stderr)\n",
    "                break \n",
    "                \n",
    "        self.net.load_state_dict(torch.load(ckpt_path))\n",
    "        return pd.DataFrame(self.history)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_data):\n",
    "        val_data = self.accelerator.prepare(val_data)\n",
    "        val_step_runner = StepRunner(net = self.net,stage=\"val\",\n",
    "                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n",
    "                    accelerator = self.accelerator)\n",
    "        val_epoch_runner = EpochRunner(val_step_runner)\n",
    "        val_metrics = val_epoch_runner(val_data)\n",
    "        return val_metrics\n",
    "        \n",
    "       \n",
    "    @torch.no_grad()\n",
    "    def predict(self, dataloader):\n",
    "        dataloader = self.accelerator.prepare(dataloader)\n",
    "        self.net.eval()\n",
    "        result = torch.cat([self.forward(t) for t in dataloader])\n",
    "        return result.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41baccf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c23e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras.metrics import AUC\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "metrics_dict = {\"auc\":AUC()}\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.002, weight_decay=0.001) \n",
    "\n",
    "model = KerasModel(net,\n",
    "                   loss_fn = loss_fn,\n",
    "                   metrics_dict= metrics_dict,\n",
    "                   optimizer = optimizer\n",
    "                  )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bf220",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhistory = model.fit(train_data=dl_train,val_data=dl_val,epochs=100, patience=10,\n",
    "                      monitor = \"val_auc\",mode=\"max\",ckpt_path='checkpoint.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d2931",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h3z8ccgn9ij20p507qmy7.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c57a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9340ed18",
   "metadata": {},
   "source": [
    "### 4ï¼Œè¯„ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[\"train_\"+metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc705d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory,\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16172f",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h3z8afwf3zj20f20aiglx.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9136b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory,\"auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac7283",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h3z8ddym3bj20f20ab74o.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9806e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d6c18",
   "metadata": {},
   "source": [
    "{'val_loss': 0.7020544648170471, 'val_auc': 0.6469045281410217}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce035e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a4dfbe",
   "metadata": {},
   "source": [
    "### 5ï¼Œä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "labels = torch.tensor([x[\"label\"] for x in ds_val])\n",
    "preds = model.predict(dl_val)\n",
    "val_auc = roc_auc_score(labels.cpu().numpy(),preds.cpu().numpy())\n",
    "print(val_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6f35a",
   "metadata": {},
   "source": [
    "```\n",
    "0.6469045283797497\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5c24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a69ecb80",
   "metadata": {},
   "source": [
    "### 6ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.net.state_dict(),\"best_dien.pt\")\n",
    "net_clone = create_net()\n",
    "net_clone.load_state_dict(torch.load(\"best_dien.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fa7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_clone.eval()\n",
    "labels = torch.tensor([x[\"label\"] for x in ds_val])\n",
    "preds = torch.cat([net_clone(x)[0].data for x in dl_val]) \n",
    "val_auc = roc_auc_score(labels.cpu().numpy(),preds.cpu().numpy())\n",
    "print(val_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b62168",
   "metadata": {},
   "source": [
    "```\n",
    "0.6469045283797497\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7801172",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"ç®—æ³•ç¾é£Ÿå±‹\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚\n",
    "\n",
    "![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
