# 2-3,åŠ¨æ€è®¡ç®—å›¾


æœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç» Pytorchçš„åŠ¨æ€è®¡ç®—å›¾ã€‚

åŒ…æ‹¬ï¼š 

* åŠ¨æ€è®¡ç®—å›¾ç®€ä»‹

* è®¡ç®—å›¾ä¸­çš„Function

* è®¡ç®—å›¾å’Œåå‘ä¼ æ’­

* å¶å­èŠ‚ç‚¹å’Œéå¶å­èŠ‚ç‚¹

* è®¡ç®—å›¾åœ¨TensorBoardä¸­çš„å¯è§†åŒ–




### ä¸€ï¼ŒåŠ¨æ€è®¡ç®—å›¾ç®€ä»‹


![](./data/torchåŠ¨æ€å›¾.gif)


Pytorchçš„è®¡ç®—å›¾ç”±èŠ‚ç‚¹å’Œè¾¹ç»„æˆï¼ŒèŠ‚ç‚¹è¡¨ç¤ºå¼ é‡æˆ–è€…Functionï¼Œè¾¹è¡¨ç¤ºå¼ é‡å’ŒFunctionä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

Pytorchä¸­çš„è®¡ç®—å›¾æ˜¯åŠ¨æ€å›¾ã€‚è¿™é‡Œçš„åŠ¨æ€ä¸»è¦æœ‰ä¸¤é‡å«ä¹‰ã€‚

ç¬¬ä¸€å±‚å«ä¹‰æ˜¯ï¼šè®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­æ˜¯ç«‹å³æ‰§è¡Œçš„ã€‚æ— éœ€ç­‰å¾…å®Œæ•´çš„è®¡ç®—å›¾åˆ›å»ºå®Œæ¯•ï¼Œæ¯æ¡è¯­å¥éƒ½ä¼šåœ¨è®¡ç®—å›¾ä¸­åŠ¨æ€æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ï¼Œå¹¶ç«‹å³æ‰§è¡Œæ­£å‘ä¼ æ’­å¾—åˆ°è®¡ç®—ç»“æœã€‚

ç¬¬äºŒå±‚å«ä¹‰æ˜¯ï¼šè®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ã€‚ä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°æ„å»ºè®¡ç®—å›¾ã€‚å¦‚æœåœ¨ç¨‹åºä¸­ä½¿ç”¨äº†backwardæ–¹æ³•æ‰§è¡Œäº†åå‘ä¼ æ’­ï¼Œæˆ–è€…åˆ©ç”¨torch.autograd.gradæ–¹æ³•è®¡ç®—äº†æ¢¯åº¦ï¼Œé‚£ä¹ˆåˆ›å»ºçš„è®¡ç®—å›¾ä¼šè¢«ç«‹å³é”€æ¯ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´ï¼Œä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°åˆ›å»ºã€‚



**1ï¼Œè®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­æ˜¯ç«‹å³æ‰§è¡Œçš„ã€‚**

```python
import torch 
w = torch.tensor([[3.0,1.0]],requires_grad=True)
b = torch.tensor([[3.0]],requires_grad=True)
X = torch.randn(10,2)
Y = torch.randn(10,1)
Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³
loss = torch.mean(torch.pow(Y_hat-Y,2))

print(loss.data)
print(Y_hat.data)
```

```
tensor(17.8969)
tensor([[3.2613],
        [4.7322],
        [4.5037],
        [7.5899],
        [7.0973],
        [1.3287],
        [6.1473],
        [1.3492],
        [1.3911],
        [1.2150]])
```

```python

```

**2ï¼Œè®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ã€‚**

```python
import torch 
w = torch.tensor([[3.0,1.0]],requires_grad=True)
b = torch.tensor([[3.0]],requires_grad=True)
X = torch.randn(10,2)
Y = torch.randn(10,1)
Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³
loss = torch.mean(torch.pow(Y_hat-Y,2))

#è®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ï¼Œå¦‚æœéœ€è¦ä¿ç•™è®¡ç®—å›¾, éœ€è¦è®¾ç½®retain_graph = True
loss.backward()  #loss.backward(retain_graph = True) 

#loss.backward() #å¦‚æœå†æ¬¡æ‰§è¡Œåå‘ä¼ æ’­å°†æŠ¥é”™

```

```python

```

### äºŒï¼Œè®¡ç®—å›¾ä¸­çš„Function


è®¡ç®—å›¾ä¸­çš„ å¼ é‡æˆ‘ä»¬å·²ç»æ¯”è¾ƒç†Ÿæ‚‰äº†, è®¡ç®—å›¾ä¸­çš„å¦å¤–ä¸€ç§èŠ‚ç‚¹æ˜¯Function, å®é™…ä¸Šå°±æ˜¯ Pytorchä¸­å„ç§å¯¹å¼ é‡æ“ä½œçš„å‡½æ•°ã€‚

è¿™äº›Functionå’Œæˆ‘ä»¬Pythonä¸­çš„å‡½æ•°æœ‰ä¸€ä¸ªè¾ƒå¤§çš„åŒºåˆ«ï¼Œé‚£å°±æ˜¯å®ƒåŒæ—¶åŒ…æ‹¬æ­£å‘è®¡ç®—é€»è¾‘å’Œåå‘ä¼ æ’­çš„é€»è¾‘ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿torch.autograd.Functionæ¥åˆ›å»ºè¿™ç§æ”¯æŒåå‘ä¼ æ’­çš„Function


```python
class MyReLU(torch.autograd.Function):
   
    #æ­£å‘ä¼ æ’­é€»è¾‘ï¼Œå¯ä»¥ç”¨ctxå­˜å‚¨ä¸€äº›å€¼ï¼Œä¾›åå‘ä¼ æ’­ä½¿ç”¨ã€‚
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    #åå‘ä¼ æ’­é€»è¾‘
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input
    
```

```python
import torch 
w = torch.tensor([[3.0,1.0]],requires_grad=True)
b = torch.tensor([[3.0]],requires_grad=True)
X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])
Y = torch.tensor([[2.0,3.0]])

relu = MyReLU.apply # reluç°åœ¨ä¹Ÿå¯ä»¥å…·æœ‰æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­åŠŸèƒ½
Y_hat = relu(X@w.t() + b)
loss = torch.mean(torch.pow(Y_hat-Y,2))

loss.backward()

print(w.grad)
print(b.grad)
```

```
tensor([[4.5000, 4.5000]])
tensor([[4.5000]])
```

```python
# Y_hatçš„æ¢¯åº¦å‡½æ•°å³æ˜¯æˆ‘ä»¬è‡ªå·±æ‰€å®šä¹‰çš„ MyReLU.backward

print(Y_hat.grad_fn)
```

```
<torch.autograd.function.MyReLUBackward object at 0x1205a46c8>
```

```python

```

### ä¸‰ï¼Œè®¡ç®—å›¾ä¸åå‘ä¼ æ’­


äº†è§£äº†Functionçš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç†è§£ä¸€ä¸‹åå‘ä¼ æ’­çš„åŸç†å’Œè¿‡ç¨‹ã€‚ç†è§£è¯¥éƒ¨åˆ†åŸç†éœ€è¦ä¸€äº›é«˜ç­‰æ•°å­¦ä¸­æ±‚å¯¼é“¾å¼æ³•åˆ™çš„åŸºç¡€çŸ¥è¯†ã€‚


```python
import torch 

x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

loss.backward()

```

loss.backward()è¯­å¥è°ƒç”¨åï¼Œä¾æ¬¡å‘ç”Ÿä»¥ä¸‹è®¡ç®—è¿‡ç¨‹ã€‚

1ï¼Œlossè‡ªå·±çš„gradæ¢¯åº¦èµ‹å€¼ä¸º1ï¼Œå³å¯¹è‡ªèº«çš„æ¢¯åº¦ä¸º1ã€‚

2ï¼Œlossæ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„backwardæ–¹æ³•ï¼Œè®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡å³y1å’Œy2çš„æ¢¯åº¦ï¼Œå°†è¯¥å€¼èµ‹å€¼åˆ°y1.gradå’Œy2.gradã€‚

3ï¼Œy2å’Œy1æ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„backwardæ–¹æ³•, åˆ†åˆ«è®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡xçš„æ¢¯åº¦ï¼Œx.gradå°†å…¶æ”¶åˆ°çš„å¤šä¸ªæ¢¯åº¦å€¼ç´¯åŠ ã€‚

ï¼ˆæ³¨æ„ï¼Œ1,2,3æ­¥éª¤çš„æ±‚æ¢¯åº¦é¡ºåºå’Œå¯¹å¤šä¸ªæ¢¯åº¦å€¼çš„ç´¯åŠ è§„åˆ™æ°å¥½æ˜¯æ±‚å¯¼é“¾å¼æ³•åˆ™çš„ç¨‹åºè¡¨è¿°ï¼‰

æ­£å› ä¸ºæ±‚å¯¼é“¾å¼æ³•åˆ™è¡ç”Ÿçš„æ¢¯åº¦ç´¯åŠ è§„åˆ™ï¼Œå¼ é‡çš„gradæ¢¯åº¦ä¸ä¼šè‡ªåŠ¨æ¸…é›¶ï¼Œåœ¨éœ€è¦çš„æ—¶å€™éœ€è¦æ‰‹åŠ¨ç½®é›¶ã€‚


```python

```

### å››ï¼Œå¶å­èŠ‚ç‚¹å’Œéå¶å­èŠ‚ç‚¹


æ‰§è¡Œä¸‹é¢ä»£ç ï¼Œæˆ‘ä»¬ä¼šå‘ç° loss.gradå¹¶ä¸æ˜¯æˆ‘ä»¬æœŸæœ›çš„1,è€Œæ˜¯ Noneã€‚

ç±»ä¼¼åœ° y1.grad ä»¥åŠ y2.gradä¹Ÿæ˜¯ None.

è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè¿™æ˜¯ç”±äºå®ƒä»¬ä¸æ˜¯å¶å­èŠ‚ç‚¹å¼ é‡ã€‚

åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œåªæœ‰ is_leaf=True çš„å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦æ±‚å¯¼çš„å¼ é‡çš„å¯¼æ•°ç»“æœæ‰ä¼šè¢«æœ€åä¿ç•™ä¸‹æ¥ã€‚

é‚£ä¹ˆä»€ä¹ˆæ˜¯å¶å­èŠ‚ç‚¹å¼ é‡å‘¢ï¼Ÿå¶å­èŠ‚ç‚¹å¼ é‡éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ã€‚

1ï¼Œå¶å­èŠ‚ç‚¹å¼ é‡æ˜¯ç”±ç”¨æˆ·ç›´æ¥åˆ›å»ºçš„å¼ é‡ï¼Œè€Œéç”±æŸä¸ªFunctioné€šè¿‡è®¡ç®—å¾—åˆ°çš„å¼ é‡ã€‚

2ï¼Œå¶å­èŠ‚ç‚¹å¼ é‡çš„ requires_gradå±æ€§å¿…é¡»ä¸ºTrue.

Pytorchè®¾è®¡è¿™æ ·çš„è§„åˆ™ä¸»è¦æ˜¯ä¸ºäº†èŠ‚çº¦å†…å­˜æˆ–è€…æ˜¾å­˜ç©ºé—´ï¼Œå› ä¸ºå‡ ä¹æ‰€æœ‰çš„æ—¶å€™ï¼Œç”¨æˆ·åªä¼šå…³å¿ƒä»–è‡ªå·±ç›´æ¥åˆ›å»ºçš„å¼ é‡çš„æ¢¯åº¦ã€‚

æ‰€æœ‰ä¾èµ–äºå¶å­èŠ‚ç‚¹å¼ é‡çš„å¼ é‡, å…¶requires_grad å±æ€§å¿…å®šæ˜¯Trueçš„ï¼Œä½†å…¶æ¢¯åº¦å€¼åªåœ¨è®¡ç®—è¿‡ç¨‹ä¸­è¢«ç”¨åˆ°ï¼Œä¸ä¼šæœ€ç»ˆå­˜å‚¨åˆ°gradå±æ€§ä¸­ã€‚

å¦‚æœéœ€è¦ä¿ç•™ä¸­é—´è®¡ç®—ç»“æœçš„æ¢¯åº¦åˆ°gradå±æ€§ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ retain_gradæ–¹æ³•ã€‚
å¦‚æœä»…ä»…æ˜¯ä¸ºäº†è°ƒè¯•ä»£ç æŸ¥çœ‹æ¢¯åº¦å€¼ï¼Œå¯ä»¥åˆ©ç”¨register_hookæ‰“å°æ—¥å¿—ã€‚


```python
import torch 

x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

loss.backward()
print("loss.grad:", loss.grad)
print("y1.grad:", y1.grad)
print("y2.grad:", y2.grad)
print(x.grad)
```

```
loss.grad: None
y1.grad: None
y2.grad: None
tensor(4.)
```

```python
print(x.is_leaf)
print(y1.is_leaf)
print(y2.is_leaf)
print(loss.is_leaf)
```

```
True
False
False
False
```


åˆ©ç”¨retain_gradå¯ä»¥ä¿ç•™éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼ï¼Œåˆ©ç”¨register_hookå¯ä»¥æŸ¥çœ‹éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼ã€‚

```python
import torch 

#æ­£å‘ä¼ æ’­
x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

#éå¶å­èŠ‚ç‚¹æ¢¯åº¦æ˜¾ç¤ºæ§åˆ¶
y1.register_hook(lambda grad: print('y1 grad: ', grad))
y2.register_hook(lambda grad: print('y2 grad: ', grad))
loss.retain_grad()

#åå‘ä¼ æ’­
loss.backward()
print("loss.grad:", loss.grad)
print("x.grad:", x.grad)
```

```
y2 grad:  tensor(4.)
y1 grad:  tensor(-4.)
loss.grad: tensor(1.)
x.grad: tensor(4.)
```

```python

```

### äº”ï¼Œè®¡ç®—å›¾åœ¨TensorBoardä¸­çš„å¯è§†åŒ–


å¯ä»¥åˆ©ç”¨ torch.utils.tensorboard å°†è®¡ç®—å›¾å¯¼å‡ºåˆ° TensorBoardè¿›è¡Œå¯è§†åŒ–ã€‚

```python
from torch import nn 
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.w = nn.Parameter(torch.randn(2,1))
        self.b = nn.Parameter(torch.zeros(1,1))

    def forward(self, x):
        y = x@self.w + self.b
        return y

net = Net()

```

```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('./data/tensorboard')
writer.add_graph(net,input_to_model = torch.rand(10,2))
writer.close()

```

```python
%load_ext tensorboard
#%tensorboard --logdir ./data/tensorboard

```

```python
from tensorboard import notebook
notebook.list() 

```

```python
#åœ¨tensorboardä¸­æŸ¥çœ‹æ¨¡å‹
notebook.start("--logdir ./data/tensorboard")
```

![](./data/2-3-è®¡ç®—å›¾å¯è§†åŒ–.png)

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)
