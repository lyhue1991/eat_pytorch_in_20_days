<!-- #region -->
# 3-3,é«˜é˜¶APIç¤ºèŒƒ

Pytorchæ²¡æœ‰å®˜æ–¹çš„é«˜é˜¶APIï¼Œä¸€èˆ¬éœ€è¦ç”¨æˆ·è‡ªå·±å®ç°è®­ç»ƒå¾ªç¯ã€éªŒè¯å¾ªç¯ã€å’Œé¢„æµ‹å¾ªç¯ã€‚

ä½œè€…é€šè¿‡ä»¿ç…§kerasçš„åŠŸèƒ½å¯¹Pytorchçš„nn.Moduleè¿›è¡Œäº†å°è£…ï¼Œè®¾è®¡äº†torchkeras.KerasModelç±»ï¼Œ

å®ç°äº† fit, evaluateï¼Œpredictç­‰æ–¹æ³•ï¼Œç›¸å½“äºç”¨æˆ·è‡ªå®šä¹‰é«˜é˜¶APIã€‚

å¹¶ç¤ºèŒƒäº†ç”¨å®ƒå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚

æ­¤å¤–ï¼Œä½œè€…è¿˜é€šè¿‡å€Ÿç”¨pytorch_lightningçš„åŠŸèƒ½ï¼Œå°è£…äº†ç±»Kerasæ¥å£çš„å¦å¤–ä¸€ç§å®ç°ï¼Œå³torchkeras.LightModelç±»ã€‚

å¹¶ç¤ºèŒƒäº†ç”¨å®ƒå®ç°DNNäºŒåˆ†ç±»æ¨¡å‹ã€‚


torchkeras.KerasModelç±»å’Œtorchkeras.LightModelç±»çœ‹èµ·æ¥éå¸¸å¼ºå¤§ï¼Œä½†å®é™…ä¸Šå®ƒä»¬çš„æºç éå¸¸ç®€å•ï¼Œä¸è¶³200è¡Œã€‚
æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­`ä¸€ã€Pytorchçš„å»ºæ¨¡æµç¨‹`ç”¨åˆ°çš„è®­ç»ƒä»£ç å…¶å®å°±æ˜¯torchkerasåº“çš„æ ¸å¿ƒæºç ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç”±äºæœ‰äº›æ¨¡å‹çš„è¾“å…¥è¾“å‡ºä»¥åŠLossç»“æ„å’Œtorchkerasçš„å‡è®¾ç»“æ„æœ‰æ‰€ä¸åŒï¼Œç›´æ¥è°ƒç”¨torchkeraså¯èƒ½ä¸èƒ½æ»¡è¶³éœ€æ±‚ï¼Œä¸è¦å®³æ€•ï¼Œcopyå‡ºæ¥
torchkeras.KerasModelæˆ–è€…torchkeras.LightModelçš„æºç ï¼Œåœ¨è¾“å…¥è¾“å‡ºå’ŒLossä¸Šç®€å•æ”¹åŠ¨ä¸€ä¸‹å°±å¯ä»¥ã€‚

<!-- #endregion -->

```python

```

```python
import os
import datetime

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 

```

```python
!pip install torchkeras==3.2.2 -i https://pypi.python.org/simple
```

```python
import torch 
import torchkeras 


print("torch.__version__="+torch.__version__) 
print("torchkeras.__version__="+torchkeras.__version__) 
```

```
torch.__version__=1.10.0
torchkeras.__version__=3.2.2
```

```python

```

### ä¸€ï¼Œçº¿æ€§å›å½’æ¨¡å‹


æ­¤èŒƒä¾‹æˆ‘ä»¬é€šè¿‡ç»§æ‰¿torchkeras.Modelæ¨¡å‹æ¥å£ï¼Œå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚


**1ï¼Œå‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd
from matplotlib import pyplot as plt 
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset

#æ ·æœ¬æ•°é‡
n = 400

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = 10*torch.rand([n,2])-5.0  #torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0],[-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0 + b0 + torch.normal( 0.0,2.0,size = [n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

```

```python
# æ•°æ®å¯è§†åŒ–

%matplotlib inline
%config InlineBackend.figure_format = 'svg'

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0],Y[:,0], c = "b",label = "samples")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)

ax2 = plt.subplot(122)
ax2.scatter(X[:,1],Y[:,0], c = "g",label = "samples")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)
plt.show()

```

![](./data/3-3-å›å½’æ•°æ®å¯è§†åŒ–.png)

```python
#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“
ds = TensorDataset(X,Y)
ds_train,ds_val = torch.utils.data.random_split(ds,[int(400*0.7),400-int(400*0.7)])
dl_train = DataLoader(ds_train,batch_size = 10,shuffle=True,num_workers=2)
dl_val = DataLoader(ds_val,batch_size = 10,num_workers=2)

features,labels = next(iter(dl_train))

```

```python

```

**2ï¼Œå®šä¹‰æ¨¡å‹**

```python
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.fc = nn.Linear(2,1)
    
    def forward(self,x):
        return self.fc(x)

net = LinearRegression()

```

```python

from torchkeras import summary 

summary(net,input_data=features);
```

```
--------------------------------------------------------------------------
Layer (type)                            Output Shape              Param #
==========================================================================
Linear-1                                     [-1, 1]                    3
==========================================================================
Total params: 3
Trainable params: 3
Non-trainable params: 0
--------------------------------------------------------------------------
Input size (MB): 0.000069
Forward/backward pass size (MB): 0.000008
Params size (MB): 0.000011
Estimated Total Size (MB): 0.000088
--------------------------------------------------------------------------
```

```python

```

**3ï¼Œè®­ç»ƒæ¨¡å‹**

```python
from torchkeras import KerasModel 

import torchmetrics

net = LinearRegression()
model = KerasModel(net=net,
                   loss_fn = nn.MSELoss(),
                   metrics_dict = {"mae":torchmetrics.MeanAbsoluteError()},
                   optimizer= torch.optim.Adam(net.parameters(),lr = 0.05))

dfhistory = model.fit(train_data=dl_train,
      val_data=dl_val,
      epochs=20,
      ckpt_path='checkpoint.pt',
      patience=5,
      monitor='val_loss',
      mode='min')

```

```python
# ç»“æœå¯è§†åŒ–

%matplotlib inline
%config InlineBackend.figure_format = 'svg'

w,b = net.state_dict()["fc.weight"],net.state_dict()["fc.bias"]

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0],Y[:,0], c = "b",label = "samples")
ax1.plot(X[:,0],w[0,0]*X[:,0]+b[0],"-r",linewidth = 5.0,label = "model")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)


ax2 = plt.subplot(122)
ax2.scatter(X[:,1],Y[:,0], c = "g",label = "samples")
ax2.plot(X[:,1],w[0,1]*X[:,1]+b[0],"-r",linewidth = 5.0,label = "model")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)

plt.show()

```

**4ï¼Œè¯„ä¼°æ¨¡å‹**

```python
dfhistory.tail()
```

```
	train_loss	train_mae	val_loss	val_mae	epoch
15	4.339620	1.635648	3.119237	1.384351	16
16	4.313104	1.631849	2.999482	1.352427	17
17	4.319547	1.628811	3.022779	1.355054	18
18	4.315403	1.636815	3.087339	1.369488	19
19	4.266822	1.627701	2.937751	1.330670	20
```

```python

```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory["train_"+metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
    
```

```python
plot_metric(dfhistory,"loss")
```

![](./data/3-3-lossæ›²çº¿.png)

```python
plot_metric(dfhistory,"mae")
```

![](./data/3-3-mapeæ›²çº¿.png)

```python
# è¯„ä¼°
model.evaluate(dl_val)
```

```
{'val_loss': 2.9377514322598777, 'val_mae': 1.3306695222854614}
```

```python

```

**5ï¼Œä½¿ç”¨æ¨¡å‹**

```python
# é¢„æµ‹
dl = DataLoader(TensorDataset(X))
model.predict(dl)[0:10]
```

```
tensor([[ 3.9212],
        [ 8.6336],
        [ 6.1982],
        [ 6.1212],
        [-5.0974],
        [-6.3183],
        [ 4.6588],
        [ 5.5349],
        [11.9106],
        [24.6937]])
```

```python
# é¢„æµ‹
model.predict(dl_val)[0:10]
```

```
tensor([[-11.0324],
        [ 26.2708],
        [ 24.8866],
        [ 12.2698],
        [-12.0984],
        [  6.7254],
        [ 12.8081],
        [ 20.6977],
        [  5.4715],
        [  2.0188]])
```

```python

```

### äºŒï¼ŒDNNäºŒåˆ†ç±»æ¨¡å‹


æ­¤èŒƒä¾‹æˆ‘ä»¬é€šè¿‡ç»§æ‰¿torchkeras.LightModelæ¨¡å‹æ¥å£ï¼Œå®ç°DNNäºŒåˆ†ç±»æ¨¡å‹ã€‚



**1ï¼Œå‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset
import torchkeras 
import pytorch_lightning as pl 
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æ­£è´Ÿæ ·æœ¬æ•°é‡
n_positive,n_negative = 2000,2000

#ç”Ÿæˆæ­£æ ·æœ¬, å°åœ†ç¯åˆ†å¸ƒ
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) 
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)
Yp = torch.ones_like(r_p)

#ç”Ÿæˆè´Ÿæ ·æœ¬, å¤§åœ†ç¯åˆ†å¸ƒ
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) 
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#æ±‡æ€»æ ·æœ¬
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)


#å¯è§†åŒ–
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0],Xp[:,1],c = "r")
plt.scatter(Xn[:,0],Xn[:,1],c = "g")
plt.legend(["positive","negative"]);

```

![](./data/3-3-åˆ†ç±»æ•°æ®å¯è§†åŒ–.png)

```python
ds = TensorDataset(X,Y)

ds_train,ds_val = torch.utils.data.random_split(ds,[int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])
dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True,num_workers=2)
dl_val = DataLoader(ds_val,batch_size = 100,num_workers=2)

```

```python

```

**2ï¼Œå®šä¹‰æ¨¡å‹**

```python
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2,4)
        self.fc2 = nn.Linear(4,8) 
        self.fc3 = nn.Linear(8,1)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        y = self.fc3(x)
        return y
    
```

```python
import torchkeras 
from torchkeras.metrics import Accuracy 

net = Net()
loss_fn = nn.BCEWithLogitsLoss()
metric_dict = {"acc":Accuracy()}

optimizer = torch.optim.Adam(net.parameters(), lr=0.05)
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.0001)

model = torchkeras.LightModel(net,
                   loss_fn = loss_fn,
                   metrics_dict= metric_dict,
                   optimizer = optimizer,
                   lr_scheduler = lr_scheduler,
                  )       

from torchkeras import summary
summary(model,input_data=features);

```

```python

```

**3ï¼Œè®­ç»ƒæ¨¡å‹**

```python
import pytorch_lightning as pl     

#1ï¼Œè®¾ç½®å›è°ƒå‡½æ•°
model_ckpt = pl.callbacks.ModelCheckpoint(
    monitor='val_acc',
    save_top_k=1,
    mode='max'
)

early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_acc',
                           patience=3,
                           mode = 'max'
                          )

#2ï¼Œè®¾ç½®è®­ç»ƒå‚æ•°

# gpus=0 åˆ™ä½¿ç”¨cpuè®­ç»ƒï¼Œgpus=1åˆ™ä½¿ç”¨1ä¸ªgpuè®­ç»ƒï¼Œgpus=2åˆ™ä½¿ç”¨2ä¸ªgpuè®­ç»ƒï¼Œgpus=-1åˆ™ä½¿ç”¨æ‰€æœ‰gpuè®­ç»ƒï¼Œ
# gpus=[0,1]åˆ™æŒ‡å®šä½¿ç”¨0å·å’Œ1å·gpuè®­ç»ƒï¼Œ gpus="0,1,2,3"åˆ™ä½¿ç”¨0,1,2,3å·gpuè®­ç»ƒ
# tpus=1 åˆ™ä½¿ç”¨1ä¸ªtpuè®­ç»ƒ
trainer = pl.Trainer(logger=True,
                     min_epochs=3,max_epochs=20,
                     gpus=0,
                     callbacks = [model_ckpt,early_stopping],
                     enable_progress_bar = True) 


##4ï¼Œå¯åŠ¨è®­ç»ƒå¾ªç¯
trainer.fit(model,dl_train,dl_val)


```

```
================================================================================2022-07-16 20:25:49
{'epoch': 0, 'val_loss': 0.3484574854373932, 'val_acc': 0.8766666650772095}
{'epoch': 0, 'train_loss': 0.5639581680297852, 'train_acc': 0.708214282989502}
<<<<<< reach best val_acc : 0.8766666650772095 >>>>>>

================================================================================2022-07-16 20:25:54
{'epoch': 1, 'val_loss': 0.18654096126556396, 'val_acc': 0.925000011920929}
{'epoch': 1, 'train_loss': 0.2512527406215668, 'train_acc': 0.9117857217788696}
<<<<<< reach best val_acc : 0.925000011920929 >>>>>>

================================================================================2022-07-16 20:25:59
{'epoch': 2, 'val_loss': 0.19609291851520538, 'val_acc': 0.9191666841506958}
{'epoch': 2, 'train_loss': 0.19115397334098816, 'train_acc': 0.9257143139839172}

================================================================================2022-07-16 20:26:04
{'epoch': 3, 'val_loss': 0.18749761581420898, 'val_acc': 0.925000011920929}
{'epoch': 3, 'train_loss': 0.19545568525791168, 'train_acc': 0.9235714077949524}

================================================================================2022-07-16 20:26:09
{'epoch': 4, 'val_loss': 0.21518440544605255, 'val_acc': 0.9083333611488342}
{'epoch': 4, 'train_loss': 0.1998639553785324, 'train_acc': 0.9192857146263123}
```


```python
# ç»“æœå¯è§†åŒ–
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(net.forward(X)>=0.5)]
Xn_pred = X[torch.squeeze(net.forward(X)<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");

```

![](./data/3-3-åˆ†ç±»ç»“æœå¯è§†åŒ–.png)


**4ï¼Œè¯„ä¼°æ¨¡å‹**

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory["train_"+metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
dfhistory  = model.get_history() 
plot_metric(dfhistory,"loss")

```

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h491k7wtl0j20f70a6wes.jpg)

```python

```

```python
plot_metric(dfhistory,"acc")
```

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h491k8if3hj20ev0aaglw.jpg)

```python
#ä½¿ç”¨æœ€ä½³ä¿å­˜ç‚¹è¿›è¡Œè¯„ä¼°
trainer.test(ckpt_path='best', dataloaders=dl_val,verbose = False)
```

```
{'test_loss': 0.18654096126556396, 'test_acc': 0.925000011920929}
```

```python

```

**5ï¼Œä½¿ç”¨æ¨¡å‹**

```python
predictions = F.sigmoid(torch.cat(trainer.predict(model, dl_val, ckpt_path='best'))) 
predictions 
```

```
tensor([[0.3873],
        [0.0028],
        [0.8772],
        ...,
        [0.9886],
        [0.4970],
        [0.8094]])
```

```python
def predict(model,dl):
    model.eval()
    result = torch.cat([model.forward(t[0]) for t in dl])
    return(result.data)

print(model.device)
predictions = F.sigmoid(predict(model,dl_val)[:10]) 
```

```python

```

**6ï¼Œä¿å­˜æ¨¡å‹**

```python
print(trainer.checkpoint_callback.best_model_path)
print(trainer.checkpoint_callback.best_model_score)
```

```python
model_loaded = torchkeras.LightModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)
```

```python

```

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)
