{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd775b13",
   "metadata": {},
   "source": [
    "# 5-1, Datasetå’ŒDataLoader\n",
    "\n",
    "Pytorché€šå¸¸ä½¿ç”¨Datasetå’ŒDataLoaderè¿™ä¸¤ä¸ªå·¥å…·ç±»æ¥æ„å»ºæ•°æ®ç®¡é“ã€‚\n",
    "\n",
    "Datasetå®šä¹‰äº†æ•°æ®é›†çš„å†…å®¹ï¼Œå®ƒç›¸å½“äºä¸€ä¸ªç±»ä¼¼åˆ—è¡¨çš„æ•°æ®ç»“æ„ï¼Œå…·æœ‰ç¡®å®šçš„é•¿åº¦ï¼Œèƒ½å¤Ÿç”¨ç´¢å¼•è·å–æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚\n",
    "\n",
    "è€ŒDataLoaderå®šä¹‰äº†æŒ‰batchåŠ è½½æ•°æ®é›†çš„æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ä¸ªå®ç°äº†`__iter__`æ–¹æ³•çš„å¯è¿­ä»£å¯¹è±¡ï¼Œæ¯æ¬¡è¿­ä»£è¾“å‡ºä¸€ä¸ªbatchçš„æ•°æ®ã€‚\n",
    "\n",
    "DataLoaderèƒ½å¤Ÿæ§åˆ¶batchçš„å¤§å°ï¼Œbatchä¸­å…ƒç´ çš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥åŠå°†batchç»“æœæ•´ç†æˆæ¨¡å‹æ‰€éœ€è¾“å…¥å½¢å¼çš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ã€‚\n",
    "\n",
    "åœ¨ç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç”¨æˆ·åªéœ€å®ç°Datasetçš„`__len__`æ–¹æ³•å’Œ`__getitem__`æ–¹æ³•ï¼Œå°±å¯ä»¥è½»æ¾æ„å»ºè‡ªå·±çš„æ•°æ®é›†ï¼Œå¹¶ç”¨é»˜è®¤æ•°æ®ç®¡é“è¿›è¡ŒåŠ è½½ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1565cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "\n",
    "print(\"torch.__version__=\"+torch.__version__) \n",
    "print(\"torchvision.__version__=\"+torchvision.__version__) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45557a5b",
   "metadata": {},
   "source": [
    "```\n",
    "torch.__version__=1.10.0\n",
    "torchvision.__version__=0.11.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7a37f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<font color=\"red\">\n",
    " \n",
    "å…¬ä¼—å· **ç®—æ³•ç¾é£Ÿå±‹** å›å¤å…³é”®è¯ï¼š**pytorch**ï¼Œ è·å–æœ¬é¡¹ç›®æºç å’Œæ‰€ç”¨æ•°æ®é›†ç™¾åº¦äº‘ç›˜ä¸‹è½½é“¾æ¥ã€‚\n",
    "    \n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875fb8c",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒDatasetå’ŒDataLoaderæ¦‚è¿°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f8516",
   "metadata": {},
   "source": [
    "**1ï¼Œè·å–ä¸€ä¸ªbatchæ•°æ®çš„æ­¥éª¤**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906005c3",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸‹ä»ä¸€ä¸ªæ•°æ®é›†ä¸­è·å–ä¸€ä¸ªbatchçš„æ•°æ®éœ€è¦å“ªäº›æ­¥éª¤ã€‚\n",
    "\n",
    "(å‡å®šæ•°æ®é›†çš„ç‰¹å¾å’Œæ ‡ç­¾åˆ†åˆ«è¡¨ç¤ºä¸ºå¼ é‡`X`å’Œ`Y`ï¼Œæ•°æ®é›†å¯ä»¥è¡¨ç¤ºä¸º`(X,Y)`, å‡å®šbatchå¤§å°ä¸º`m`)\n",
    "\n",
    "1ï¼Œé¦–å…ˆæˆ‘ä»¬è¦ç¡®å®šæ•°æ®é›†çš„é•¿åº¦`n`ã€‚\n",
    "\n",
    "ç»“æœç±»ä¼¼ï¼š`n = 1000`ã€‚\n",
    "\n",
    "2ï¼Œç„¶åæˆ‘ä»¬ä»`0`åˆ°`n-1`çš„èŒƒå›´ä¸­æŠ½æ ·å‡º`m`ä¸ªæ•°(batchå¤§å°)ã€‚\n",
    "\n",
    "å‡å®š`m=4`, æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œç±»ä¼¼ï¼š`indices = [1,4,8,9]`\n",
    "\n",
    "3ï¼Œæ¥ç€æˆ‘ä»¬ä»æ•°æ®é›†ä¸­å»å–è¿™`m`ä¸ªæ•°å¯¹åº”ä¸‹æ ‡çš„å…ƒç´ ã€‚\n",
    "\n",
    "æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œç±»ä¼¼ï¼š`samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]`\n",
    "\n",
    "4ï¼Œæœ€åæˆ‘ä»¬å°†ç»“æœæ•´ç†æˆä¸¤ä¸ªå¼ é‡ä½œä¸ºè¾“å‡ºã€‚\n",
    "\n",
    "æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸¤ä¸ªå¼ é‡ï¼Œç±»ä¼¼`batch = (features,labels) `ï¼Œ \n",
    "\n",
    "å…¶ä¸­ `features = torch.stack([X[1],X[4],X[8],X[9]])`\n",
    "\n",
    "`labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2de58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c26c2d91",
   "metadata": {},
   "source": [
    "**2ï¼ŒDatasetå’ŒDataLoaderçš„åŠŸèƒ½åˆ†å·¥**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99820480",
   "metadata": {},
   "source": [
    "ä¸Šè¿°ç¬¬1ä¸ªæ­¥éª¤ç¡®å®šæ•°æ®é›†çš„é•¿åº¦æ˜¯ç”± Datasetçš„`__len__` æ–¹æ³•å®ç°çš„ã€‚\n",
    "\n",
    "ç¬¬2ä¸ªæ­¥éª¤ä»`0`åˆ°`n-1`çš„èŒƒå›´ä¸­æŠ½æ ·å‡º`m`ä¸ªæ•°çš„æ–¹æ³•æ˜¯ç”± DataLoaderçš„ `sampler`å’Œ `batch_sampler`å‚æ•°æŒ‡å®šçš„ã€‚\n",
    "\n",
    "`sampler`å‚æ•°æŒ‡å®šå•ä¸ªå…ƒç´ æŠ½æ ·æ–¹æ³•ï¼Œä¸€èˆ¬æ— éœ€ç”¨æˆ·è®¾ç½®ï¼Œç¨‹åºé»˜è®¤åœ¨DataLoaderçš„å‚æ•°`shuffle=True`æ—¶é‡‡ç”¨éšæœºæŠ½æ ·ï¼Œ`shuffle=False`æ—¶é‡‡ç”¨é¡ºåºæŠ½æ ·ã€‚\n",
    "\n",
    "`batch_sampler`å‚æ•°å°†å¤šä¸ªæŠ½æ ·çš„å…ƒç´ æ•´ç†æˆä¸€ä¸ªåˆ—è¡¨ï¼Œä¸€èˆ¬æ— éœ€ç”¨æˆ·è®¾ç½®ï¼Œé»˜è®¤æ–¹æ³•åœ¨DataLoaderçš„å‚æ•°`drop_last=True`æ—¶ä¼šä¸¢å¼ƒæ•°æ®é›†æœ€åä¸€ä¸ªé•¿åº¦ä¸èƒ½è¢«batchå¤§å°æ•´é™¤çš„æ‰¹æ¬¡ï¼Œåœ¨`drop_last=False`æ—¶ä¿ç•™æœ€åä¸€ä¸ªæ‰¹æ¬¡ã€‚\n",
    "\n",
    "ç¬¬3ä¸ªæ­¥éª¤çš„æ ¸å¿ƒé€»è¾‘æ ¹æ®ä¸‹æ ‡å–æ•°æ®é›†ä¸­çš„å…ƒç´  æ˜¯ç”± Datasetçš„ `__getitem__`æ–¹æ³•å®ç°çš„ã€‚\n",
    "\n",
    "ç¬¬4ä¸ªæ­¥éª¤çš„é€»è¾‘ç”±DataLoaderçš„å‚æ•°`collate_fn`æŒ‡å®šã€‚ä¸€èˆ¬æƒ…å†µä¸‹ä¹Ÿæ— éœ€ç”¨æˆ·è®¾ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7951b4",
   "metadata": {},
   "source": [
    "**3ï¼ŒDatasetå’ŒDataLoaderçš„ä¸»è¦æ¥å£**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10273a5f",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹æ˜¯ Datasetå’Œ DataLoaderçš„æ ¸å¿ƒæ¥å£é€»è¾‘ä¼ªä»£ç ï¼Œä¸å®Œå…¨å’Œæºç ä¸€è‡´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self,dataset,batch_size,collate_fn,shuffle = True,drop_last = False):\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "        self.sampler =torch.utils.data.RandomSampler if shuffle else \\\n",
    "           torch.utils.data.SequentialSampler\n",
    "        self.batch_sampler = torch.utils.data.BatchSampler\n",
    "        self.sample_iter = self.batch_sampler(\n",
    "            self.sampler(range(len(dataset))),\n",
    "            batch_size = batch_size,drop_last = drop_last)\n",
    "        \n",
    "    def __next__(self):\n",
    "        indices = next(self.sample_iter)\n",
    "        batch = self.collate_fn([self.dataset[i] for i in indices])\n",
    "        return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006dbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a63a046b",
   "metadata": {},
   "source": [
    "### äºŒï¼Œä½¿ç”¨Datasetåˆ›å»ºæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c4e54",
   "metadata": {},
   "source": [
    "Datasetåˆ›å»ºæ•°æ®é›†å¸¸ç”¨çš„æ–¹æ³•æœ‰ï¼š\n",
    "\n",
    "* ä½¿ç”¨ torch.utils.data.TensorDataset æ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†(numpyçš„arrayï¼ŒPandasçš„DataFrameéœ€è¦å…ˆè½¬æ¢æˆTensor)ã€‚\n",
    "\n",
    "* ä½¿ç”¨ torchvision.datasets.ImageFolder æ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºå›¾ç‰‡æ•°æ®é›†ã€‚\n",
    "\n",
    "* ç»§æ‰¿ torch.utils.data.Dataset åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ã€‚\n",
    "\n",
    "\n",
    "æ­¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡\n",
    "\n",
    "* torch.utils.data.random_split å°†ä¸€ä¸ªæ•°æ®é›†åˆ†å‰²æˆå¤šä»½ï¼Œå¸¸ç”¨äºåˆ†å‰²è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚\n",
    "\n",
    "* è°ƒç”¨Datasetçš„åŠ æ³•è¿ç®—ç¬¦(`+`)å°†å¤šä¸ªæ•°æ®é›†åˆå¹¶æˆä¸€ä¸ªæ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478dac7",
   "metadata": {},
   "source": [
    "**1ï¼Œæ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73763881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import TensorDataset,Dataset,DataLoader,random_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4aca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†\n",
    "\n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))\n",
    "\n",
    "# åˆ†å‰²æˆè®­ç»ƒé›†å’Œé¢„æµ‹é›†\n",
    "n_train = int(len(ds_iris)*0.8)\n",
    "n_val = len(ds_iris) - n_train\n",
    "ds_train,ds_val = random_split(ds_iris,[n_train,n_val])\n",
    "\n",
    "print(type(ds_iris))\n",
    "print(type(ds_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†\n",
    "dl_train,dl_val = DataLoader(ds_train,batch_size = 8),DataLoader(ds_val,batch_size = 8)\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    print(features,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºåŠ æ³•è¿ç®—ç¬¦ï¼ˆ`+`ï¼‰çš„åˆå¹¶ä½œç”¨\n",
    "\n",
    "ds_data = ds_train + ds_val\n",
    "\n",
    "print('len(ds_train) = ',len(ds_train))\n",
    "print('len(ds_valid) = ',len(ds_val))\n",
    "print('len(ds_train+ds_valid) = ',len(ds_data))\n",
    "\n",
    "print(type(ds_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c92621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03438ccf",
   "metadata": {},
   "source": [
    "**2ï¼Œæ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºå›¾ç‰‡æ•°æ®é›†**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ¼”ç¤ºä¸€äº›å¸¸ç”¨çš„å›¾ç‰‡å¢å¼ºæ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('./data/cat.jpeg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bc7aa",
   "metadata": {},
   "source": [
    "![](./data/5-1-å‚»ä¹ä¹.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšæœºæ•°å€¼ç¿»è½¬\n",
    "transforms.RandomVerticalFlip()(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facff25",
   "metadata": {},
   "source": [
    "![](./data/5-1-ç¿»è½¬.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#éšæœºæ—‹è½¬\n",
    "transforms.RandomRotation(45)(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541a7b1",
   "metadata": {},
   "source": [
    "![](./data/5-1-æ—‹è½¬.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰å›¾ç‰‡å¢å¼ºæ“ä½œ\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "   transforms.RandomHorizontalFlip(), #éšæœºæ°´å¹³ç¿»è½¬\n",
    "   transforms.RandomVerticalFlip(), #éšæœºå‚ç›´ç¿»è½¬\n",
    "   transforms.RandomRotation(45),  #éšæœºåœ¨45åº¦è§’åº¦å†…æ—‹è½¬\n",
    "   transforms.ToTensor() #è½¬æ¢æˆå¼ é‡\n",
    "  ]\n",
    ") \n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºæ•°æ®é›†\n",
    "\n",
    "def transform_label(x):\n",
    "    return torch.tensor([x]).float()\n",
    "\n",
    "ds_train = datasets.ImageFolder(\"./eat_pytorch_datasets/cifar2/train/\",\n",
    "            transform = transform_train,target_transform= transform_label)\n",
    "ds_val = datasets.ImageFolder(\"./eat_pytorch_datasets/cifar2/test/\",\n",
    "\n",
    "\n",
    "print(ds_train.class_to_idx)\n",
    "\n",
    "# ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True)\n",
    "dl_val = DataLoader(ds_val,batch_size = 50,shuffle = True)\n",
    "\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd3a479",
   "metadata": {},
   "source": [
    "```\n",
    "{'0_airplane': 0, '1_automobile': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a767e7",
   "metadata": {},
   "source": [
    "```\n",
    "torch.Size([50, 3, 32, 32])\n",
    "torch.Size([50, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0e3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "562e9f0f",
   "metadata": {},
   "source": [
    "**3ï¼Œåˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d280d5c",
   "metadata": {},
   "source": [
    "ä¸‹é¢é€šè¿‡ç»§æ‰¿Datasetç±»åˆ›å»ºimdbæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚\n",
    "\n",
    "å¤§æ¦‚æ€è·¯å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œå¯¹è®­ç»ƒé›†æ–‡æœ¬åˆ†è¯æ„å»ºè¯å…¸ã€‚ç„¶åå°†è®­ç»ƒé›†æ–‡æœ¬å’Œæµ‹è¯•é›†æ–‡æœ¬æ•°æ®è½¬æ¢æˆtokenå•è¯ç¼–ç ã€‚\n",
    "\n",
    "æ¥ç€å°†è½¬æ¢æˆå•è¯ç¼–ç çš„è®­ç»ƒé›†æ•°æ®å’Œæµ‹è¯•é›†æ•°æ®æŒ‰æ ·æœ¬åˆ†å‰²æˆå¤šä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ã€‚\n",
    "\n",
    "æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®æ–‡ä»¶ååˆ—è¡¨è·å–å¯¹åº”åºå·çš„æ ·æœ¬å†…å®¹ï¼Œä»è€Œæ„å»ºDatasetæ•°æ®é›†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3475c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import OrderedDict\n",
    "import re,string\n",
    "\n",
    "MAX_WORDS = 10000  # ä»…è€ƒè™‘æœ€é«˜é¢‘çš„10000ä¸ªè¯\n",
    "MAX_LEN = 200  # æ¯ä¸ªæ ·æœ¬ä¿ç•™200ä¸ªè¯çš„é•¿åº¦\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "\n",
    "train_data_path = 'eat_pytorch_datasets/imdb/train.tsv'\n",
    "test_data_path = 'eat_pytorch_datasets/imdb/test.tsv'\n",
    "train_token_path = 'eat_pytorch_datasets/imdb/train_token.tsv'\n",
    "test_token_path =  'eat_pytorch_datasets/imdb/test_token.tsv'\n",
    "train_samples_path = 'eat_pytorch_datasets/imdb/train_samples/'\n",
    "test_samples_path =  'eat_pytorch_datasets/imdb/test_samples/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8e126",
   "metadata": {},
   "source": [
    "é¦–å…ˆæˆ‘ä»¬æ„å»ºè¯å…¸ï¼Œå¹¶ä¿ç•™æœ€é«˜é¢‘çš„MAX_WORDSä¸ªè¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "##æ„å»ºè¯å…¸\n",
    "\n",
    "word_count_dict = {}\n",
    "\n",
    "#æ¸…æ´—æ–‡æœ¬\n",
    "def clean_text(text):\n",
    "    lowercase = text.lower().replace(\"\\n\",\" \")\n",
    "    stripped_html = re.sub('<br />', ' ',lowercase)\n",
    "    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation),'',stripped_html)\n",
    "    return cleaned_punctuation\n",
    "\n",
    "with open(train_data_path,\"r\",encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        label,text = line.split(\"\\t\")\n",
    "        cleaned_text = clean_text(text)\n",
    "        for word in cleaned_text.split(\" \"):\n",
    "            word_count_dict[word] = word_count_dict.get(word,0)+1 \n",
    "\n",
    "df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = \"count\"))\n",
    "df_word_dict = df_word_dict.sort_values(by = \"count\",ascending =False)\n",
    "\n",
    "df_word_dict = df_word_dict[0:MAX_WORDS-2] #  \n",
    "df_word_dict[\"word_id\"] = range(2,MAX_WORDS) #ç¼–å·0å’Œ1åˆ†åˆ«ç•™ç»™æœªçŸ¥è¯<unkown>å’Œå¡«å……<padding>\n",
    "\n",
    "word_id_dict = df_word_dict[\"word_id\"].to_dict()\n",
    "\n",
    "df_word_dict.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf9482",
   "metadata": {},
   "source": [
    "![](./data/5-1-è¯å…¸.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56422a1d",
   "metadata": {},
   "source": [
    "ç„¶åæˆ‘ä»¬åˆ©ç”¨æ„å»ºå¥½çš„è¯å…¸ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆtokenåºå·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#è½¬æ¢token\n",
    "\n",
    "# å¡«å……æ–‡æœ¬\n",
    "def pad(data_list,pad_length):\n",
    "    padded_list = data_list.copy()\n",
    "    if len(data_list)> pad_length:\n",
    "         padded_list = data_list[-pad_length:]\n",
    "    if len(data_list)< pad_length:\n",
    "         padded_list = [1]*(pad_length-len(data_list))+data_list\n",
    "    return padded_list\n",
    "\n",
    "def text_to_token(text_file,token_file):\n",
    "    with open(text_file,\"r\",encoding = 'utf-8') as fin,\\\n",
    "      open(token_file,\"w\",encoding = 'utf-8') as fout:\n",
    "        for line in fin:\n",
    "            label,text = line.split(\"\\t\")\n",
    "            cleaned_text = clean_text(text)\n",
    "            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(\" \")]\n",
    "            pad_list = pad(word_token_list,MAX_LEN)\n",
    "            out_line = label+\"\\t\"+\" \".join([str(x) for x in pad_list])\n",
    "            fout.write(out_line+\"\\n\")\n",
    "        \n",
    "text_to_token(train_data_path,train_token_path)\n",
    "text_to_token(test_data_path,test_token_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7eb7f",
   "metadata": {},
   "source": [
    "æ¥ç€å°†tokenæ–‡æœ¬æŒ‰ç…§æ ·æœ¬åˆ†å‰²ï¼Œæ¯ä¸ªæ–‡ä»¶å­˜æ”¾ä¸€ä¸ªæ ·æœ¬çš„æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†å‰²æ ·æœ¬\n",
    "import os\n",
    "\n",
    "if not os.path.exists(train_samples_path):\n",
    "    os.mkdir(train_samples_path)\n",
    "    \n",
    "if not os.path.exists(test_samples_path):\n",
    "    os.mkdir(test_samples_path)\n",
    "    \n",
    "    \n",
    "def split_samples(token_path,samples_dir):\n",
    "    with open(token_path,\"r\",encoding = 'utf-8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            with open(samples_dir+\"%d.txt\"%i,\"w\",encoding = \"utf-8\") as fout:\n",
    "                fout.write(line)\n",
    "            i = i+1\n",
    "\n",
    "split_samples(train_token_path,train_samples_path)\n",
    "split_samples(test_token_path,test_samples_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(train_samples_path)[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b47ba8",
   "metadata": {},
   "source": [
    "```\n",
    "['11303.txt', '3644.txt', '19987.txt', '18441.txt', '5235.txt', '17772.txt', '1053.txt', '13514.txt', '8711.txt', '15165.txt', '7422.txt', '8077.txt', '15603.txt', '7344.txt', '1735.txt', '13272.txt', '9369.txt', '18327.txt', '5553.txt', '17014.txt', '4895.txt', '11465.txt', '3122.txt', '19039.txt', '5547.txt', '18333.txt', '17000.txt', '4881.txt', '2228.txt', '11471.txt', '3136.txt', '4659.txt', '15617.txt', '8063.txt', '7350.txt', '12178.txt', '1721.txt', '13266.txt', '14509.txt', '6728.txt', '1047.txt', '13500.txt', '15171.txt', '8705.txt', '7436.txt', '16478.txt', '11317.txt', '3650.txt', '19993.txt', '10009.txt', '5221.txt', '18455.txt', '17766.txt', '3888.txt', '6700.txt', '14247.txt', '9433.txt', '13528.txt', '12636.txt', '15159.txt', '16450.txt', '4117.txt', '19763.txt', '3678.txt', '17996.txt', '2566.txt', '10021.txt', '5209.txt', '17028.txt', '2200.txt', '10747.txt', '11459.txt', '16336.txt', '4671.txt', '19005.txt', '7378.txt', '12150.txt', '1709.txt', '6066.txt', '14521.txt', '9355.txt', '12144.txt', '289.txt', '6072.txt', '9341.txt', '14535.txt', '2214.txt', '10753.txt', '16322.txt', '19011.txt', '4665.txt', '16444.txt', '19777.txt', '4103.txt', '17982.txt', '2572.txt', '10035.txt', '18469.txt', '6714.txt', '9427.txt']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5490ca0",
   "metadata": {},
   "source": [
    "ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ•°æ®é›†Dataset, ä»æ–‡ä»¶åç§°åˆ—è¡¨ä¸­è¯»å–æ–‡ä»¶å†…å®¹äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0643c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class imdbDataset(Dataset):\n",
    "    def __init__(self,samples_dir):\n",
    "        self.samples_dir = samples_dir\n",
    "        self.samples_paths = os.listdir(samples_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples_paths)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        path = self.samples_dir + self.samples_paths[index]\n",
    "        with open(path,\"r\",encoding = \"utf-8\") as f:\n",
    "            line = f.readline()\n",
    "            label,tokens = line.split(\"\\t\")\n",
    "            label = torch.tensor([float(label)],dtype = torch.float)\n",
    "            feature = torch.tensor([int(x) for x in tokens.split(\" \")],dtype = torch.long)\n",
    "            return  (feature,label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdefe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = imdbDataset(train_samples_path)\n",
    "ds_test = imdbDataset(test_samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7199043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0211ed",
   "metadata": {},
   "source": [
    "```\n",
    "20000\n",
    "5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = True)\n",
    "dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE)\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    break\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950faf1",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([[   1,    1,    1,  ...,   29,    8,    8],\n",
    "        [  13,   11,  247,  ...,    0,    0,    8],\n",
    "        [8587,  555,   12,  ...,    3,    0,    8],\n",
    "        ...,\n",
    "        [   1,    1,    1,  ...,    2,    0,    8],\n",
    "        [ 618,   62,   25,  ...,   20,  204,    8],\n",
    "        [   1,    1,    1,  ...,   71,   85,    8]])\n",
    "tensor([[1.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cdf4e",
   "metadata": {},
   "source": [
    "æœ€åæ„å»ºæ¨¡å‹æµ‹è¯•ä¸€ä¸‹æ•°æ®é›†ç®¡é“æ˜¯å¦å¯ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #è®¾ç½®padding_idxå‚æ•°åå°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å¡«å……çš„tokenå§‹ç»ˆèµ‹å€¼ä¸º0å‘é‡\n",
    "        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = 3,padding_idx = 1)\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))\n",
    "        self.conv.add_module(\"pool_1\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_1\",nn.ReLU())\n",
    "        self.conv.add_module(\"conv_2\",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))\n",
    "        self.conv.add_module(\"pool_2\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_2\",nn.ReLU())\n",
    "        \n",
    "        self.dense = nn.Sequential()\n",
    "        self.dense.add_module(\"flatten\",nn.Flatten())\n",
    "        self.dense.add_module(\"linear\",nn.Linear(6144,1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "        \n",
    "net = Net()\n",
    "\n",
    "\n",
    "preds = net(features)\n",
    "print(net)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdac8d8",
   "metadata": {},
   "source": [
    "```\n",
    "Net(\n",
    "  (embedding): Embedding(10000, 3, padding_idx=1)\n",
    "  (conv): Sequential(\n",
    "    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
    "    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (relu_1): ReLU()\n",
    "    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
    "    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (relu_2): ReLU()\n",
    "  )\n",
    "  (dense): Sequential(\n",
    "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "    (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
    "  )\n",
    ")\n",
    "tensor([[-0.1576],\n",
    "        [-0.2841],\n",
    "        [-0.1405],\n",
    "        [-0.0690],\n",
    "        [-0.0520],\n",
    "        [-0.2680],\n",
    "        [-0.1689],\n",
    "        [-0.0814],\n",
    "        [-0.1718],\n",
    "        [-0.1264],\n",
    "        [-0.1674],\n",
    "        [-0.1763],\n",
    "        [-0.1537],\n",
    "        [-0.2502],\n",
    "        [-0.1983],\n",
    "        [-0.0943],\n",
    "        [-0.0087],\n",
    "        [-0.2154],\n",
    "        [-0.1263],\n",
    "        [-0.0274]], grad_fn=<AddmmBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11422d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras import KerasModel \n",
    "from torchkeras.metrics import Accuracy\n",
    "\n",
    "net = Net() \n",
    "model = KerasModel(net,\n",
    "                  loss_fn = nn.BCEWithLogitsLoss(),\n",
    "                  optimizer= torch.optim.Adam(net.parameters(),lr = 0.005),  \n",
    "                  metrics_dict = {\"acc\":Accuracy()}\n",
    "                )\n",
    "model.fit(dl_train,\n",
    "    val_data=dl_test,\n",
    "    epochs=10,\n",
    "    ckpt_path='checkpoint.pt',\n",
    "    patience=3,\n",
    "    monitor='val_acc',\n",
    "    mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756b181",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================2022-07-17 15:13:03\n",
    "Epoch 4 / 10\n",
    "\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:16<00:00, 59.53it/s, train_acc=0.907, train_loss=0.237]\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 107.14it/s, val_acc=0.87, val_loss=0.318] \n",
    "<<<<<< reach best val_acc : 0.8704000115394592 >>>>>>\n",
    "\n",
    "================================================================================2022-07-17 15:13:22\n",
    "Epoch 5 / 10\n",
    "\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:17<00:00, 58.38it/s, train_acc=0.928, train_loss=0.19]\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 101.77it/s, val_acc=0.87, val_loss=0.338] \n",
    "\n",
    "================================================================================2022-07-17 15:13:42\n",
    "Epoch 6 / 10\n",
    "\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:16<00:00, 59.51it/s, train_acc=0.942, train_loss=0.153]\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 104.84it/s, val_acc=0.856, val_loss=0.41] \n",
    "\n",
    "================================================================================2022-07-17 15:14:01\n",
    "Epoch 7 / 10\n",
    "\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:17<00:00, 58.78it/s, train_acc=0.954, train_loss=0.123]\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 104.06it/s, val_acc=0.863, val_loss=0.428]\n",
    "<<<<<< val_acc without improvement in 3 epoch, early stopping >>>>>>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c72ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21007f11",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0cdca",
   "metadata": {},
   "source": [
    "DataLoaderèƒ½å¤Ÿæ§åˆ¶batchçš„å¤§å°ï¼Œbatchä¸­å…ƒç´ çš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥åŠå°†batchç»“æœæ•´ç†æˆæ¨¡å‹æ‰€éœ€è¾“å…¥å½¢å¼çš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ã€‚\n",
    "\n",
    "DataLoaderçš„å‡½æ•°ç­¾åå¦‚ä¸‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d6c0b",
   "metadata": {},
   "source": [
    "```python\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    multiprocessing_context=None,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319544",
   "metadata": {},
   "source": [
    "ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»…ä»…ä¼šé…ç½® dataset, batch_size, shuffle, num_workers, drop_lastè¿™äº”ä¸ªå‚æ•°ï¼Œå…¶ä»–å‚æ•°ä½¿ç”¨é»˜è®¤å€¼å³å¯ã€‚\n",
    "\n",
    "DataLoaderé™¤äº†å¯ä»¥åŠ è½½æˆ‘ä»¬å‰é¢è®²çš„ torch.utils.data.Dataset å¤–ï¼Œè¿˜èƒ½å¤ŸåŠ è½½å¦å¤–ä¸€ç§æ•°æ®é›† torch.utils.data.IterableDatasetã€‚\n",
    "\n",
    "å’ŒDatasetæ•°æ®é›†ç›¸å½“äºä¸€ç§åˆ—è¡¨ç»“æ„ä¸åŒï¼ŒIterableDatasetç›¸å½“äºä¸€ç§è¿­ä»£å™¨ç»“æ„ã€‚ å®ƒæ›´åŠ å¤æ‚ï¼Œä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚\n",
    "\n",
    "- dataset : æ•°æ®é›†\n",
    "- batch_size: æ‰¹æ¬¡å¤§å°\n",
    "- shuffle: æ˜¯å¦ä¹±åº\n",
    "- sampler: æ ·æœ¬é‡‡æ ·å‡½æ•°ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚\n",
    "- batch_sampler: æ‰¹æ¬¡é‡‡æ ·å‡½æ•°ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚\n",
    "- num_workers: ä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ï¼Œè®¾ç½®çš„è¿›ç¨‹æ•°ã€‚\n",
    "- collate_fn: æ•´ç†ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®çš„å‡½æ•°ã€‚\n",
    "- pin_memory: æ˜¯å¦è®¾ç½®ä¸ºé”ä¸šå†…å­˜ã€‚é»˜è®¤ä¸ºFalseï¼Œé”ä¸šå†…å­˜ä¸ä¼šä½¿ç”¨è™šæ‹Ÿå†…å­˜(ç¡¬ç›˜)ï¼Œä»é”ä¸šå†…å­˜æ‹·è´åˆ°GPUä¸Šé€Ÿåº¦ä¼šæ›´å¿«ã€‚\n",
    "- drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªæ ·æœ¬æ•°é‡ä¸è¶³batch_sizeæ‰¹æ¬¡æ•°æ®ã€‚\n",
    "- timeout: åŠ è½½ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡çš„æœ€é•¿ç­‰å¾…æ—¶é—´ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚\n",
    "- worker_init_fn: æ¯ä¸ªworkerä¸­datasetçš„åˆå§‹åŒ–å‡½æ•°ï¼Œå¸¸ç”¨äº IterableDatasetã€‚ä¸€èˆ¬ä¸ä½¿ç”¨ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281047ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“\n",
    "ds = TensorDataset(torch.arange(1,50))\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = 10,\n",
    "                shuffle= True,\n",
    "                num_workers=2,\n",
    "                drop_last = True)\n",
    "#è¿­ä»£æ•°æ®\n",
    "for batch, in dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2856881",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([43, 44, 21, 36,  9,  5, 28, 16, 20, 14])\n",
    "tensor([23, 49, 35, 38,  2, 34, 45, 18, 15, 40])\n",
    "tensor([26,  6, 27, 39,  8,  4, 24, 19, 32, 17])\n",
    "tensor([ 1, 29, 11, 47, 12, 22, 48, 42, 10,  7])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308db239",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"ç®—æ³•ç¾é£Ÿå±‹\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚\n",
    "\n",
    "![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9206fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
