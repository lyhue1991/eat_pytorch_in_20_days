# 5-1, Datasetå’ŒDataLoader

Pytorché€šå¸¸ä½¿ç”¨Datasetå’ŒDataLoaderè¿™ä¸¤ä¸ªå·¥å…·ç±»æ¥æ„å»ºæ•°æ®ç®¡é“ã€‚

Datasetå®šä¹‰äº†æ•°æ®é›†çš„å†…å®¹ï¼Œå®ƒç›¸å½“äºä¸€ä¸ªç±»ä¼¼åˆ—è¡¨çš„æ•°æ®ç»“æ„ï¼Œå…·æœ‰ç¡®å®šçš„é•¿åº¦ï¼Œèƒ½å¤Ÿç”¨ç´¢å¼•è·å–æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

è€ŒDataLoaderå®šä¹‰äº†æŒ‰batchåŠ è½½æ•°æ®é›†çš„æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ä¸ªå®ç°äº†`__iter__`æ–¹æ³•çš„å¯è¿­ä»£å¯¹è±¡ï¼Œæ¯æ¬¡è¿­ä»£è¾“å‡ºä¸€ä¸ªbatchçš„æ•°æ®ã€‚

DataLoaderèƒ½å¤Ÿæ§åˆ¶batchçš„å¤§å°ï¼Œbatchä¸­å…ƒç´ çš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥åŠå°†batchç»“æœæ•´ç†æˆæ¨¡å‹æ‰€éœ€è¾“å…¥å½¢å¼çš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ã€‚

åœ¨ç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç”¨æˆ·åªéœ€å®ç°Datasetçš„`__len__`æ–¹æ³•å’Œ`__getitem__`æ–¹æ³•ï¼Œå°±å¯ä»¥è½»æ¾æ„å»ºè‡ªå·±çš„æ•°æ®é›†ï¼Œå¹¶ç”¨é»˜è®¤æ•°æ®ç®¡é“è¿›è¡ŒåŠ è½½ã€‚


```python

```

### ä¸€ï¼ŒDatasetå’ŒDataLoaderæ¦‚è¿°


**1ï¼Œè·å–ä¸€ä¸ªbatchæ•°æ®çš„æ­¥éª¤**


è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸‹ä»ä¸€ä¸ªæ•°æ®é›†ä¸­è·å–ä¸€ä¸ªbatchçš„æ•°æ®éœ€è¦å“ªäº›æ­¥éª¤ã€‚

(å‡å®šæ•°æ®é›†çš„ç‰¹å¾å’Œæ ‡ç­¾åˆ†åˆ«è¡¨ç¤ºä¸ºå¼ é‡`X`å’Œ`Y`ï¼Œæ•°æ®é›†å¯ä»¥è¡¨ç¤ºä¸º`(X,Y)`, å‡å®šbatchå¤§å°ä¸º`m`)

1ï¼Œé¦–å…ˆæˆ‘ä»¬è¦ç¡®å®šæ•°æ®é›†çš„é•¿åº¦`n`ã€‚

ç»“æœç±»ä¼¼ï¼š`n = 1000`ã€‚

2ï¼Œç„¶åæˆ‘ä»¬ä»`0`åˆ°`n-1`çš„èŒƒå›´ä¸­æŠ½æ ·å‡º`m`ä¸ªæ•°(batchå¤§å°)ã€‚

å‡å®š`m=4`, æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œç±»ä¼¼ï¼š`indices = [1,4,8,9]`

3ï¼Œæ¥ç€æˆ‘ä»¬ä»æ•°æ®é›†ä¸­å»å–è¿™`m`ä¸ªæ•°å¯¹åº”ä¸‹æ ‡çš„å…ƒç´ ã€‚

æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œç±»ä¼¼ï¼š`samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]`

4ï¼Œæœ€åæˆ‘ä»¬å°†ç»“æœæ•´ç†æˆä¸¤ä¸ªå¼ é‡ä½œä¸ºè¾“å‡ºã€‚

æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸¤ä¸ªå¼ é‡ï¼Œç±»ä¼¼`batch = (features,labels) `ï¼Œ 

å…¶ä¸­ `features = torch.stack([X[1],X[4],X[8],X[9]])`

`labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])`


```python

```

**2ï¼ŒDatasetå’ŒDataLoaderçš„åŠŸèƒ½åˆ†å·¥**


ä¸Šè¿°ç¬¬1ä¸ªæ­¥éª¤ç¡®å®šæ•°æ®é›†çš„é•¿åº¦æ˜¯ç”± Datasetçš„`__len__` æ–¹æ³•å®ç°çš„ã€‚

ç¬¬2ä¸ªæ­¥éª¤ä»`0`åˆ°`n-1`çš„èŒƒå›´ä¸­æŠ½æ ·å‡º`m`ä¸ªæ•°çš„æ–¹æ³•æ˜¯ç”± DataLoaderçš„ `sampler`å’Œ `batch_sampler`å‚æ•°æŒ‡å®šçš„ã€‚

`sampler`å‚æ•°æŒ‡å®šå•ä¸ªå…ƒç´ æŠ½æ ·æ–¹æ³•ï¼Œä¸€èˆ¬æ— éœ€ç”¨æˆ·è®¾ç½®ï¼Œç¨‹åºé»˜è®¤åœ¨DataLoaderçš„å‚æ•°`shuffle=True`æ—¶é‡‡ç”¨éšæœºæŠ½æ ·ï¼Œ`shuffle=False`æ—¶é‡‡ç”¨é¡ºåºæŠ½æ ·ã€‚

`batch_sampler`å‚æ•°å°†å¤šä¸ªæŠ½æ ·çš„å…ƒç´ æ•´ç†æˆä¸€ä¸ªåˆ—è¡¨ï¼Œä¸€èˆ¬æ— éœ€ç”¨æˆ·è®¾ç½®ï¼Œé»˜è®¤æ–¹æ³•åœ¨DataLoaderçš„å‚æ•°`drop_last=True`æ—¶ä¼šä¸¢å¼ƒæ•°æ®é›†æœ€åä¸€ä¸ªé•¿åº¦ä¸èƒ½è¢«batchå¤§å°æ•´é™¤çš„æ‰¹æ¬¡ï¼Œåœ¨`drop_last=False`æ—¶ä¿ç•™æœ€åä¸€ä¸ªæ‰¹æ¬¡ã€‚

ç¬¬3ä¸ªæ­¥éª¤çš„æ ¸å¿ƒé€»è¾‘æ ¹æ®ä¸‹æ ‡å–æ•°æ®é›†ä¸­çš„å…ƒç´  æ˜¯ç”± Datasetçš„ `__getitem__`æ–¹æ³•å®ç°çš„ã€‚

ç¬¬4ä¸ªæ­¥éª¤çš„é€»è¾‘ç”±DataLoaderçš„å‚æ•°`collate_fn`æŒ‡å®šã€‚ä¸€èˆ¬æƒ…å†µä¸‹ä¹Ÿæ— éœ€ç”¨æˆ·è®¾ç½®ã€‚

```python

```

**3ï¼ŒDatasetå’ŒDataLoaderçš„ä¸»è¦æ¥å£**


ä»¥ä¸‹æ˜¯ Datasetå’Œ DataLoaderçš„æ ¸å¿ƒæ¥å£é€»è¾‘ä¼ªä»£ç ï¼Œä¸å®Œå…¨å’Œæºç ä¸€è‡´ã€‚

```python
import torch 
class Dataset(object):
    def __init__(self):
        pass
    
    def __len__(self):
        raise NotImplementedError
        
    def __getitem__(self,index):
        raise NotImplementedError
        

class DataLoader(object):
    def __init__(self,dataset,batch_size,collate_fn,shuffle = True,drop_last = False):
        self.dataset = dataset
        self.sampler =torch.utils.data.RandomSampler if shuffle else \
           torch.utils.data.SequentialSampler
        self.batch_sampler = torch.utils.data.BatchSampler
        self.sample_iter = self.batch_sampler(
            self.sampler(range(len(dataset))),
            batch_size = batch_size,drop_last = drop_last)
        
    def __next__(self):
        indices = next(self.sample_iter)
        batch = self.collate_fn([self.dataset[i] for i in indices])
        return batch
    
```

```python

```

### äºŒï¼Œä½¿ç”¨Datasetåˆ›å»ºæ•°æ®é›†

<!-- #region -->
Datasetåˆ›å»ºæ•°æ®é›†å¸¸ç”¨çš„æ–¹æ³•æœ‰ï¼š

* ä½¿ç”¨ torch.utils.data.TensorDataset æ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†(numpyçš„arrayï¼ŒPandasçš„DataFrameéœ€è¦å…ˆè½¬æ¢æˆTensor)ã€‚

* ä½¿ç”¨ torchvision.datasets.ImageFolder æ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºå›¾ç‰‡æ•°æ®é›†ã€‚

* ç»§æ‰¿ torch.utils.data.Dataset åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ã€‚


æ­¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡

* torch.utils.data.random_split å°†ä¸€ä¸ªæ•°æ®é›†åˆ†å‰²æˆå¤šä»½ï¼Œå¸¸ç”¨äºåˆ†å‰²è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

* è°ƒç”¨Datasetçš„åŠ æ³•è¿ç®—ç¬¦(`+`)å°†å¤šä¸ªæ•°æ®é›†åˆå¹¶æˆä¸€ä¸ªæ•°æ®é›†ã€‚
<!-- #endregion -->

**1ï¼Œæ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†**

```python
import numpy as np 
import torch 
from torch.utils.data import TensorDataset,Dataset,DataLoader,random_split 

```

```python
# æ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†

from sklearn import datasets 
iris = datasets.load_iris()
ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))

# åˆ†å‰²æˆè®­ç»ƒé›†å’Œé¢„æµ‹é›†
n_train = int(len(ds_iris)*0.8)
n_valid = len(ds_iris) - n_train
ds_train,ds_valid = random_split(ds_iris,[n_train,n_valid])

print(type(ds_iris))
print(type(ds_train))

```

```python
# ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†
dl_train,dl_valid = DataLoader(ds_train,batch_size = 8),DataLoader(ds_valid,batch_size = 8)

for features,labels in dl_train:
    print(features,labels)
    break
```

```python
# æ¼”ç¤ºåŠ æ³•è¿ç®—ç¬¦ï¼ˆ`+`ï¼‰çš„åˆå¹¶ä½œç”¨

ds_data = ds_train + ds_valid

print('len(ds_train) = ',len(ds_train))
print('len(ds_valid) = ',len(ds_valid))
print('len(ds_train+ds_valid) = ',len(ds_data))

print(type(ds_data))

```

```python

```

**2ï¼Œæ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºå›¾ç‰‡æ•°æ®é›†**

```python
import numpy as np 
import torch 
from torch.utils.data import DataLoader
from torchvision import transforms,datasets 

```

```python
#æ¼”ç¤ºä¸€äº›å¸¸ç”¨çš„å›¾ç‰‡å¢å¼ºæ“ä½œ
```

```python
from PIL import Image
img = Image.open('./data/cat.jpeg')
img
```

![](./data/5-1-å‚»ä¹ä¹.png)

```python
# éšæœºæ•°å€¼ç¿»è½¬
transforms.RandomVerticalFlip()(img)
```

![](./data/5-1-ç¿»è½¬.png)

```python
#éšæœºæ—‹è½¬
transforms.RandomRotation(45)(img)
```

![](./data/5-1-æ—‹è½¬.png)

```python
# å®šä¹‰å›¾ç‰‡å¢å¼ºæ“ä½œ

transform_train = transforms.Compose([
   transforms.RandomHorizontalFlip(), #éšæœºæ°´å¹³ç¿»è½¬
   transforms.RandomVerticalFlip(), #éšæœºå‚ç›´ç¿»è½¬
   transforms.RandomRotation(45),  #éšæœºåœ¨45åº¦è§’åº¦å†…æ—‹è½¬
   transforms.ToTensor() #è½¬æ¢æˆå¼ é‡
  ]
) 

transform_valid = transforms.Compose([
    transforms.ToTensor()
  ]
)

```

```python
# æ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºæ•°æ®é›†
ds_train = datasets.ImageFolder("./data/cifar2/train/",
            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())
ds_valid = datasets.ImageFolder("./data/cifar2/test/",
            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())

print(ds_train.class_to_idx)

```

```
{'0_airplane': 0, '1_automobile': 1}
```

```python
# ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†

dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True,num_workers=3)
dl_valid = DataLoader(ds_valid,batch_size = 50,shuffle = True,num_workers=3)
```

```python
for features,labels in dl_train:
    print(features.shape)
    print(labels.shape)
    break
```

```
torch.Size([50, 3, 32, 32])
torch.Size([50, 1])
```

```python

```

**3ï¼Œåˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†**


ä¸‹é¢é€šè¿‡ç»§æ‰¿Datasetç±»åˆ›å»ºimdbæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚

å¤§æ¦‚æ€è·¯å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œå¯¹è®­ç»ƒé›†æ–‡æœ¬åˆ†è¯æ„å»ºè¯å…¸ã€‚ç„¶åå°†è®­ç»ƒé›†æ–‡æœ¬å’Œæµ‹è¯•é›†æ–‡æœ¬æ•°æ®è½¬æ¢æˆtokenå•è¯ç¼–ç ã€‚

æ¥ç€å°†è½¬æ¢æˆå•è¯ç¼–ç çš„è®­ç»ƒé›†æ•°æ®å’Œæµ‹è¯•é›†æ•°æ®æŒ‰æ ·æœ¬åˆ†å‰²æˆå¤šä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®æ–‡ä»¶ååˆ—è¡¨è·å–å¯¹åº”åºå·çš„æ ·æœ¬å†…å®¹ï¼Œä»è€Œæ„å»ºDatasetæ•°æ®é›†ã€‚


```python
import numpy as np 
import pandas as pd 
from collections import OrderedDict
import re,string

MAX_WORDS = 10000  # ä»…è€ƒè™‘æœ€é«˜é¢‘çš„10000ä¸ªè¯
MAX_LEN = 200  # æ¯ä¸ªæ ·æœ¬ä¿ç•™200ä¸ªè¯çš„é•¿åº¦
BATCH_SIZE = 20 

train_data_path = 'data/imdb/train.tsv'
test_data_path = 'data/imdb/test.tsv'
train_token_path = 'data/imdb/train_token.tsv'
test_token_path =  'data/imdb/test_token.tsv'
train_samples_path = 'data/imdb/train_samples/'
test_samples_path =  'data/imdb/test_samples/'
```

é¦–å…ˆæˆ‘ä»¬æ„å»ºè¯å…¸ï¼Œå¹¶ä¿ç•™æœ€é«˜é¢‘çš„MAX_WORDSä¸ªè¯ã€‚

```python
##æ„å»ºè¯å…¸

word_count_dict = {}

#æ¸…æ´—æ–‡æœ¬
def clean_text(text):
    lowercase = text.lower().replace("\n"," ")
    stripped_html = re.sub('<br />', ' ',lowercase)
    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation),'',stripped_html)
    return cleaned_punctuation

with open(train_data_path,"r",encoding = 'utf-8') as f:
    for line in f:
        label,text = line.split("\t")
        cleaned_text = clean_text(text)
        for word in cleaned_text.split(" "):
            word_count_dict[word] = word_count_dict.get(word,0)+1 

df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = "count"))
df_word_dict = df_word_dict.sort_values(by = "count",ascending =False)

df_word_dict = df_word_dict[0:MAX_WORDS-2] #  
df_word_dict["word_id"] = range(2,MAX_WORDS) #ç¼–å·0å’Œ1åˆ†åˆ«ç•™ç»™æœªçŸ¥è¯<unkown>å’Œå¡«å……<padding>

word_id_dict = df_word_dict["word_id"].to_dict()

df_word_dict.head(10)

```

![](./data/5-1-è¯å…¸.png)


ç„¶åæˆ‘ä»¬åˆ©ç”¨æ„å»ºå¥½çš„è¯å…¸ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆtokenåºå·ã€‚

```python
#è½¬æ¢token

# å¡«å……æ–‡æœ¬
def pad(data_list,pad_length):
    padded_list = data_list.copy()
    if len(data_list)> pad_length:
         padded_list = data_list[-pad_length:]
    if len(data_list)< pad_length:
         padded_list = [1]*(pad_length-len(data_list))+data_list
    return padded_list

def text_to_token(text_file,token_file):
    with open(text_file,"r",encoding = 'utf-8') as fin,\
      open(token_file,"w",encoding = 'utf-8') as fout:
        for line in fin:
            label,text = line.split("\t")
            cleaned_text = clean_text(text)
            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(" ")]
            pad_list = pad(word_token_list,MAX_LEN)
            out_line = label+"\t"+" ".join([str(x) for x in pad_list])
            fout.write(out_line+"\n")
        
text_to_token(train_data_path,train_token_path)
text_to_token(test_data_path,test_token_path)

```

æ¥ç€å°†tokenæ–‡æœ¬æŒ‰ç…§æ ·æœ¬åˆ†å‰²ï¼Œæ¯ä¸ªæ–‡ä»¶å­˜æ”¾ä¸€ä¸ªæ ·æœ¬çš„æ•°æ®ã€‚

```python
# åˆ†å‰²æ ·æœ¬
import os

if not os.path.exists(train_samples_path):
    os.mkdir(train_samples_path)
    
if not os.path.exists(test_samples_path):
    os.mkdir(test_samples_path)
    
    
def split_samples(token_path,samples_dir):
    with open(token_path,"r",encoding = 'utf-8') as fin:
        i = 0
        for line in fin:
            with open(samples_dir+"%d.txt"%i,"w",encoding = "utf-8") as fout:
                fout.write(line)
            i = i+1

split_samples(train_token_path,train_samples_path)
split_samples(test_token_path,test_samples_path)
```

```python
print(os.listdir(train_samples_path)[0:100])
```

```
['11303.txt', '3644.txt', '19987.txt', '18441.txt', '5235.txt', '17772.txt', '1053.txt', '13514.txt', '8711.txt', '15165.txt', '7422.txt', '8077.txt', '15603.txt', '7344.txt', '1735.txt', '13272.txt', '9369.txt', '18327.txt', '5553.txt', '17014.txt', '4895.txt', '11465.txt', '3122.txt', '19039.txt', '5547.txt', '18333.txt', '17000.txt', '4881.txt', '2228.txt', '11471.txt', '3136.txt', '4659.txt', '15617.txt', '8063.txt', '7350.txt', '12178.txt', '1721.txt', '13266.txt', '14509.txt', '6728.txt', '1047.txt', '13500.txt', '15171.txt', '8705.txt', '7436.txt', '16478.txt', '11317.txt', '3650.txt', '19993.txt', '10009.txt', '5221.txt', '18455.txt', '17766.txt', '3888.txt', '6700.txt', '14247.txt', '9433.txt', '13528.txt', '12636.txt', '15159.txt', '16450.txt', '4117.txt', '19763.txt', '3678.txt', '17996.txt', '2566.txt', '10021.txt', '5209.txt', '17028.txt', '2200.txt', '10747.txt', '11459.txt', '16336.txt', '4671.txt', '19005.txt', '7378.txt', '12150.txt', '1709.txt', '6066.txt', '14521.txt', '9355.txt', '12144.txt', '289.txt', '6072.txt', '9341.txt', '14535.txt', '2214.txt', '10753.txt', '16322.txt', '19011.txt', '4665.txt', '16444.txt', '19777.txt', '4103.txt', '17982.txt', '2572.txt', '10035.txt', '18469.txt', '6714.txt', '9427.txt']
```


ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ•°æ®é›†Dataset, ä»æ–‡ä»¶åç§°åˆ—è¡¨ä¸­è¯»å–æ–‡ä»¶å†…å®¹äº†ã€‚

```python
import os
class imdbDataset(Dataset):
    def __init__(self,samples_dir):
        self.samples_dir = samples_dir
        self.samples_paths = os.listdir(samples_dir)
    
    def __len__(self):
        return len(self.samples_paths)
    
    def __getitem__(self,index):
        path = self.samples_dir + self.samples_paths[index]
        with open(path,"r",encoding = "utf-8") as f:
            line = f.readline()
            label,tokens = line.split("\t")
            label = torch.tensor([float(label)],dtype = torch.float)
            feature = torch.tensor([int(x) for x in tokens.split(" ")],dtype = torch.long)
            return  (feature,label)
    
```

```python
ds_train = imdbDataset(train_samples_path)
ds_test = imdbDataset(test_samples_path)
```

```python
print(len(ds_train))
print(len(ds_test))
```

```
20000
5000
```

```python
dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = True,num_workers=4)
dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE,num_workers=4)

for features,labels in dl_train:
    print(features)
    print(labels)
    break
```

```
tensor([[   1,    1,    1,  ...,   29,    8,    8],
        [  13,   11,  247,  ...,    0,    0,    8],
        [8587,  555,   12,  ...,    3,    0,    8],
        ...,
        [   1,    1,    1,  ...,    2,    0,    8],
        [ 618,   62,   25,  ...,   20,  204,    8],
        [   1,    1,    1,  ...,   71,   85,    8]])
tensor([[1.],
        [0.],
        [0.],
        [1.],
        [0.],
        [1.],
        [0.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [1.],
        [0.],
        [1.],
        [1.],
        [1.],
        [0.],
        [1.]])
```


æœ€åæ„å»ºæ¨¡å‹æµ‹è¯•ä¸€ä¸‹æ•°æ®é›†ç®¡é“æ˜¯å¦å¯ç”¨ã€‚

```python
import torch
from torch import nn 
import importlib 
from torchkeras import Model,summary

class Net(Model):
    
    def __init__(self):
        super(Net, self).__init__()
        
        #è®¾ç½®padding_idxå‚æ•°åå°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å¡«å……çš„tokenå§‹ç»ˆèµ‹å€¼ä¸º0å‘é‡
        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = 3,padding_idx = 1)
        self.conv = nn.Sequential()
        self.conv.add_module("conv_1",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))
        self.conv.add_module("pool_1",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_1",nn.ReLU())
        self.conv.add_module("conv_2",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))
        self.conv.add_module("pool_2",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_2",nn.ReLU())
        
        self.dense = nn.Sequential()
        self.dense.add_module("flatten",nn.Flatten())
        self.dense.add_module("linear",nn.Linear(6144,1))
        self.dense.add_module("sigmoid",nn.Sigmoid())
        
    def forward(self,x):
        x = self.embedding(x).transpose(1,2)
        x = self.conv(x)
        y = self.dense(x)
        return y
        
model = Net()
print(model)

model.summary(input_shape = (200,),input_dtype = torch.LongTensor)

```

```
Net(
  (embedding): Embedding(10000, 3, padding_idx=1)
  (conv): Sequential(
    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_1): ReLU()
    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_2): ReLU()
  )
  (dense): Sequential(
    (flatten): Flatten()
    (linear): Linear(in_features=6144, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         Embedding-1               [-1, 200, 3]          30,000
            Conv1d-2              [-1, 16, 196]             256
         MaxPool1d-3               [-1, 16, 98]               0
              ReLU-4               [-1, 16, 98]               0
            Conv1d-5              [-1, 128, 97]           4,224
         MaxPool1d-6              [-1, 128, 48]               0
              ReLU-7              [-1, 128, 48]               0
           Flatten-8                 [-1, 6144]               0
            Linear-9                    [-1, 1]           6,145
          Sigmoid-10                    [-1, 1]               0
================================================================
Total params: 40,625
Trainable params: 40,625
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000763
Forward/backward pass size (MB): 0.287796
Params size (MB): 0.154972
Estimated Total Size (MB): 0.443531
----------------------------------------------------------------
```

```python
# ç¼–è¯‘æ¨¡å‹
def accuracy(y_pred,y_true):
    y_pred = torch.where(y_pred>0.5,torch.ones_like(y_pred,dtype = torch.float32),
                      torch.zeros_like(y_pred,dtype = torch.float32))
    acc = torch.mean(1-torch.abs(y_true-y_pred))
    return acc

model.compile(loss_func = nn.BCELoss(),optimizer= torch.optim.Adagrad(model.parameters(),lr = 0.02),
             metrics_dict={"accuracy":accuracy})

```

```python
# è®­ç»ƒæ¨¡å‹
dfhistory = model.fit(10,dl_train,dl_val=dl_test,log_step_freq= 200)
```

```
Start Training ...

================================================================================2020-07-11 23:21:53
{'step': 200, 'loss': 0.956, 'accuracy': 0.521}
{'step': 400, 'loss': 0.823, 'accuracy': 0.53}
{'step': 600, 'loss': 0.774, 'accuracy': 0.545}
{'step': 800, 'loss': 0.747, 'accuracy': 0.56}
{'step': 1000, 'loss': 0.726, 'accuracy': 0.572}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.726 |  0.572   |  0.661   |    0.613     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:22:20
{'step': 200, 'loss': 0.605, 'accuracy': 0.668}
{'step': 400, 'loss': 0.602, 'accuracy': 0.674}
{'step': 600, 'loss': 0.592, 'accuracy': 0.681}
{'step': 800, 'loss': 0.584, 'accuracy': 0.687}
{'step': 1000, 'loss': 0.575, 'accuracy': 0.696}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   2   | 0.575 |  0.696   |  0.553   |    0.716     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:25:53
{'step': 200, 'loss': 0.294, 'accuracy': 0.877}
{'step': 400, 'loss': 0.299, 'accuracy': 0.875}
{'step': 600, 'loss': 0.298, 'accuracy': 0.875}
{'step': 800, 'loss': 0.296, 'accuracy': 0.876}
{'step': 1000, 'loss': 0.298, 'accuracy': 0.875}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   10  | 0.298 |  0.875   |  0.464   |    0.795     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:26:19
Finished Training...

```

```python

```

### ä¸‰ï¼Œä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†


DataLoaderèƒ½å¤Ÿæ§åˆ¶batchçš„å¤§å°ï¼Œbatchä¸­å…ƒç´ çš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥åŠå°†batchç»“æœæ•´ç†æˆæ¨¡å‹æ‰€éœ€è¾“å…¥å½¢å¼çš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ã€‚

DataLoaderçš„å‡½æ•°ç­¾åå¦‚ä¸‹ã€‚

<!-- #region -->
```python
DataLoader(
    dataset,
    batch_size=1,
    shuffle=False,
    sampler=None,
    batch_sampler=None,
    num_workers=0,
    collate_fn=None,
    pin_memory=False,
    drop_last=False,
    timeout=0,
    worker_init_fn=None,
    multiprocessing_context=None,
)
```

<!-- #endregion -->

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»…ä»…ä¼šé…ç½® dataset, batch_size, shuffle, num_workers, drop_lastè¿™äº”ä¸ªå‚æ•°ï¼Œå…¶ä»–å‚æ•°ä½¿ç”¨é»˜è®¤å€¼å³å¯ã€‚

DataLoaderé™¤äº†å¯ä»¥åŠ è½½æˆ‘ä»¬å‰é¢è®²çš„ torch.utils.data.Dataset å¤–ï¼Œè¿˜èƒ½å¤ŸåŠ è½½å¦å¤–ä¸€ç§æ•°æ®é›† torch.utils.data.IterableDatasetã€‚

å’ŒDatasetæ•°æ®é›†ç›¸å½“äºä¸€ç§åˆ—è¡¨ç»“æ„ä¸åŒï¼ŒIterableDatasetç›¸å½“äºä¸€ç§è¿­ä»£å™¨ç»“æ„ã€‚ å®ƒæ›´åŠ å¤æ‚ï¼Œä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

- dataset : æ•°æ®é›†
- batch_size: æ‰¹æ¬¡å¤§å°
- shuffle: æ˜¯å¦ä¹±åº
- sampler: æ ·æœ¬é‡‡æ ·å‡½æ•°ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚
- batch_sampler: æ‰¹æ¬¡é‡‡æ ·å‡½æ•°ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚
- num_workers: ä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ï¼Œè®¾ç½®çš„è¿›ç¨‹æ•°ã€‚
- collate_fn: æ•´ç†ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®çš„å‡½æ•°ã€‚
- pin_memory: æ˜¯å¦è®¾ç½®ä¸ºé”ä¸šå†…å­˜ã€‚é»˜è®¤ä¸ºFalseï¼Œé”ä¸šå†…å­˜ä¸ä¼šä½¿ç”¨è™šæ‹Ÿå†…å­˜(ç¡¬ç›˜)ï¼Œä»é”ä¸šå†…å­˜æ‹·è´åˆ°GPUä¸Šé€Ÿåº¦ä¼šæ›´å¿«ã€‚
- drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªæ ·æœ¬æ•°é‡ä¸è¶³batch_sizeæ‰¹æ¬¡æ•°æ®ã€‚
- timeout: åŠ è½½ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡çš„æœ€é•¿ç­‰å¾…æ—¶é—´ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚
- worker_init_fn: æ¯ä¸ªworkerä¸­datasetçš„åˆå§‹åŒ–å‡½æ•°ï¼Œå¸¸ç”¨äº IterableDatasetã€‚ä¸€èˆ¬ä¸ä½¿ç”¨ã€‚



```python
#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“
ds = TensorDataset(torch.arange(1,50))
dl = DataLoader(ds,
                batch_size = 10,
                shuffle= True,
                num_workers=2,
                drop_last = True)
#è¿­ä»£æ•°æ®
for batch, in dl:
    print(batch)
```

```
tensor([43, 44, 21, 36,  9,  5, 28, 16, 20, 14])
tensor([23, 49, 35, 38,  2, 34, 45, 18, 15, 40])
tensor([26,  6, 27, 39,  8,  4, 24, 19, 32, 17])
tensor([ 1, 29, 11, 47, 12, 22, 48, 42, 10,  7])
```


**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾é£Ÿå±‹logo.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)

```python

```
