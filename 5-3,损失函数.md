
# 5-5,æŸå¤±å‡½æ•°losses

ä¸€èˆ¬æ¥è¯´ï¼Œç›‘ç£å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ç”±æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ç»„æˆã€‚(Objective = Loss + Regularization)

Pytorchä¸­çš„æŸå¤±å‡½æ•°ä¸€èˆ¬åœ¨è®­ç»ƒæ¨¡åž‹æ—¶å€™æŒ‡å®šã€‚

æ³¨æ„Pytorchä¸­å†…ç½®çš„æŸå¤±å‡½æ•°çš„å‚æ•°å’Œtensorflowä¸åŒï¼Œæ˜¯y_predåœ¨å‰ï¼Œy_trueåœ¨åŽï¼Œè€ŒTensorflowæ˜¯y_trueåœ¨å‰ï¼Œy_predåœ¨åŽã€‚

å¯¹äºŽå›žå½’æ¨¡åž‹ï¼Œé€šå¸¸ä½¿ç”¨çš„å†…ç½®æŸå¤±å‡½æ•°æ˜¯å‡æ–¹æŸå¤±å‡½æ•°nn.MSELoss ã€‚

å¯¹äºŽäºŒåˆ†ç±»æ¨¡åž‹ï¼Œé€šå¸¸ä½¿ç”¨çš„æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°nn.BCELoss (è¾“å…¥å·²ç»æ˜¯sigmoidæ¿€æ´»å‡½æ•°ä¹‹åŽçš„ç»“æžœ) 
æˆ–è€… nn.BCEWithLogitsLoss (è¾“å…¥å°šæœªç»è¿‡nn.Sigmoidæ¿€æ´»å‡½æ•°) ã€‚

å¯¹äºŽå¤šåˆ†ç±»æ¨¡åž‹ï¼Œä¸€èˆ¬æŽ¨èä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•° nn.CrossEntropyLossã€‚
(y_trueéœ€è¦æ˜¯ä¸€ç»´çš„ï¼Œæ˜¯ç±»åˆ«ç¼–ç ã€‚y_predæœªç»è¿‡nn.Softmaxæ¿€æ´»ã€‚) 

æ­¤å¤–ï¼Œå¦‚æžœå¤šåˆ†ç±»çš„y_predç»è¿‡äº†nn.LogSoftmaxæ¿€æ´»ï¼Œå¯ä»¥ä½¿ç”¨nn.NLLLossæŸå¤±å‡½æ•°(The negative log likelihood loss)ã€‚
è¿™ç§æ–¹æ³•å’Œç›´æŽ¥ä½¿ç”¨nn.CrossEntropyLossç­‰ä»·ã€‚


å¦‚æžœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°éœ€è¦æŽ¥æ”¶ä¸¤ä¸ªå¼ é‡y_predï¼Œy_trueä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚

Pytorchä¸­çš„æ­£åˆ™åŒ–é¡¹ä¸€èˆ¬é€šè¿‡è‡ªå®šä¹‰çš„æ–¹å¼å’ŒæŸå¤±å‡½æ•°ä¸€èµ·æ·»åŠ ä½œä¸ºç›®æ ‡å‡½æ•°ã€‚

å¦‚æžœä»…ä»…ä½¿ç”¨L2æ­£åˆ™åŒ–ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨ä¼˜åŒ–å™¨çš„weight_decayå‚æ•°æ¥å®žçŽ°ç›¸åŒçš„æ•ˆæžœã€‚



### ä¸€ï¼Œå†…ç½®æŸå¤±å‡½æ•°

```python
import numpy as np
import pandas as pd
import torch 
from torch import nn 
import torch.nn.functional as F 


y_pred = torch.tensor([[10.0,0.0,-10.0],[8.0,8.0,8.0]])
y_true = torch.tensor([0,2])

# ç›´æŽ¥è°ƒç”¨äº¤å‰ç†µæŸå¤±
ce = nn.CrossEntropyLoss()(y_pred,y_true)
print(ce)

# ç­‰ä»·äºŽå…ˆè®¡ç®—nn.LogSoftmaxæ¿€æ´»ï¼Œå†è°ƒç”¨NLLLoss
y_pred_logsoftmax = nn.LogSoftmax(dim = 1)(y_pred)
nll = nn.NLLLoss()(y_pred_logsoftmax,y_true)
print(nll)

```

```
tensor(0.5493)
tensor(0.5493)
```



å†…ç½®çš„æŸå¤±å‡½æ•°ä¸€èˆ¬æœ‰ç±»çš„å®žçŽ°å’Œå‡½æ•°çš„å®žçŽ°ä¸¤ç§å½¢å¼ã€‚

å¦‚ï¼šnn.BCE å’Œ F.binary_cross_entropy éƒ½æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå‰è€…æ˜¯ç±»çš„å®žçŽ°å½¢å¼ï¼ŒåŽè€…æ˜¯å‡½æ•°çš„å®žçŽ°å½¢å¼ã€‚

å®žé™…ä¸Šç±»çš„å®žçŽ°å½¢å¼é€šå¸¸æ˜¯è°ƒç”¨å‡½æ•°çš„å®žçŽ°å½¢å¼å¹¶ç”¨nn.Moduleå°è£…åŽå¾—åˆ°çš„ã€‚

ä¸€èˆ¬æˆ‘ä»¬å¸¸ç”¨çš„æ˜¯ç±»çš„å®žçŽ°å½¢å¼ã€‚å®ƒä»¬å°è£…åœ¨torch.nnæ¨¡å—ä¸‹ï¼Œå¹¶ä¸”ç±»åä»¥Lossç»“å°¾ã€‚

å¸¸ç”¨çš„ä¸€äº›å†…ç½®æŸå¤±å‡½æ•°è¯´æ˜Žå¦‚ä¸‹ã€‚


* nn.MSELossï¼ˆå‡æ–¹è¯¯å·®æŸå¤±ï¼Œä¹Ÿå«åšL2æŸå¤±ï¼Œç”¨äºŽå›žå½’ï¼‰

* nn.L1Loss ï¼ˆL1æŸå¤±ï¼Œä¹Ÿå«åšç»å¯¹å€¼è¯¯å·®æŸå¤±ï¼Œç”¨äºŽå›žå½’ï¼‰

* nn.SmoothL1Loss (å¹³æ»‘L1æŸå¤±ï¼Œå½“è¾“å…¥åœ¨-1åˆ°1ä¹‹é—´æ—¶ï¼Œå¹³æ»‘ä¸ºL2æŸå¤±ï¼Œç”¨äºŽå›žå½’)

* nn.BCELoss (äºŒå…ƒäº¤å‰ç†µï¼Œç”¨äºŽäºŒåˆ†ç±»ï¼Œè¾“å…¥å·²ç»è¿‡nn.Sigmoidæ¿€æ´»ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®é›†å¯ä»¥ç”¨weigthså‚æ•°è°ƒæ•´ç±»åˆ«æƒé‡)

* nn.BCEWithLogitsLoss (äºŒå…ƒäº¤å‰ç†µï¼Œç”¨äºŽäºŒåˆ†ç±»ï¼Œè¾“å…¥æœªç»è¿‡nn.Sigmoidæ¿€æ´»)

* nn.CrossEntropyLoss (äº¤å‰ç†µï¼Œç”¨äºŽå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºç¨€ç–ç¼–ç ï¼Œè¾“å…¥æœªç»è¿‡nn.Softmaxæ¿€æ´»ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®é›†å¯ä»¥ç”¨weigthså‚æ•°è°ƒæ•´ç±»åˆ«æƒé‡)

* nn.NLLLoss (è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œç”¨äºŽå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºç¨€ç–ç¼–ç ï¼Œè¾“å…¥ç»è¿‡nn.LogSoftmaxæ¿€æ´»)

* nn.CosineSimilarity(ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¯ç”¨äºŽå¤šåˆ†ç±»)

* nn.AdaptiveLogSoftmaxWithLoss (ä¸€ç§é€‚åˆéžå¸¸å¤šç±»åˆ«ä¸”ç±»åˆ«åˆ†å¸ƒå¾ˆä¸å‡è¡¡çš„æŸå¤±å‡½æ•°ï¼Œä¼šè‡ªé€‚åº”åœ°å°†å¤šä¸ªå°ç±»åˆ«åˆæˆä¸€ä¸ªcluster)


æ›´å¤šæŸå¤±å‡½æ•°çš„ä»‹ç»å‚è€ƒå¦‚ä¸‹çŸ¥ä¹Žæ–‡ç« ï¼š

ã€ŠPyTorchçš„åå…«ä¸ªæŸå¤±å‡½æ•°ã€‹

https://zhuanlan.zhihu.com/p/61379965


### äºŒï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°


è‡ªå®šä¹‰æŸå¤±å‡½æ•°æŽ¥æ”¶ä¸¤ä¸ªå¼ é‡y_pred,y_trueä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚

ä¹Ÿå¯ä»¥å¯¹nn.Moduleè¿›è¡Œå­ç±»åŒ–ï¼Œé‡å†™forwardæ–¹æ³•å®žçŽ°æŸå¤±çš„è®¡ç®—é€»è¾‘ï¼Œä»Žè€Œå¾—åˆ°æŸå¤±å‡½æ•°çš„ç±»çš„å®žçŽ°ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªFocal Lossçš„è‡ªå®šä¹‰å®žçŽ°ç¤ºèŒƒã€‚Focal Lossæ˜¯ä¸€ç§å¯¹binary_crossentropyçš„æ”¹è¿›æŸå¤±å‡½æ•°å½¢å¼ã€‚

å®ƒåœ¨æ ·æœ¬ä¸å‡è¡¡å’Œå­˜åœ¨è¾ƒå¤šæ˜“åˆ†ç±»çš„æ ·æœ¬æ—¶ç›¸æ¯”binary_crossentropyå…·æœ‰æ˜Žæ˜¾çš„ä¼˜åŠ¿ã€‚

å®ƒæœ‰ä¸¤ä¸ªå¯è°ƒå‚æ•°ï¼Œalphaå‚æ•°å’Œgammaå‚æ•°ã€‚å…¶ä¸­alphaå‚æ•°ä¸»è¦ç”¨äºŽè¡°å‡è´Ÿæ ·æœ¬çš„æƒé‡ï¼Œgammaå‚æ•°ä¸»è¦ç”¨äºŽè¡°å‡å®¹æ˜“è®­ç»ƒæ ·æœ¬çš„æƒé‡ã€‚

ä»Žè€Œè®©æ¨¡åž‹æ›´åŠ èšç„¦åœ¨æ­£æ ·æœ¬å’Œå›°éš¾æ ·æœ¬ä¸Šã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™ä¸ªæŸå¤±å‡½æ•°å«åšFocal Lossã€‚

è¯¦è§ã€Š5åˆ†é’Ÿç†è§£Focal Lossä¸ŽGHMâ€”â€”è§£å†³æ ·æœ¬ä¸å¹³è¡¡åˆ©å™¨ã€‹

https://zhuanlan.zhihu.com/p/80594704



$$focal\_loss(y,p) = 
\begin{cases} -\alpha (1-p)^{\gamma}\log(p) & \text{if y = 1}\\
-(1-\alpha) p^{\gamma}\log(1-p) & \text{if y = 0} 
\end{cases} $$

```python
class FocalLoss(nn.Module):
    
    def __init__(self,gamma=2.0,alpha=0.75):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self,y_pred,y_true):
        bce = torch.nn.BCELoss(reduction = "none")(y_pred,y_true)
        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))
        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)
        modulating_factor = torch.pow(1.0 - p_t, self.gamma)
        loss = torch.mean(alpha_factor * modulating_factor * bce)
        return loss
    
    
    
```

```python
#å›°éš¾æ ·æœ¬
y_pred_hard = torch.tensor([[0.5],[0.5]])
y_true_hard = torch.tensor([[1.0],[0.0]])

#å®¹æ˜“æ ·æœ¬
y_pred_easy = torch.tensor([[0.9],[0.1]])
y_true_easy = torch.tensor([[1.0],[0.0]])

focal_loss = FocalLoss()
bce_loss = nn.BCELoss()

print("focal_loss(hard samples):", focal_loss(y_pred_hard,y_true_hard))
print("bce_loss(hard samples):", bce_loss(y_pred_hard,y_true_hard))
print("focal_loss(easy samples):", focal_loss(y_pred_easy,y_true_easy))
print("bce_loss(easy samples):", bce_loss(y_pred_easy,y_true_easy))

#å¯è§ focal_lossè®©å®¹æ˜“æ ·æœ¬çš„æƒé‡è¡°å‡åˆ°åŽŸæ¥çš„ 0.0005/0.1054 = 0.00474
#è€Œè®©å›°éš¾æ ·æœ¬çš„æƒé‡åªè¡°å‡åˆ°åŽŸæ¥çš„ 0.0866/0.6931=0.12496

# å› æ­¤ç›¸å¯¹è€Œè¨€ï¼Œfocal_losså¯ä»¥è¡°å‡å®¹æ˜“æ ·æœ¬çš„æƒé‡ã€‚



```

```
focal_loss(hard samples): tensor(0.0866)
bce_loss(hard samples): tensor(0.6931)
focal_loss(easy samples): tensor(0.0005)
bce_loss(easy samples): tensor(0.1054)
```


FocalLossçš„ä½¿ç”¨å®Œæ•´èŒƒä¾‹å¯ä»¥å‚è€ƒä¸‹é¢ä¸­`è‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹`ä¸­çš„èŒƒä¾‹ï¼Œè¯¥èŒƒä¾‹æ—¢æ¼”ç¤ºäº†è‡ªå®šä¹‰æ­£åˆ™åŒ–é¡¹çš„æ–¹æ³•ï¼Œä¹Ÿæ¼”ç¤ºäº†FocalLossçš„ä½¿ç”¨æ–¹æ³•ã€‚



### ä¸‰ï¼Œè‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹


é€šå¸¸è®¤ä¸ºL1 æ­£åˆ™åŒ–å¯ä»¥äº§ç”Ÿç¨€ç–æƒå€¼çŸ©é˜µï¼Œå³äº§ç”Ÿä¸€ä¸ªç¨€ç–æ¨¡åž‹ï¼Œå¯ä»¥ç”¨äºŽç‰¹å¾é€‰æ‹©ã€‚

è€ŒL2 æ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢æ¨¡åž‹è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰ã€‚ä¸€å®šç¨‹åº¦ä¸Šï¼ŒL1ä¹Ÿå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

ä¸‹é¢ä»¥ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤ºç»™æ¨¡åž‹çš„ç›®æ ‡å‡½æ•°æ·»åŠ è‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹çš„æ–¹æ³•ã€‚

è¿™ä¸ªèŒƒä¾‹åŒæ—¶æ¼”ç¤ºäº†ä¸Šä¸€ä¸ªéƒ¨åˆ†çš„FocalLossçš„ä½¿ç”¨ã€‚




**1ï¼Œå‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset
import torchkeras 
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æ­£è´Ÿæ ·æœ¬æ•°é‡
n_positive,n_negative = 200,6000

#ç”Ÿæˆæ­£æ ·æœ¬, å°åœ†çŽ¯åˆ†å¸ƒ
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) 
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)
Yp = torch.ones_like(r_p)

#ç”Ÿæˆè´Ÿæ ·æœ¬, å¤§åœ†çŽ¯åˆ†å¸ƒ
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) 
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#æ±‡æ€»æ ·æœ¬
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)


#å¯è§†åŒ–
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0],Xp[:,1],c = "r")
plt.scatter(Xn[:,0],Xn[:,1],c = "g")
plt.legend(["positive","negative"]);

```

![](./data/5-3-åŒå¿ƒåœ†åˆ†å¸ƒ.png)

```python
ds = TensorDataset(X,Y)

ds_train,ds_valid = torch.utils.data.random_split(ds,[int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])
dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True,num_workers=2)
dl_valid = DataLoader(ds_valid,batch_size = 100,num_workers=2)

```

**2ï¼Œå®šä¹‰æ¨¡åž‹**

```python
class DNNModel(torchkeras.Model):
    def __init__(self):
        super(DNNModel, self).__init__()
        self.fc1 = nn.Linear(2,4)
        self.fc2 = nn.Linear(4,8) 
        self.fc3 = nn.Linear(8,1)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        y = nn.Sigmoid()(self.fc3(x))
        return y
        
model = DNNModel()

model.summary(input_shape =(2,))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                    [-1, 4]              12
            Linear-2                    [-1, 8]              40
            Linear-3                    [-1, 1]               9
================================================================
Total params: 61
Trainable params: 61
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000008
Forward/backward pass size (MB): 0.000099
Params size (MB): 0.000233
Estimated Total Size (MB): 0.000340
----------------------------------------------------------------
```


**3ï¼Œè®­ç»ƒæ¨¡åž‹**

```python
# å‡†ç¡®çŽ‡
def accuracy(y_pred,y_true):
    y_pred = torch.where(y_pred>0.5,torch.ones_like(y_pred,dtype = torch.float32),
                      torch.zeros_like(y_pred,dtype = torch.float32))
    acc = torch.mean(1-torch.abs(y_true-y_pred))
    return acc

# L2æ­£åˆ™åŒ–
def L2Loss(model,alpha):
    l2_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name: #ä¸€èˆ¬ä¸å¯¹åç½®é¡¹ä½¿ç”¨æ­£åˆ™
            l2_loss = l2_loss + (0.5 * alpha * torch.sum(torch.pow(param, 2)))
    return l2_loss

# L1æ­£åˆ™åŒ–
def L1Loss(model,beta):
    l1_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name:
            l1_loss = l1_loss +  beta * torch.sum(torch.abs(param))
    return l1_loss

# å°†L2æ­£åˆ™å’ŒL1æ­£åˆ™æ·»åŠ åˆ°FocalLossæŸå¤±ï¼Œä¸€èµ·ä½œä¸ºç›®æ ‡å‡½æ•°
def focal_loss_with_regularization(y_pred,y_true):
    focal = FocalLoss()(y_pred,y_true) 
    l2_loss = L2Loss(model,0.001) #æ³¨æ„è®¾ç½®æ­£åˆ™åŒ–é¡¹ç³»æ•°
    l1_loss = L1Loss(model,0.001)
    total_loss = focal + l2_loss + l1_loss
    return total_loss

model.compile(loss_func =focal_loss_with_regularization,
              optimizer= torch.optim.Adam(model.parameters(),lr = 0.01),
             metrics_dict={"accuracy":accuracy})

dfhistory = model.fit(30,dl_train = dl_train,dl_val = dl_valid,log_step_freq = 30)

```

```
Start Training ...

================================================================================2020-07-11 23:34:17
{'step': 30, 'loss': 0.021, 'accuracy': 0.972}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.022 |  0.971   |  0.025   |     0.96     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:34:27
{'step': 30, 'loss': 0.016, 'accuracy': 0.984}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   30  | 0.016 |  0.981   |  0.017   |    0.983     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:34:27
Finished Training...
```

```python

```

```python
# ç»“æžœå¯è§†åŒ–
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(model.forward(X)>=0.5)]
Xn_pred = X[torch.squeeze(model.forward(X)<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");

```

![](./data/5-3-focal_lossé¢„æµ‹ç»“æžœ.png)

```python

```

### å››ï¼Œé€šè¿‡ä¼˜åŒ–å™¨å®žçŽ°L2æ­£åˆ™åŒ–


å¦‚æžœä»…ä»…éœ€è¦ä½¿ç”¨L2æ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥åˆ©ç”¨ä¼˜åŒ–å™¨çš„weight_decayå‚æ•°æ¥å®žçŽ°ã€‚

weight_decayå‚æ•°å¯ä»¥è®¾ç½®å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡°å‡ï¼Œè¿™å’ŒL2æ­£åˆ™åŒ–çš„ä½œç”¨æ•ˆæžœç­‰ä»·ã€‚


```
before L2 regularization:

gradient descent: w = w - lr * dloss_dw 

after L2 regularization:

gradient descent: w = w - lr * (dloss_dw+beta*w) = (1-lr*beta)*w - lr*dloss_dw

so ï¼ˆ1-lr*betaï¼‰is the weight decay ratio.
```


Pytorchçš„ä¼˜åŒ–å™¨æ”¯æŒä¸€ç§ç§°ä¹‹ä¸ºPer-parameter optionsçš„æ“ä½œï¼Œå°±æ˜¯å¯¹æ¯ä¸€ä¸ªå‚æ•°è¿›è¡Œç‰¹å®šçš„å­¦ä¹ çŽ‡ï¼Œæƒé‡è¡°å‡çŽ‡æŒ‡å®šï¼Œä»¥æ»¡è¶³æ›´ä¸ºç»†è‡´çš„è¦æ±‚ã€‚

```python
weight_params = [param for name, param in model.named_parameters() if "bias" not in name]
bias_params = [param for name, param in model.named_parameters() if "bias" in name]

optimizer = torch.optim.SGD([{'params': weight_params, 'weight_decay':1e-5},
                             {'params': bias_params, 'weight_decay':0}],
                            lr=1e-2, momentum=0.9)

```

```python

```

**å¦‚æžœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ðŸ˜Š!** 

å¦‚æžœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿Žåœ¨å…¬ä¼—å·"ç®—æ³•ç¾Žé£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›žå¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åŽå°å›žå¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾Žé£Ÿå±‹logo.png](./data/ç®—æ³•ç¾Žé£Ÿå±‹äºŒç»´ç .jpg)

```python

```
